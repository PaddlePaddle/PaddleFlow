diff --git a/Makefile b/Makefile
index c379d7d..2e30903 100644
--- a/Makefile
+++ b/Makefile
@@ -12,8 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-BIN_DIR=_output/bin
-RELEASE_DIR=_output/release
+BIN_DIR=output/bin
+RELEASE_DIR=output/release
 REPO_PATH=volcano.sh/volcano
 IMAGE_PREFIX=volcanosh/vc
 CRD_OPTIONS ?= "crd:crdVersions=v1"
@@ -21,6 +21,7 @@ CC ?= "gcc"
 SUPPORT_PLUGINS ?= "no"
 CRD_VERSION ?= v1
 
+export PATH := $(GO_1_16_BIN):$(PATH)
 # Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)
 ifeq (,$(shell go env GOBIN))
 GOBIN=$(shell go env GOPATH)/bin
@@ -59,6 +60,8 @@ include Makefile.def
 all: vc-scheduler vc-controller-manager vc-webhook-manager vcctl command-lines
 
 init:
+	go env -w CGO_ENABLED=0
+	go version
 	mkdir -p ${BIN_DIR}
 	mkdir -p ${RELEASE_DIR}
 
@@ -70,6 +73,7 @@ vc-controller-manager: init
 
 vc-webhook-manager: init
 	go build -ldflags ${LD_FLAGS} -o=${BIN_DIR}/vc-webhook-manager ./cmd/webhook-manager
+	cp installer/dockerfile/webhook-manager/gen-admission-secret.sh ${BIN_DIR}
 
 vcctl: init
 	go build -ldflags ${LD_FLAGS} -o=${BIN_DIR}/vcctl ./cmd/cli
diff --git a/cmd/controller-manager/app/options/options.go b/cmd/controller-manager/app/options/options.go
index 558f6b0..b044146 100644
--- a/cmd/controller-manager/app/options/options.go
+++ b/cmd/controller-manager/app/options/options.go
@@ -50,6 +50,9 @@ type ServerOption struct {
 	// HealthzBindAddress is the IP address and port for the health check server to serve on,
 	// defaulting to 0.0.0.0:11252
 	HealthzBindAddress string
+
+	// Namespace for controller to sync
+	Namespace []string
 }
 
 // NewServerOption creates a new CMServer with a default config.
@@ -74,6 +77,7 @@ func (s *ServerOption) AddFlags(fs *pflag.FlagSet) {
 		"Larger number = faster job updating, but more CPU load")
 	fs.StringVar(&s.SchedulerName, "scheduler-name", defaultSchedulerName, "Volcano will handle pods whose .spec.SchedulerName is same as scheduler-name")
 	fs.IntVar(&s.MaxRequeueNum, "max-requeue-num", defaultMaxRequeueNum, "The number of times a job, queue or command will be requeued before it is dropped out of the queue")
+	fs.StringArrayVar(&s.Namespace, "namespace", nil, "vc-controller-manager watching namespaces")
 }
 
 // CheckOptionOrDie checks the LockObjectNamespace.
diff --git a/cmd/controller-manager/app/server.go b/cmd/controller-manager/app/server.go
index 9e5f1aa..43a0b32 100644
--- a/cmd/controller-manager/app/server.go
+++ b/cmd/controller-manager/app/server.go
@@ -111,17 +111,21 @@ func Run(opt *options.ServerOption) error {
 }
 
 func startControllers(config *rest.Config, opt *options.ServerOption) func(ctx context.Context) {
-	controllerOpt := &framework.ControllerOption{}
+	controllerOpt := &framework.ControllerOption{
+		Namespaces: make([]string, len(opt.Namespace)),
+	}
 
 	controllerOpt.SchedulerName = opt.SchedulerName
 	controllerOpt.WorkerNum = opt.WorkerThreads
 	controllerOpt.MaxRequeueNum = opt.MaxRequeueNum
+	copy(controllerOpt.Namespaces, opt.Namespace)
 
 	// TODO: add user agent for different controllers
 	controllerOpt.KubeClient = kubeclientset.NewForConfigOrDie(config)
 	controllerOpt.VolcanoClient = vcclientset.NewForConfigOrDie(config)
 	controllerOpt.SharedInformerFactory = informers.NewSharedInformerFactory(controllerOpt.KubeClient, 0)
 
+	klog.Infof("volcano controller options <%+v>", controllerOpt)
 	return func(ctx context.Context) {
 		framework.ForeachController(func(c framework.Controller) {
 			if err := c.Initialize(controllerOpt); err != nil {
diff --git a/cmd/controller-manager/main.go b/cmd/controller-manager/main.go
index 86b1732..8a4a622 100644
--- a/cmd/controller-manager/main.go
+++ b/cmd/controller-manager/main.go
@@ -27,6 +27,7 @@ import (
 	cliflag "k8s.io/component-base/cli/flag"
 	"k8s.io/klog"
 
+	_ "volcano.sh/volcano/pkg/controllers/elasticresourcequota"
 	_ "volcano.sh/volcano/pkg/controllers/garbagecollector"
 	_ "volcano.sh/volcano/pkg/controllers/job"
 	_ "volcano.sh/volcano/pkg/controllers/podgroup"
diff --git a/cmd/webhook-manager/app/options/options.go b/cmd/webhook-manager/app/options/options.go
index e327abe..64e3359 100644
--- a/cmd/webhook-manager/app/options/options.go
+++ b/cmd/webhook-manager/app/options/options.go
@@ -22,6 +22,7 @@ import (
 	"github.com/spf13/pflag"
 
 	"volcano.sh/volcano/pkg/kube"
+	"volcano.sh/volcano/pkg/webhooks/router"
 )
 
 const (
@@ -32,6 +33,7 @@ const (
 
 // Config admission-controller server config.
 type Config struct {
+	CgpuOptions       router.CGPUOptions
 	KubeClientOptions kube.ClientOptions
 	CertFile          string
 	KeyFile           string
@@ -52,6 +54,14 @@ func NewConfig() *Config {
 
 // AddFlags add flags.
 func (c *Config) AddFlags(fs *pflag.FlagSet) {
+	fs.BoolVar(&c.CgpuOptions.GPUCGroupEnable, "gpu-cgroup-enable", false, "Global switch to limit vGPU resource")
+	fs.StringVar(&c.CgpuOptions.CGroupMapMode, "cgroup-map-mode", "0", "type of mapping progress in host, 0:docker 1:kongming")
+	fs.StringVar(&c.CgpuOptions.SharedGPUMemoryUnit, "shared-gpu-memory-unit", "GiB", "The vgpu resources unit reported by device-plugin")
+	fs.StringVar(&c.CgpuOptions.Limit, "LIMIT", "0", "LIMIT 0:soft limit & 1:strict limit, which is different type of resource limitation")
+	fs.StringVar(&c.CgpuOptions.NvidiaVisibleDevices, "nvidia-visible-devices", "all", "the visible devices ids of nvidia")
+	fs.StringVar(&c.CgpuOptions.NvidiaLibCudaVolumeHostPath, "nvidia-lib-cuda-volume-host-path", "/data/vgpu_share/volumes/linux-gnu", "The nvidia lib cuda volume path on each host node")
+	fs.StringVar(&c.CgpuOptions.NvidiaLibCudaVolumeContainerPath, "nvidia-lib-cuda-volume-centos-container-path", "/usr/tmp/vgpu_share", "The nvidia lib cuda volume path on centos docker container")
+
 	fs.StringVar(&c.KubeClientOptions.Master, "master", c.KubeClientOptions.Master, "The address of the Kubernetes API server (overrides any value in kubeconfig)")
 	fs.StringVar(&c.KubeClientOptions.KubeConfig, "kubeconfig", c.KubeClientOptions.KubeConfig, "Path to kubeconfig file with authorization and master location information.")
 	fs.StringVar(&c.CertFile, "tls-cert-file", c.CertFile, ""+
diff --git a/cmd/webhook-manager/app/server.go b/cmd/webhook-manager/app/server.go
index 573df67..4557edf 100644
--- a/cmd/webhook-manager/app/server.go
+++ b/cmd/webhook-manager/app/server.go
@@ -67,8 +67,10 @@ func Run(config *options.Config) error {
 	router.ForEachAdmission(func(service *router.AdmissionService) {
 		if service.Config != nil {
 			service.Config.VolcanoClient = vClient
+			service.Config.KubeClient = kubeClient
 			service.Config.SchedulerName = config.SchedulerName
 			service.Config.Recorder = recorder
+			service.Config.CgpuOptions = config.CgpuOptions
 		}
 
 		klog.V(3).Infof("Registered '%s' as webhook.", service.Path)
diff --git a/cmd/webhook-manager/main.go b/cmd/webhook-manager/main.go
index 88aab70..9780c34 100644
--- a/cmd/webhook-manager/main.go
+++ b/cmd/webhook-manager/main.go
@@ -29,11 +29,14 @@ import (
 
 	"volcano.sh/volcano/cmd/webhook-manager/app"
 	"volcano.sh/volcano/cmd/webhook-manager/app/options"
-
+	_ "volcano.sh/volcano/pkg/webhooks/admission/elasticresourcequotas/mutate"
+	_ "volcano.sh/volcano/pkg/webhooks/admission/elasticresourcequotas/validate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/jobs/mutate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/jobs/validate"
+	_ "volcano.sh/volcano/pkg/webhooks/admission/nodes/validate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/podgroups/mutate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/pods"
+	_ "volcano.sh/volcano/pkg/webhooks/admission/pods/mutate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/queues/mutate"
 	_ "volcano.sh/volcano/pkg/webhooks/admission/queues/validate"
 )
diff --git a/config/crd/bases/scheduling.volcano.sh_elasticresourcequotas.yaml b/config/crd/bases/scheduling.volcano.sh_elasticresourcequotas.yaml
new file mode 100644
index 0000000..ddded56
--- /dev/null
+++ b/config/crd/bases/scheduling.volcano.sh_elasticresourcequotas.yaml
@@ -0,0 +1,106 @@
+
+---
+apiVersion: apiextensions.k8s.io/v1
+kind: CustomResourceDefinition
+metadata:
+  annotations:
+    controller-gen.kubebuilder.io/version: v0.4.1
+  creationTimestamp: null
+  name: elasticresourcequotas.scheduling.volcano.sh
+spec:
+  group: scheduling.volcano.sh
+  names:
+    kind: ElasticResourceQuota
+    listKind: ElasticResourceQuotaList
+    plural: elasticresourcequotas
+    shortNames:
+    - equota
+    - equota-v1beta1
+    singular: elasticresourcequota
+  scope: Cluster
+  versions:
+  - name: v1beta1
+    schema:
+      openAPIV3Schema:
+        description: Elastic Resource Quota
+        properties:
+          apiVersion:
+            description: 'APIVersion defines the versioned schema of this representation
+              of an object. Servers should convert recognized schemas to the latest
+              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
+            type: string
+          kind:
+            description: 'Kind is a string value representing the REST resource this
+              object represents. Servers may infer this from the endpoint the client
+              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
+            type: string
+          metadata:
+            type: object
+          spec:
+            description: 'Specification of the desired behavior of the ElasticResourceQuota.
+              More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status'
+            properties:
+              hardwareTypes:
+                description: HardwareTypes defines hardware types of elastic resource
+                  quota
+                items:
+                  type: string
+                type: array
+              max:
+                additionalProperties:
+                  anyOf:
+                  - type: integer
+                  - type: string
+                  pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                  x-kubernetes-int-or-string: true
+                description: Max is the upper bound of elastic resource quota
+                type: object
+              min:
+                additionalProperties:
+                  anyOf:
+                  - type: integer
+                  - type: string
+                  pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                  x-kubernetes-int-or-string: true
+                description: Min is the lower bound of elastic resource quota
+                type: object
+              namespace:
+                description: namespace defines elastic resource quota belongs to one
+                  namespace
+                type: string
+              reclaimable:
+                description: Reclaimable indicate whether the elastic quota can be
+                  reclaimed by other elastic resource quota
+                type: boolean
+            type: object
+          status:
+            description: The status of ElasticResourceQuota.
+            properties:
+              isLeaf:
+                description: IsLeaf defines whether elastic resource quota is leaf
+                  or not
+                type: boolean
+              queueName:
+                description: QueueName indicate bound queue
+                type: string
+              used:
+                additionalProperties:
+                  anyOf:
+                  - type: integer
+                  - type: string
+                  pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                  x-kubernetes-int-or-string: true
+                description: Used resource of elastic resource quota
+                type: object
+            type: object
+        type: object
+    served: true
+    storage: true
+    subresources:
+      status: {}
+status:
+  acceptedNames:
+    kind: ""
+    plural: ""
+  conditions: []
+  storedVersions: []
diff --git a/config/crd/v1beta1/scheduling.volcano.sh_elasticresourcequotas.yaml b/config/crd/v1beta1/scheduling.volcano.sh_elasticresourcequotas.yaml
new file mode 100644
index 0000000..0bdc824
--- /dev/null
+++ b/config/crd/v1beta1/scheduling.volcano.sh_elasticresourcequotas.yaml
@@ -0,0 +1,107 @@
+
+---
+apiVersion: apiextensions.k8s.io/v1beta1
+kind: CustomResourceDefinition
+metadata:
+  annotations:
+    controller-gen.kubebuilder.io/version: v0.4.1
+  creationTimestamp: null
+  name: elasticresourcequotas.scheduling.volcano.sh
+spec:
+  group: scheduling.volcano.sh
+  names:
+    kind: ElasticResourceQuota
+    listKind: ElasticResourceQuotaList
+    plural: elasticresourcequotas
+    shortNames:
+    - equota
+    - equota-v1beta1
+    singular: elasticresourcequota
+  scope: Cluster
+  subresources:
+    status: {}
+  validation:
+    openAPIV3Schema:
+      description: Elastic Resource Quota
+      properties:
+        apiVersion:
+          description: 'APIVersion defines the versioned schema of this representation
+            of an object. Servers should convert recognized schemas to the latest
+            internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
+          type: string
+        kind:
+          description: 'Kind is a string value representing the REST resource this
+            object represents. Servers may infer this from the endpoint the client
+            submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
+          type: string
+        metadata:
+          type: object
+        spec:
+          description: 'Specification of the desired behavior of the ElasticResourceQuota.
+            More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status'
+          properties:
+            hardwareTypes:
+              description: HardwareTypes defines hardware types of elastic resource
+                quota
+              items:
+                type: string
+              type: array
+            max:
+              additionalProperties:
+                anyOf:
+                - type: integer
+                - type: string
+                pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                x-kubernetes-int-or-string: true
+              description: Max is the upper bound of elastic resource quota
+              type: object
+            min:
+              additionalProperties:
+                anyOf:
+                - type: integer
+                - type: string
+                pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                x-kubernetes-int-or-string: true
+              description: Min is the lower bound of elastic resource quota
+              type: object
+            namespace:
+              description: namespace defines elastic resource quota belongs to one
+                namespace
+              type: string
+            reclaimable:
+              description: Reclaimable indicate whether the elastic quota can be reclaimed
+                by other elastic resource quota
+              type: boolean
+          type: object
+        status:
+          description: The status of ElasticResourceQuota.
+          properties:
+            isLeaf:
+              description: IsLeaf defines whether elastic resource quota is leaf or
+                not
+              type: boolean
+            queueName:
+              description: QueueName indicate bound queue
+              type: string
+            used:
+              additionalProperties:
+                anyOf:
+                - type: integer
+                - type: string
+                pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
+                x-kubernetes-int-or-string: true
+              description: Used resource of elastic resource quota
+              type: object
+          type: object
+      type: object
+  version: v1beta1
+  versions:
+  - name: v1beta1
+    served: true
+    storage: true
+status:
+  acceptedNames:
+    kind: ""
+    plural: ""
+  conditions: []
+  storedVersions: []
diff --git a/go.sum b/go.sum
index d8349fd..b5345cc 100644
--- a/go.sum
+++ b/go.sum
@@ -1,7 +1,6 @@
 bitbucket.org/bertimus9/systemstat v0.0.0-20180207000608-0eeff89b0690/go.mod h1:Ulb78X89vxKYgdL24HMTiXYHlyHEvruOj1ZPlqeNEZM=
 cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=
 cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=
-cloud.google.com/go v0.38.0 h1:ROfEUZz+Gh5pa62DJWXSaonyu3StP6EA6lPEXPI6mCo=
 cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=
 cloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=
 cloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=
@@ -57,9 +56,7 @@ github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a/go.mod h1:l
 github.com/auth0/go-jwt-middleware v0.0.0-20170425171159-5493cabe49f7/go.mod h1:LWMyo4iOLWXHGdBki7NIht1kHru/0wM179h+d3g8ATM=
 github.com/aws/aws-sdk-go v1.6.10/go.mod h1:ZRmQr0FajVIyZ4ZzBYKG5P3ZqPz9IHG41ZoMu1ADI3k=
 github.com/aws/aws-sdk-go v1.28.2/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=
-github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973 h1:xJ4a3vCFaGF/jqvzLMYoU8P317H5OQ+Via4RmuPwCS0=
 github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=
-github.com/beorn7/perks v1.0.0 h1:HWo1m869IqiPhD389kmkxeTalrjNbbJTC8LXupb+sl0=
 github.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=
 github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=
 github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
@@ -158,7 +155,6 @@ github.com/fatih/camelcase v1.0.0/go.mod h1:yN2Sb0lFhZJUdVvtELVWefmrXpuZESvPmqwo
 github.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=
 github.com/flynn/go-shlex v0.0.0-20150515145356-3f9db97f8568/go.mod h1:xEzjJPgXI435gkrCt3MPfRiAkVrwSbHsst4LCFVfpJc=
 github.com/fogleman/gg v1.2.1-0.20190220221249-0403632d5b90/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=
-github.com/fsnotify/fsnotify v1.4.7 h1:IXs+QLmnXW2CcXuY+8Mzv/fWEsPGWxqefPtCP5CnV9I=
 github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=
 github.com/fsnotify/fsnotify v1.4.9 h1:hsms1Qyu0jgnwNXIxa+/V/PDsU6CfLf6CNO8H7IWoS4=
 github.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=
@@ -230,7 +226,6 @@ github.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXP
 github.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0/go.mod h1:E/TSTwGwJL78qG/PmXZO1EjYhfJinVAhrmmHX6Z8B9k=
 github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b h1:VKtxabqXZkF25pY9ekfRL6a582T4P37/31XEstQ5p58=
 github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=
-github.com/golang/groupcache v0.0.0-20160516000752-02826c3e7903 h1:LbsanbbD6LieFkXbj9YNNBupiGHJgFeLpO0j0Fza1h8=
 github.com/golang/groupcache v0.0.0-20160516000752-02826c3e7903/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
 github.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
 github.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
@@ -240,9 +235,7 @@ github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfb
 github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=
 github.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=
 github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
-github.com/golang/protobuf v1.3.1 h1:YF8+flBXS5eO826T4nzqPrxfhQThhXl0YzfuUPu4SBg=
 github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
-github.com/golang/protobuf v1.3.2 h1:6nsPYzhq5kReh6QImI3k5qWzO4PEbvbIW2cwSfR/6xs=
 github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
 github.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=
 github.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=
@@ -257,18 +250,15 @@ github.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw
 github.com/golangplus/bytes v0.0.0-20160111154220-45c989fe5450/go.mod h1:Bk6SMAONeMXrxql8uvOKuAZSu8aM5RUGv+1C6IJaEho=
 github.com/golangplus/fmt v0.0.0-20150411045040-2a5d6d7d2995/go.mod h1:lJgMEyOkYFkPcDKwRXegd+iM6E7matEszMG5HhwytU8=
 github.com/golangplus/testing v0.0.0-20180327235837-af21d9c3145e/go.mod h1:0AA//k/eakGydO4jKRoRL2j92ZKSzTgj9tclaCrvXHk=
-github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c h1:964Od4U6p2jUkFxvCydnIczKteheJEzHRToSGK3Bnlw=
 github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
 github.com/google/btree v1.0.0 h1:0udJVsspx3VBr5FwtLhQQtuAsVc79tTq0ocGIPAU6qo=
 github.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
 github.com/google/cadvisor v0.37.3/go.mod h1:BalYQhwl2UV8lpB3oFssiaW8Uj6sqfFDxw5nEs9sBgU=
 github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=
-github.com/google/go-cmp v0.3.0 h1:crn/baboCvb5fXaQ0IJ1SGTsTVrWpDsCWC8EGETZijY=
 github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
 github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
 github.com/google/go-cmp v0.4.0 h1:xsAVV57WRhGj6kEIi8ReJzQlHHqcBYCElAvkovg3B/4=
 github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
-github.com/google/gofuzz v1.0.0 h1:A8PeW59pxE9IoFRqBp37U+mSNaQoZ46F1f0f863XSXw=
 github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
 github.com/google/gofuzz v1.1.0 h1:Hsa8mG0dQ46ij8Sl2AYJDUv1oA9/d6Vk+3LG99Oe02g=
 github.com/google/gofuzz v1.1.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
@@ -328,7 +318,6 @@ github.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439af/go.mod h1:Nht
 github.com/jonboulle/clockwork v0.1.0 h1:VKV+ZcuP6l3yW9doeqz6ziZGgcynBVQO+obU0+0hcPo=
 github.com/jonboulle/clockwork v0.1.0/go.mod h1:Ii8DK3G1RaLaWxj9trq07+26W01tbo22gdxWY5EU2bo=
 github.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=
-github.com/json-iterator/go v1.1.7 h1:KfgG9LzI+pYjr4xvmz/5H4FXjokeP+rlHLhv3iH62Fo=
 github.com/json-iterator/go v1.1.7/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=
 github.com/json-iterator/go v1.1.10 h1:Kz6Cvnvv2wGdaG/V8yMvfkmNiXq9Ya2KUv4rouJJr68=
 github.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=
@@ -342,13 +331,11 @@ github.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvW
 github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=
 github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
 github.com/klauspost/cpuid v1.2.0/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=
-github.com/konsorten/go-windows-terminal-sequences v1.0.1 h1:mweAR1A6xJ3oS2pRaGiHgQ4OO8tzTaLawm8vnODuwDk=
 github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
 github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
 github.com/konsorten/go-windows-terminal-sequences v1.0.3 h1:CE8S1cTafDpPvMhIxNJKvHsGVBgn1xWYf1NbHQhywc8=
 github.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
 github.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=
-github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=
 github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
 github.com/kr/pretty v0.2.0 h1:s5hAObm+yFO5uHYt5dYjxi2rXrsnmRpJx4OYvIWUaQs=
 github.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
@@ -377,7 +364,6 @@ github.com/marten-seemann/qtls v0.2.3/go.mod h1:xzjG7avBwGGbdZ8dTGxlBnLArsVKLvwm
 github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=
 github.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=
 github.com/mattn/go-runewidth v0.0.2/go.mod h1:LwmH8dsx7+W8Uxz3IHJYH5QSwggIsqBzpuz5H//U1FU=
-github.com/matttproud/golang_protobuf_extensions v1.0.1 h1:4hp9jkHxhMHkqkrB3Ix0jegS5sx/RkqARlsWZ6pIwiU=
 github.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=
 github.com/matttproud/golang_protobuf_extensions v1.0.2-0.20181231171920-c182affec369 h1:I0XW9+e1XWDxdcEniV4rQAIOPUGDq67JSCiRCgGCZLI=
 github.com/matttproud/golang_protobuf_extensions v1.0.2-0.20181231171920-c182affec369/go.mod h1:BSXmuO+STAnVfrANrmjBb36TMTDstsz7MSK+HVaYKv4=
@@ -431,12 +417,10 @@ github.com/opencontainers/runtime-spec v1.0.2/go.mod h1:jwyrGlmzljRJv/Fgzds9SsS/
 github.com/opencontainers/runtime-spec v1.0.3-0.20200520003142-237cc4f519e2/go.mod h1:jwyrGlmzljRJv/Fgzds9SsS/C5hL+LL3ko9hs6T5lQ0=
 github.com/opencontainers/selinux v1.5.1/go.mod h1:yTcKuYAh6R95iDpefGLQaPaRwJFwyzAJufJyiTt7s0g=
 github.com/opencontainers/selinux v1.5.2/go.mod h1:yTcKuYAh6R95iDpefGLQaPaRwJFwyzAJufJyiTt7s0g=
-github.com/pborman/uuid v1.2.0 h1:J7Q5mO4ysT1dv8hyrUGHb9+ooztCXu1D8MY8DZYsu3g=
 github.com/pborman/uuid v1.2.0/go.mod h1:X/NO0urCmaxf9VXbdlT7C2Yzkj2IKimNn4k+gtPdI/k=
 github.com/pelletier/go-toml v1.2.0/go.mod h1:5z9KED0ma1S8pY6P1sdut58dfprrGBbd/94hg7ilaic=
 github.com/peterbourgon/diskv v2.0.1+incompatible/go.mod h1:uqqh8zWWbv1HBMNONnaR/tNboyR3/BZd58JJSHlUSCU=
 github.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
-github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=
 github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
 github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
 github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
@@ -445,11 +429,9 @@ github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZN
 github.com/pquerna/cachecontrol v0.0.0-20171018203845-0dec1b30a021/go.mod h1:prYjPmNq4d1NPVmpShWobRqXY3q7Vp+80DqgxxUrUIA=
 github.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=
 github.com/prometheus/client_golang v0.9.3/go.mod h1:/TN21ttK/J9q6uSwhBd54HahCDft0ttaMvbicHlPoso=
-github.com/prometheus/client_golang v1.0.0 h1:vrDKnkGzuGvhNAL56c7DBz29ZL+KxnoR0x7enabFceM=
 github.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=
 github.com/prometheus/client_golang v1.7.1 h1:NTGy1Ja9pByO+xAeH/qiWnLrKtr3hJPNjaVUwnjpdpA=
 github.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=
-github.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910 h1:idejC8f05m9MGOsuEi1ATq9shN03HrxNkD/luQvxCv8=
 github.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=
 github.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
 github.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
@@ -457,14 +439,12 @@ github.com/prometheus/client_model v0.2.0 h1:uq5h0d+GuxiXLJLNABMgp2qUWDPiLvgCzz2
 github.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
 github.com/prometheus/common v0.0.0-20181113130724-41aa239b4cce/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=
 github.com/prometheus/common v0.4.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
-github.com/prometheus/common v0.4.1 h1:K0MGApIoQvMw27RTdJkPbr3JZ7DNbtxQNyi5STVM6Kw=
 github.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=
 github.com/prometheus/common v0.10.0 h1:RyRA7RzGXQZiW+tGMr7sxa85G1z0yOpM1qq5c8lNawc=
 github.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=
 github.com/prometheus/procfs v0.0.0-20180125133057-cb4147076ac7/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=
 github.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=
 github.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=
-github.com/prometheus/procfs v0.0.2 h1:6LJUbpNm42llc4HRCuvApCSWB/WfhuNo9K98Q9sNGfs=
 github.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=
 github.com/prometheus/procfs v0.1.3 h1:F0+tqvhOksq22sc6iCHF5WGlWjdwj92p0udFh1VFBS8=
 github.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=
@@ -484,7 +464,6 @@ github.com/sergi/go-diff v1.0.0/go.mod h1:0CfEIISq7TuYL3j771MWULgwwjU+GofnZX9QAm
 github.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=
 github.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=
 github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=
-github.com/sirupsen/logrus v1.4.2 h1:SPIRibHv4MatM3XXNO2BJeFLZwZ2LvZgfQ5+UNI2im4=
 github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=
 github.com/sirupsen/logrus v1.6.0 h1:UBcNElsrwanuuMsnGSlYmtmgbb23qDR5dG+6X6Oo89I=
 github.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=
@@ -514,14 +493,12 @@ github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+
 github.com/stretchr/objx v0.2.0 h1:Hbg2NidpLE8veEBkEZTL3CvlkUIVzuU9jDplZO54c48=
 github.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=
 github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=
-github.com/stretchr/testify v1.3.0 h1:TivCn/peBQ7UY8ooIcPgZFpTNSz0Q2U6UrFlUfqbe0Q=
 github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
 github.com/stretchr/testify v1.4.0 h1:2E4SXV/wtOkTonXsotYi4li6zVWxYlZuYNCXe9XRJyk=
 github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=
 github.com/syndtr/gocapability v0.0.0-20180916011248-d98352740cb2/go.mod h1:hkRG7XYTFWNJGYcbNJQlaLq0fg1yr4J4t/NcTQtrfww=
 github.com/thecodeteam/goscaleio v0.1.0/go.mod h1:68sdkZAsK8bvEwBlbQnlLS+xU+hvLYM/iQ8KXej1AwM=
 github.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy+sBenx6DDtFZJxhVysOjyk=
-github.com/tmc/grpc-websocket-proxy v0.0.0-20170815181823-89b8d40f7ca8 h1:ndzgwNDnKIqyCvHTXaCqh9KlOWKvBry6nuXMJmonVsE=
 github.com/tmc/grpc-websocket-proxy v0.0.0-20170815181823-89b8d40f7ca8/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=
 github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5 h1:LnC5Kc/wtumK+WB441p7ynQJzVuNRJiqddSIE3IlSEQ=
 github.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=
@@ -541,7 +518,6 @@ github.com/xlab/handysort v0.0.0-20150421192137-fb3537ed64a1/go.mod h1:QcJo0QPSf
 github.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77/go.mod h1:aYKd//L2LvnjZzWKhF00oedf4jCCReLcmhLdhm1A27Q=
 github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
 go.etcd.io/bbolt v1.3.2/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL+uU=
-go.etcd.io/bbolt v1.3.3 h1:MUGmc65QhB3pIlaQ5bB4LwqSj6GIonVJXpZiaKNyaKk=
 go.etcd.io/bbolt v1.3.3/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL+uU=
 go.etcd.io/bbolt v1.3.5 h1:XAzx9gjCb0Rxj7EoqcClPD1d5ZBxZJk0jbuoPHenBt0=
 go.etcd.io/bbolt v1.3.5/go.mod h1:G5EMThwa9y8QZGBClrRx5EY+Yw9kAhnjy3bSjsnlVTQ=
@@ -553,7 +529,6 @@ go.mongodb.org/mongo-driver v1.1.2/go.mod h1:u7ryQJ+DOzQmeO7zB6MHyr8jkEQvC8vH7qL
 go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=
 go.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=
 go.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=
-go.uber.org/atomic v1.3.2 h1:2Oa65PReHzfn29GpvgsYwloV9AVFHPDk8tYxt2c2tr4=
 go.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
 go.uber.org/atomic v1.4.0 h1:cxzIVoETapQEqDhQu3QfnvXAV4AlzcvUCxkVUFw3+EU=
 go.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
@@ -591,7 +566,6 @@ golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTk
 golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=
 golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
 golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
-golang.org/x/lint v0.0.0-20190409202823-959b441ac422 h1:QzoH/1pFpZguR8NrRHLcO6jKqfv2zpuSqZLgdm7ZmjI=
 golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
 golang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
 golang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=
@@ -623,7 +597,6 @@ golang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLL
 golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20190813141303-74dc4d7220e7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
-golang.org/x/net v0.0.0-20191004110552-13f9640d40b9 h1:rjwSpXsdiK0dV8/Naq3kAw9ymfAeJIyd0upUIElB+lI=
 golang.org/x/net v0.0.0-20191004110552-13f9640d40b9/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
@@ -633,7 +606,6 @@ golang.org/x/net v0.0.0-20201110031124-69a78807bb2b h1:uwuIcX0g4Yl1NC5XAz37xsr2l
 golang.org/x/net v0.0.0-20201110031124-69a78807bb2b/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
 golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=
 golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
-golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45 h1:SVwTIAaPC2U/AvvLNZ2a7OVsmBpC8L5BlwK1whH3hm0=
 golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
 golang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6 h1:pE8b58s1HRDMi8RDc79m0HISf9D4TzseP40cEA6IGfs=
 golang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
@@ -665,7 +637,6 @@ golang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7w
 golang.org/x/sys v0.0.0-20190826190057-c7b8b68b1456/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20190916202348-b4ddaad3f8a3/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20191005200804-aed5e4c7ecf9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
-golang.org/x/sys v0.0.0-20191022100944-742c48ecaeb7 h1:HmbHVPwrPEKPGLAcHSrMe6+hqSUlvZU0rab6x5EXfGU=
 golang.org/x/sys v0.0.0-20191022100944-742c48ecaeb7/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20191115151921-52ab43148777/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20191120155948-bd437916bb0e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
@@ -686,14 +657,11 @@ golang.org/x/sys v0.0.0-20201112073958-5cba982894dd h1:5CtCZbICpIOFdgO940moixOPj
 golang.org/x/sys v0.0.0-20201112073958-5cba982894dd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
 golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
-golang.org/x/text v0.3.2 h1:tW2bmiBqwgJj/UpqtC8EpXEZVYOwU0yG4iWbprSVAcs=
 golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=
 golang.org/x/text v0.3.3 h1:cokOdA+Jmi5PJGXLlLllQSgYigAEfHXJAERHVMaCc2k=
 golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
 golang.org/x/time v0.0.0-20180412165947-fbb02b2291d2/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
-golang.org/x/time v0.0.0-20181108054448-85acf8d2951c h1:fqgJT0MGcGpPgpWU7VRdRjuArfcOvC4AoJmILihzhDg=
 golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
-golang.org/x/time v0.0.0-20190308202827-9d24e82272b4 h1:SvFZT6jyqRaOeXpc5h/JSfZenJ2O330aBsf7JfSUXmQ=
 golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
 golang.org/x/time v0.0.0-20191024005414-555d28b269f0 h1:/5xXl8Y5W96D+TtHSlonuFqGHIWVuyCkGJLwGh9JJFs=
 golang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
@@ -745,7 +713,6 @@ google.golang.org/api v0.15.1-0.20200106000736-b8fc810ca6b5/go.mod h1:iLdEw5Ide6
 google.golang.org/api v0.15.1/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=
 google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=
 google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=
-google.golang.org/appengine v1.5.0 h1:KxkO13IPW4Lslp2bz+KHP2E3gtFlrIGNThxkZQ3g+4c=
 google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=
 google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=
 google.golang.org/appengine v1.6.5 h1:tycE03LOZYQNhDpS27tcQdAzLCVMaj7QT2SXxebnpCM=
@@ -754,10 +721,8 @@ google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoA
 google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
 google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
 google.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
-google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873 h1:nfPFGzJkUDX6uBmpN/pSw7MbOAWegH5QDQuoXFHedLg=
 google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
 google.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=
-google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55 h1:gSJIx1SDwno+2ElGhA4+qG2zF97qiUzTM+rQ0klBOcE=
 google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=
 google.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=
 google.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
@@ -768,10 +733,8 @@ google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZi
 google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=
 google.golang.org/grpc v1.21.0/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=
 google.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=
-google.golang.org/grpc v1.23.0 h1:AzbTB6ux+okLTzP8Ru1Xs41C303zdcfEht7MQnYJt5A=
 google.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=
 google.golang.org/grpc v1.23.1/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=
-google.golang.org/grpc v1.26.0 h1:2dTRdpdFEEhJYQD8EMLB61nnrzSCTbG38PhqdhvOltg=
 google.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
 google.golang.org/grpc v1.27.0 h1:rRYRFMVgRv6E0D70Skyfsr28tDXIuuPZyWGMPdMcnXg=
 google.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
@@ -787,7 +750,6 @@ google.golang.org/protobuf v1.24.0 h1:UhZDfRO8JRQru4/+LlLE0BRKGF8L+PICnvYZmx/fEG
 google.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=
 gopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=
 gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
-gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127 h1:qIbj1fsPNlZgppZ+VLlY7N33q108Sa+fhmuc+sWQYwY=
 gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
 gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=
 gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
@@ -877,10 +839,7 @@ sigs.k8s.io/apiserver-network-proxy/konnectivity-client v0.0.9/go.mod h1:dzAXnQb
 sigs.k8s.io/kustomize v2.0.3+incompatible/go.mod h1:MkjgH3RdOWrievjo6c9T245dYlB5QeXV4WCbnt/PEpU=
 sigs.k8s.io/structured-merge-diff/v4 v4.0.1 h1:YXTMot5Qz/X1iBRJhAt+vI+HVttY0WkSqqhKxQ0xVbA=
 sigs.k8s.io/structured-merge-diff/v4 v4.0.1/go.mod h1:bJZC9H9iH24zzfZ/41RGcq60oK1F7G282QMXDPYydCw=
-sigs.k8s.io/yaml v1.1.0 h1:4A07+ZFc2wgJwo8YNlQpr1rVlgUDlxXHhPJciaPY5gs=
 sigs.k8s.io/yaml v1.1.0/go.mod h1:UJmg0vDUVViEyp3mgSv9WPwZCDxu4rQW1olrI1uml+o=
 sigs.k8s.io/yaml v1.2.0 h1:kr/MCeFWJWTwyaHoR9c8EjH9OumOmoF9YGiZd7lFm/Q=
 sigs.k8s.io/yaml v1.2.0/go.mod h1:yfXDCHCao9+ENCvLSE62v9VSji2MKu5jeNfTrofGhJc=
 vbom.ml/util v0.0.0-20160121211510-db5cfe13f5cc/go.mod h1:so/NYdZXCz+E3ZpW0uAoCj6uzU2+8OWDFv/HxUSs7kI=
-volcano.sh/apis v0.0.0-20210518032656-21e2239e42bc h1:anGQGhvjBeBe0HAC3zomoPEt/XO7myMg0XBzdf0kJCg=
-volcano.sh/apis v0.0.0-20210518032656-21e2239e42bc/go.mod h1:UaeJ/s5Hyd+ZhFLc+Kw9YlgM8gRZ/5OzXqHa0yKOoXY=
diff --git a/installer/dockerfile/controller-manager/Dockerfile b/installer/dockerfile/controller-manager/Dockerfile
index 63ea0fd..f6b41c0 100644
--- a/installer/dockerfile/controller-manager/Dockerfile
+++ b/installer/dockerfile/controller-manager/Dockerfile
@@ -13,7 +13,7 @@
 # limitations under the License.
 
 
-FROM alpine:latest
+FROM iregistry.baidu-int.com/bmlc/alpine:latest
 
-ADD vc-controller-manager /vc-controller-manager
+ADD output/bin/vc-controller-manager /vc-controller-manager
 ENTRYPOINT ["/vc-controller-manager"]
diff --git a/installer/dockerfile/scheduler/Dockerfile b/installer/dockerfile/scheduler/Dockerfile
index 0581191..14a74f7 100644
--- a/installer/dockerfile/scheduler/Dockerfile
+++ b/installer/dockerfile/scheduler/Dockerfile
@@ -13,7 +13,7 @@
 # limitations under the License.
 
 
-FROM alpine:latest
+FROM iregistry.baidu-int.com/bmlc/alpine:latest
 
-ADD vc-scheduler /vc-scheduler
+ADD output/bin/vc-scheduler /vc-scheduler
 ENTRYPOINT ["/vc-scheduler"]
diff --git a/installer/dockerfile/webhook-manager/Dockerfile b/installer/dockerfile/webhook-manager/Dockerfile
index 38e2e2d..ba48d3c 100644
--- a/installer/dockerfile/webhook-manager/Dockerfile
+++ b/installer/dockerfile/webhook-manager/Dockerfile
@@ -15,8 +15,8 @@
 
 # The base image is created via `Dockerfile.base`, the base image is cached
 # since the required packages change very rarely.
-FROM volcanosh/vc-webhook-manager-base:1.3.1
+FROM iregistry.baidu-int.com/bmlc/vc-webhook-manager-base:1.3.1
 
-ADD vc-webhook-manager /vc-webhook-manager
-ADD gen-admission-secret.sh /gen-admission-secret.sh
+ADD output/bin/vc-webhook-manager /vc-webhook-manager
+ADD output/bin/gen-admission-secret.sh /gen-admission-secret.sh
 ENTRYPOINT ["/vc-webhook-manager"]
diff --git a/pkg/common/action.go b/pkg/common/action.go
new file mode 100644
index 0000000..5c72254
--- /dev/null
+++ b/pkg/common/action.go
@@ -0,0 +1,32 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package common
+
+import (
+	busv1alpha1 "volcano.sh/apis/pkg/apis/bus/v1alpha1"
+)
+
+const (
+	// SyncEQuotaAction is the action to sync ElasticResourceQuota.
+	SyncEQuotaAction busv1alpha1.Action = "SyncElasticResourceQuota"
+
+	// SyncEQuotaUsedAction is the action to sync status.used of ElasticResourceQuota.
+	SyncEQuotaUsedAction busv1alpha1.Action = "SyncElasticResourceQuotaUsed"
+
+	// RemoveEQuotaAction is the action to remove ElasticResourceQuota.
+	RemoveEQuotaAction busv1alpha1.Action = "RemoveElasticResourceQuota"
+)
diff --git a/pkg/common/device_info.go b/pkg/common/device_info.go
new file mode 100644
index 0000000..691015d
--- /dev/null
+++ b/pkg/common/device_info.go
@@ -0,0 +1,72 @@
+/*
+Copyright 2020 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package common
+
+import (
+	v1 "k8s.io/api/core/v1"
+)
+
+// GPUDevice include gpu id, memory and the pods that are sharing it.
+type GPUDevice struct {
+	// GPU ID
+	ID int
+	// The pods that are sharing this GPU
+	PodMap map[string]*v1.Pod
+	// memory per card
+	Memory uint
+}
+
+// NewGPUDevice creates a device
+func NewGPUDevice(id int, mem uint) *GPUDevice {
+	return &GPUDevice{
+		ID:     id,
+		Memory: mem,
+		PodMap: map[string]*v1.Pod{},
+	}
+}
+
+// getUsedGPUMemory calculates the used memory of the device.
+func (g *GPUDevice) getUsedGPUMemory() uint {
+	res := uint(0)
+	for _, pod := range g.PodMap {
+		if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
+			continue
+		} else {
+			gpuRequest := GetGPUResourceOfPod(pod)
+			res += gpuRequest
+		}
+	}
+	return res
+}
+
+// GetGPUResourceOfPod returns the GPU resource required by the pod.
+func GetGPUResourceOfPod(pod *v1.Pod) uint {
+	var mem uint
+	for _, container := range pod.Spec.Containers {
+		mem += getGPUResourceOfContainer(&container)
+	}
+	return mem
+}
+
+// getGPUResourceOfPod returns the GPU resource required by the container.
+func getGPUResourceOfContainer(container *v1.Container) uint {
+	var mem uint
+	if val, ok := container.Resources.Limits[VolcanoGPUResource]; ok {
+		mem = uint(val.Value())
+	}
+	return mem
+}
diff --git a/pkg/common/node_info.go b/pkg/common/node_info.go
new file mode 100644
index 0000000..839be24
--- /dev/null
+++ b/pkg/common/node_info.go
@@ -0,0 +1,216 @@
+/*
+Copyright 2017 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package common
+
+import (
+	"fmt"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/klog"
+)
+
+const (
+	// VolcanoGPUResource extended gpu resource
+	VolcanoGPUResource = "volcano.sh/gpu-memory"
+	// VolcanoGPUNumber virtual GPU card number
+	VolcanoGPUNumber = "volcano.sh/gpu-number"
+)
+
+// NodeInfo is node level aggregated information.
+type NodeInfo struct {
+	Name string
+	Node *v1.Node
+
+	// The state of node
+	State NodeState
+
+	// The releasing resource on that node
+	Releasing *Resource
+	// The pipelined resource on that node
+	Pipelined *Resource
+	// The idle resource on that node
+	Idle *Resource
+
+	Capability *Resource
+	Allocatable *Resource
+
+	RevocableZone string
+
+	// Used to store custom information
+	Others     map[string]interface{}
+	GPUDevices map[int]*GPUDevice
+}
+
+// FutureIdle returns resources that will be idle in the future:
+//
+// That is current idle resources plus released resources minus pipelined resources.
+func (ni *NodeInfo) FutureIdle() *Resource {
+	return ni.Idle.Clone().Add(ni.Releasing).Sub(ni.Pipelined)
+}
+
+// NodeState defines the current state of node.
+type NodeState struct {
+	Phase  NodePhase
+	Reason string
+}
+
+// NewNodeInfo is used to create new nodeInfo object
+func NewNodeInfo(node *v1.Node) *NodeInfo {
+	nodeInfo := &NodeInfo{
+		Releasing: EmptyResource(),
+		Pipelined: EmptyResource(),
+		Idle:      EmptyResource(),
+
+		Capability: EmptyResource(),
+		Allocatable: EmptyResource(),
+
+		GPUDevices: make(map[int]*GPUDevice),
+	}
+
+	if node != nil {
+		nodeInfo.Name = node.Name
+		nodeInfo.Node = node
+		nodeInfo.Idle = NewResource(node.Status.Allocatable)
+		nodeInfo.Capability = NewResource(node.Status.Capacity)
+		nodeInfo.Allocatable = NewResource(node.Status.Allocatable)
+	}
+	nodeInfo.setNodeGPUInfo(node)
+	nodeInfo.setNodeState(node)
+
+	return nodeInfo
+}
+
+// Clone used to clone nodeInfo Object
+func (ni *NodeInfo) Clone() *NodeInfo {
+	res := NewNodeInfo(ni.Node)
+	return res
+}
+
+// Ready returns whether node is ready for scheduling
+func (ni *NodeInfo) Ready() bool {
+	return ni.State.Phase == Ready
+}
+
+func (ni *NodeInfo) setNodeState(node *v1.Node) {
+	// If node is nil, the node is un-initialized in cache
+	if node == nil {
+		ni.State = NodeState{
+			Phase:  NotReady,
+			Reason: "UnInitialized",
+		}
+		return
+	}
+
+	// If node not ready, e.g. power off
+	for _, cond := range node.Status.Conditions {
+		if cond.Type == v1.NodeReady && cond.Status != v1.ConditionTrue {
+			ni.State = NodeState{
+				Phase:  NotReady,
+				Reason: "NotReady",
+			}
+			return
+		}
+	}
+
+	// Node is ready (ignore node conditions because of taint/toleration)
+	ni.State = NodeState{
+		Phase:  Ready,
+		Reason: "",
+	}
+}
+
+func (ni *NodeInfo) setNodeGPUInfo(node *v1.Node) {
+	if node == nil {
+		return
+	}
+	memory, ok := node.Status.Capacity[VolcanoGPUResource]
+	if !ok {
+		return
+	}
+	totalMemory := memory.Value()
+
+	res, ok := node.Status.Capacity[VolcanoGPUNumber]
+	if !ok {
+		return
+	}
+	gpuNumber := res.Value()
+	if gpuNumber == 0 {
+		klog.Warningf("invalid %s=%s", VolcanoGPUNumber, res.String())
+		return
+	}
+
+	memoryPerCard := uint(totalMemory / gpuNumber)
+	for i := 0; i < int(gpuNumber); i++ {
+		ni.GPUDevices[i] = NewGPUDevice(i, memoryPerCard)
+	}
+}
+
+// SetNode sets kubernetes node object to nodeInfo object
+func (ni *NodeInfo) SetNode(node *v1.Node) {
+	ni.setNodeState(node)
+	ni.setNodeGPUInfo(node)
+
+	if !ni.Ready() {
+		klog.Warningf("Failed to set node info, phase: %s, reason: %s",
+			ni.State.Phase, ni.State.Reason)
+		return
+	}
+
+	ni.Name = node.Name
+	ni.Node = node
+
+	ni.Capability = NewResource(node.Status.Capacity)
+	ni.Allocatable = NewResource(node.Status.Allocatable)
+	ni.Releasing = EmptyResource()
+	ni.Pipelined = EmptyResource()
+	ni.Idle = NewResource(node.Status.Allocatable)
+}
+
+// String returns nodeInfo details in string format
+func (ni NodeInfo) String() string {
+	return fmt.Sprintf("Node (%s): idle <%v>, releasing <%v>, state <phase %s, reaseon %s>, taints <%v>",
+		ni.Name, ni.Idle, ni.Releasing, ni.State.Phase, ni.State.Reason, ni.Node.Spec.Taints)
+}
+
+// GetDevicesIdleGPUMemory returns all the idle GPU memory by gpu card.
+func (ni *NodeInfo) GetDevicesIdleGPUMemory() map[int]uint {
+	devicesAllGPUMemory := ni.getDevicesAllGPUMemory()
+	devicesUsedGPUMemory := ni.getDevicesUsedGPUMemory()
+	res := map[int]uint{}
+	for id, allMemory := range devicesAllGPUMemory {
+		if usedMemory, found := devicesUsedGPUMemory[id]; found {
+			res[id] = allMemory - usedMemory
+		}
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesUsedGPUMemory() map[int]uint {
+	res := map[int]uint{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.getUsedGPUMemory()
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesAllGPUMemory() map[int]uint {
+	res := map[int]uint{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.Memory
+	}
+	return res
+}
diff --git a/pkg/common/resource_info.go b/pkg/common/resource_info.go
new file mode 100644
index 0000000..f433391
--- /dev/null
+++ b/pkg/common/resource_info.go
@@ -0,0 +1,479 @@
+/*
+Copyright 2017 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package common
+
+import (
+	"fmt"
+	"math"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
+	v1helper "k8s.io/kubernetes/pkg/apis/core/v1/helper"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/scheduler/util/assert"
+)
+
+// Resource struct defines all the resource type
+type Resource struct {
+	MilliCPU float64
+	Memory   float64
+
+	// ScalarResources
+	ScalarResources map[v1.ResourceName]float64
+
+	// MaxTaskNum is only used by predicates; it should NOT
+	// be accounted in other operators, e.g. Add.
+}
+
+const (
+	// GPUResourceName need to follow https://github.com/NVIDIA/k8s-device-plugin/blob/66a35b71ac4b5cbfb04714678b548bd77e5ba719/server.go#L20
+	GPUResourceName = "nvidia.com/gpu"
+)
+
+// EmptyResource creates a empty resource object and returns
+func EmptyResource() *Resource {
+	return &Resource{}
+}
+
+// Clone is used to clone a resource type
+func (r *Resource) Clone() *Resource {
+	clone := &Resource{
+		MilliCPU: r.MilliCPU,
+		Memory:   r.Memory,
+	}
+
+	if r.ScalarResources != nil {
+		clone.ScalarResources = make(map[v1.ResourceName]float64)
+		for k, v := range r.ScalarResources {
+			clone.ScalarResources[k] = v
+		}
+	}
+
+	return clone
+}
+
+var minMilliCPU float64 = 10
+var minMilliScalarResources float64 = 1
+var minMemory float64 = 1
+
+// NewResource create a new resource object from resource list
+func NewResource(rl v1.ResourceList) *Resource {
+	r := EmptyResource()
+	for rName, rQuant := range rl {
+		switch rName {
+		case v1.ResourceCPU:
+			r.MilliCPU += float64(rQuant.MilliValue())
+		case v1.ResourceMemory:
+			r.Memory += float64(rQuant.Value())
+		default:
+			//NOTE: When converting this back to k8s resource, we need record the format as well as / 1000
+			if v1helper.IsScalarResourceName(rName) {
+				r.AddScalar(rName, float64(rQuant.Value()))
+			}
+		}
+	}
+	return r
+}
+
+// NewResource create a new resource object from resource list
+func NewResourceList(r *Resource) v1.ResourceList {
+	resourceList := v1.ResourceList{}
+	cPUQuantity := resource.NewMilliQuantity(int64(r.MilliCPU), resource.BinarySI)
+	memoryQuantity := resource.NewQuantity(int64(r.Memory), resource.BinarySI)
+
+	resourceList[v1.ResourceCPU] = *cPUQuantity
+	resourceList[v1.ResourceMemory] = *memoryQuantity
+
+	for resourceName, RQuant := range r.ScalarResources {
+		quantity := resource.NewQuantity(int64(RQuant), resource.BinarySI)
+		resourceList[resourceName] = *quantity
+	}
+	return resourceList
+}
+
+// IsEmpty returns bool after checking any of resource is less than min possible value
+func (r *Resource) IsEmpty() bool {
+	if !(r.MilliCPU < minMilliCPU && r.Memory < minMemory) {
+		return false
+	}
+
+	for _, rQuant := range r.ScalarResources {
+		if rQuant >= minMilliScalarResources {
+			return false
+		}
+	}
+
+	return true
+}
+
+// IsLessThanMinimum returns bool after checking all resource if less than zero
+func (r *Resource) IsLessThanMinimum() bool {
+	if r.MilliCPU < 0 || r.Memory < 0 {
+		return true
+	}
+
+	for _, rQuant := range r.ScalarResources {
+		if rQuant < 0 {
+			return true
+		}
+	}
+
+	return false
+}
+
+// IsZero checks whether that resource is less than min possible value
+func (r *Resource) IsZero(rn v1.ResourceName) bool {
+	switch rn {
+	case v1.ResourceCPU:
+		return r.MilliCPU < minMilliCPU
+	case v1.ResourceMemory:
+		return r.Memory < minMemory
+	default:
+		if r.ScalarResources == nil {
+			return true
+		}
+
+		_, found := r.ScalarResources[rn]
+		assert.Assertf(found, "unknown resource %s", rn)
+
+		return r.ScalarResources[rn] < minMilliScalarResources
+	}
+}
+
+// Add is used to add the two resources
+func (r *Resource) Add(rr *Resource) *Resource {
+	r.MilliCPU += rr.MilliCPU
+	r.Memory += rr.Memory
+
+	if r.ScalarResources == nil {
+		// init ScalarResources
+		r.ScalarResources = map[v1.ResourceName]float64{}
+	}
+
+	// add rr's resources to r
+	for rName, rQuant := range rr.ScalarResources {
+		if schedulingv1beta1.ResourceFieldFilter[rName.String()] || rName == "cpu" || rName == "memory" {
+			continue
+		}
+		r.ScalarResources[rName] += rQuant
+	}
+	return r
+}
+
+// Scale updates resource to the provided scale
+func (r *Resource) Scale(scale float64) *Resource {
+	r.MilliCPU *= scale
+	r.Memory *= scale
+	for rName, rQuant := range r.ScalarResources {
+		r.ScalarResources[rName] = rQuant * scale
+	}
+	return r
+}
+
+//Sub subtracts two Resource objects.
+func (r *Resource) Sub(rr *Resource) *Resource {
+	assert.Assertf(rr.LessEqual(r), "resource is not sufficient to do operation: <%v> sub <%v>", r, rr)
+
+	r.MilliCPU -= rr.MilliCPU
+	r.Memory -= rr.Memory
+
+	for rrName, rrQuant := range rr.ScalarResources {
+		if r.ScalarResources == nil {
+			return r
+		}
+		r.ScalarResources[rrName] -= rrQuant
+	}
+
+	return r
+}
+
+// SetMaxResource compares with ResourceList and takes max value for each Resource.
+func (r *Resource) SetMaxResource(rr *Resource) {
+	if r == nil || rr == nil {
+		return
+	}
+
+	if rr.MilliCPU > r.MilliCPU {
+		r.MilliCPU = rr.MilliCPU
+	}
+	if rr.Memory > r.Memory {
+		r.Memory = rr.Memory
+	}
+
+	for rrName, rrQuant := range rr.ScalarResources {
+		if r.ScalarResources == nil {
+			r.ScalarResources = make(map[v1.ResourceName]float64)
+			for k, v := range rr.ScalarResources {
+				r.ScalarResources[k] = v
+			}
+			return
+		}
+
+		if rrQuant > r.ScalarResources[rrName] {
+			r.ScalarResources[rrName] = rrQuant
+		}
+	}
+}
+
+//FitDelta Computes the delta between a resource object representing available
+//resources an operand representing resources being requested.  Any
+//field that is less than 0 after the operation represents an
+//insufficient resource.
+func (r *Resource) FitDelta(rr *Resource) *Resource {
+	if rr.MilliCPU > 0 {
+		r.MilliCPU -= rr.MilliCPU + minMilliCPU
+	}
+
+	if rr.Memory > 0 {
+		r.Memory -= rr.Memory + minMemory
+	}
+
+	for rrName, rrQuant := range rr.ScalarResources {
+		if r.ScalarResources == nil {
+			r.ScalarResources = map[v1.ResourceName]float64{}
+		}
+
+		if rrQuant > 0 {
+			r.ScalarResources[rrName] -= rrQuant + minMilliScalarResources
+		}
+	}
+
+	return r
+}
+
+// Multi multiples the resource with ratio provided
+func (r *Resource) Multi(ratio float64) *Resource {
+	r.MilliCPU *= ratio
+	r.Memory *= ratio
+	for rName, rQuant := range r.ScalarResources {
+		r.ScalarResources[rName] = rQuant * ratio
+	}
+	return r
+}
+
+// Less checks whether a resource is less than other
+func (r *Resource) Less(rr *Resource) bool {
+	lessFunc := func(l, r float64) bool {
+		return l < r
+	}
+
+	if !lessFunc(r.MilliCPU, rr.MilliCPU) {
+		return false
+	}
+	if !lessFunc(r.Memory, rr.Memory) {
+		return false
+	}
+
+	if r.ScalarResources == nil {
+		if rr.ScalarResources != nil {
+			for _, rrQuant := range rr.ScalarResources {
+				if rrQuant <= minMilliScalarResources {
+					return false
+				}
+			}
+		}
+		return true
+	}
+
+	if rr.ScalarResources == nil {
+		return false
+	}
+
+	for rName, rQuant := range r.ScalarResources {
+		rrQuant := rr.ScalarResources[rName]
+		if !lessFunc(rQuant, rrQuant) {
+			return false
+		}
+	}
+
+	return true
+}
+
+// LessEqualStrict checks whether a resource is less or equal than other
+func (r *Resource) LessEqualStrict(rr *Resource) bool {
+	lessFunc := func(l, r float64) bool {
+		return l <= r
+	}
+
+	if !lessFunc(r.MilliCPU, rr.MilliCPU) {
+		return false
+	}
+	if !lessFunc(r.Memory, rr.Memory) {
+		return false
+	}
+
+	for rName, rQuant := range r.ScalarResources {
+		if !lessFunc(rQuant, rr.ScalarResources[rName]) {
+			return false
+		}
+	}
+
+	return true
+}
+
+// LessEqual checks whether a resource is less than other resource
+func (r *Resource) LessEqual(rr *Resource) bool {
+	lessEqualFunc := func(l, r, diff float64) bool {
+		if l < r || math.Abs(l-r) < diff {
+			return true
+		}
+		return false
+	}
+
+	if !lessEqualFunc(r.MilliCPU, rr.MilliCPU, minMilliCPU) {
+		return false
+	}
+	if !lessEqualFunc(r.Memory, rr.Memory, minMemory) {
+		return false
+	}
+
+	if r.ScalarResources == nil {
+		return true
+	}
+
+	for rName, rQuant := range r.ScalarResources {
+		if rQuant <= minMilliScalarResources {
+			continue
+		}
+		if rr.ScalarResources == nil {
+			return false
+		}
+
+		rrQuant := rr.ScalarResources[rName]
+		if !lessEqualFunc(rQuant, rrQuant, minMilliScalarResources) {
+			return false
+		}
+	}
+
+	return true
+}
+
+// Diff calculate the difference between two resource
+func (r *Resource) Diff(rr *Resource) (*Resource, *Resource) {
+	increasedVal := EmptyResource()
+	decreasedVal := EmptyResource()
+	if r.MilliCPU > rr.MilliCPU {
+		increasedVal.MilliCPU += r.MilliCPU - rr.MilliCPU
+	} else {
+		decreasedVal.MilliCPU += rr.MilliCPU - r.MilliCPU
+	}
+
+	if r.Memory > rr.Memory {
+		increasedVal.Memory += r.Memory - rr.Memory
+	} else {
+		decreasedVal.Memory += rr.Memory - r.Memory
+	}
+
+	for rName, rQuant := range r.ScalarResources {
+		rrQuant := rr.ScalarResources[rName]
+
+		if rQuant > rrQuant {
+			if increasedVal.ScalarResources == nil {
+				increasedVal.ScalarResources = map[v1.ResourceName]float64{}
+			}
+			increasedVal.ScalarResources[rName] += rQuant - rrQuant
+		} else {
+			if decreasedVal.ScalarResources == nil {
+				decreasedVal.ScalarResources = map[v1.ResourceName]float64{}
+			}
+			decreasedVal.ScalarResources[rName] += rrQuant - rQuant
+		}
+	}
+
+	return increasedVal, decreasedVal
+}
+
+// String returns resource details in string format
+func (r *Resource) String() string {
+	str := fmt.Sprintf("cpu %0.2f, memory %0.2f", r.MilliCPU, r.Memory)
+	for rName, rQuant := range r.ScalarResources {
+		str = fmt.Sprintf("%s, %s %0.2f", str, rName, rQuant)
+	}
+	return str
+}
+
+// Get returns the resource value for that particular resource type
+func (r *Resource) Get(rn v1.ResourceName) float64 {
+	switch rn {
+	case v1.ResourceCPU:
+		return r.MilliCPU
+	case v1.ResourceMemory:
+		return r.Memory
+	default:
+		if r.ScalarResources == nil {
+			return 0
+		}
+		return r.ScalarResources[rn]
+	}
+}
+
+// ResourceNames returns all resource types
+func (r *Resource) ResourceNames() []v1.ResourceName {
+	resNames := []v1.ResourceName{v1.ResourceCPU, v1.ResourceMemory}
+
+	for rName := range r.ScalarResources {
+		resNames = append(resNames, rName)
+	}
+
+	return resNames
+}
+
+// AddScalar adds a resource by a scalar value of this resource.
+func (r *Resource) AddScalar(name v1.ResourceName, quantity float64) {
+	r.SetScalar(name, r.ScalarResources[name]+quantity)
+}
+
+// SetScalar sets a resource by a scalar value of this resource.
+func (r *Resource) SetScalar(name v1.ResourceName, quantity float64) {
+	// Lazily allocate scalar resource map.
+	if r.ScalarResources == nil {
+		r.ScalarResources = map[v1.ResourceName]float64{}
+	}
+	r.ScalarResources[name] = quantity
+}
+
+// MinDimensionResource is used to reset the r resource dimension which is less than rr
+// e.g r resource is <cpu 2000.00, memory 4047845376.00, hugepages-2Mi 0.00, hugepages-1Gi 0.00>
+// rr resource is <cpu 3000.00, memory 1000.00>
+// return r resource is <cpu 2000.00, memory 1000.00, hugepages-2Mi 0.00, hugepages-1Gi 0.00>
+func (r *Resource) MinDimensionResource(rr *Resource) *Resource {
+
+	if rr.MilliCPU < r.MilliCPU {
+		r.MilliCPU = rr.MilliCPU
+	}
+	if rr.Memory < r.Memory {
+		r.Memory = rr.Memory
+	}
+
+	if rr.ScalarResources == nil {
+		if r.ScalarResources != nil {
+			for name := range r.ScalarResources {
+				r.ScalarResources[name] = 0
+			}
+		}
+	} else {
+		if r.ScalarResources != nil {
+			for name, quant := range rr.ScalarResources {
+				if quant < r.ScalarResources[name] {
+					r.ScalarResources[name] = quant
+				}
+			}
+		}
+	}
+	return r
+}
diff --git a/pkg/common/types.go b/pkg/common/types.go
new file mode 100644
index 0000000..818d531
--- /dev/null
+++ b/pkg/common/types.go
@@ -0,0 +1,38 @@
+/*
+Copyright 2018 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package common
+
+// NodePhase defines the phase of node
+type NodePhase int
+
+const (
+	// Ready means the node is ready for scheduling
+	Ready NodePhase = 1 << iota
+	// NotReady means the node is not ready for scheduling
+	NotReady
+)
+
+func (np NodePhase) String() string {
+	switch np {
+	case Ready:
+		return "Ready"
+	case NotReady:
+		return "NotReady"
+	}
+
+	return "Unknown"
+}
diff --git a/pkg/common/utils.go b/pkg/common/utils.go
new file mode 100644
index 0000000..e254654
--- /dev/null
+++ b/pkg/common/utils.go
@@ -0,0 +1,282 @@
+package common
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/apis/pkg/client/clientset/versioned"
+)
+
+func ListNodesByQuotaLabel(kubeClient kubernetes.Interface, quotaKey string) ([]v1.Node, error) {
+	if quotaKey == "" {
+		return nil, fmt.Errorf("quotaKey is nil")
+	}
+	labelSelector := metav1.LabelSelector{}
+
+	if !schedulingv1beta1.GlobalNodeManageEnabled {
+		labelSelector = metav1.LabelSelector{
+			MatchExpressions: []metav1.LabelSelectorRequirement{
+				{
+					Key:      schedulingv1beta1.QuotaManagedNodeKey,
+					Operator: metav1.LabelSelectorOpExists,
+				},
+			},
+		}
+	}
+
+	labelSelector.MatchExpressions = append(labelSelector.MatchExpressions, metav1.LabelSelectorRequirement{
+		Key:     schedulingv1beta1.QuotaLabelKey,
+		Operator: metav1.LabelSelectorOpIn,
+		Values:   []string{quotaKey},
+	})
+	klog.V(6).Infof("listNodes by quotaKey[%s]  labelSelector=[%s]", quotaKey, metav1.FormatLabelSelector(&labelSelector))
+	nodes, err := kubeClient.CoreV1().Nodes().List(context.TODO(), metav1.ListOptions{LabelSelector: metav1.FormatLabelSelector(&labelSelector)})
+	if err != nil {
+		klog.Errorf("list node by quotaKey[%s] failed, err:[%v]", quotaKey, err)
+		return nil, err
+	}
+	klog.V(6).Infof("listNodes By quotaKey[%s], [%+v]", quotaKey, nodes)
+	return nodes.Items, nil
+}
+
+// SplitMetaNamespaceKey value of label can not format such as xxx/yyy, this func convert it
+func SplitMetaNamespaceKey(key string) (namespace, name string, err error) {
+	// every NS has only one resourceQuota that NS.name is equals to rp.name
+	return key, key, nil
+}
+
+// MetaNamespaceKeyFunc instead cache.MetaNamespaceKeyFunc when namespace and name are known as well as obj is unknown
+func MetaNamespaceKeyFunc(namespace, name string) string {
+	return name
+}
+
+func IsControlledBy(node *v1.Node) bool {
+	isControlled := false
+	if schedulingv1beta1.GlobalNodeManageEnabled {
+		isControlled = true
+	} else if _, exist := node.Labels[schedulingv1beta1.QuotaManagedNodeKey]; exist {
+		isControlled = true
+	}
+	return isControlled
+}
+
+// GetQuotaLabelByNode get resourcequota name from node's labels
+func GetQuotaLabelByNode(node *v1.Node) string {
+	label, ok := node.Labels[schedulingv1beta1.QuotaLabelKey]
+	if !ok || label == "" {
+		klog.V(6).Infof("resourceQuota key exist=[%v], "+
+			"true means resourceQuotaLabel[%v] of node exist but value is null", ok, schedulingv1beta1.QuotaLabelKey)
+		label = schedulingv1beta1.DefaultNamespace
+	}
+	return label
+}
+
+// GetEQuotaLabelByNode get elasticresourcequota name from node's labels
+func GetEQuotaLabelByNode(node *v1.Node) string {
+	quotaName, ok := node.Labels[schedulingv1beta1.QuotaLabelKey]
+	if !ok || quotaName == "" || quotaName == schedulingv1beta1.NodeScaleDown {
+		klog.V(6).Infof("quotaKey exist=[%v], quotaLabel=[%s] of node[%s]", ok, schedulingv1beta1.QuotaLabelKey, node.Name)
+	}
+	return quotaName
+}
+
+func NewQuota(name string, capability v1.ResourceList) *v1.ResourceQuota {
+	return &v1.ResourceQuota{
+		TypeMeta: metav1.TypeMeta{},
+		ObjectMeta: metav1.ObjectMeta{
+			Name:        name,
+			Annotations: make(map[string]string),
+			Labels:      make(map[string]string),
+		},
+		Spec: v1.ResourceQuotaSpec{
+			Hard: capability,
+		},
+	}
+}
+
+// ListLogicLeafQuotas get list of leaf logic-quota in the only one logic true
+func ListLogicLeafQuotas(vcClient versioned.Interface) ([]schedulingv1beta1.ElasticResourceQuota, error) {
+	// Get all logical quotas
+	labelSelector := metav1.LabelSelector{}
+	labelSelector.MatchExpressions = append(labelSelector.MatchExpressions, metav1.LabelSelectorRequirement{
+		Key:     schedulingv1beta1.QuotaTypeKey,
+		Operator: metav1.LabelSelectorOpIn,
+		Values:   []string{schedulingv1beta1.QuotaTypeLogical},
+	})
+	quotas, err := vcClient.SchedulingV1beta1().ElasticResourceQuotas().List(context.TODO(), metav1.ListOptions{
+		LabelSelector: metav1.FormatLabelSelector(&labelSelector)})
+	if err != nil {
+		return nil, err
+	}
+	var list []schedulingv1beta1.ElasticResourceQuota
+	for _, quota := range quotas.Items {
+		if quota.Status.IsLeaf {
+			list = append(list, quota)
+		}
+	}
+	return list, nil
+}
+
+// ListPublicNodes get list of node without label[QuotaManagedNodeKey]
+func ListPublicNodes(kubeClient kubernetes.Interface) ([]v1.Node, error) {
+	labelSelector := metav1.LabelSelector{}
+
+	if !schedulingv1beta1.GlobalNodeManageEnabled {
+		labelSelector = metav1.LabelSelector{
+			MatchExpressions: []metav1.LabelSelectorRequirement{
+				{
+					Key:      schedulingv1beta1.QuotaManagedNodeKey,
+					Operator: metav1.LabelSelectorOpExists,
+				},
+			},
+		}
+	}
+	labelSelector.MatchExpressions = append(labelSelector.MatchExpressions, metav1.LabelSelectorRequirement{
+		Key:     schedulingv1beta1.QuotaLabelKey,
+		Operator: metav1.LabelSelectorOpDoesNotExist,
+	})
+
+	nodes, err := kubeClient.CoreV1().Nodes().List(context.TODO(), metav1.ListOptions{LabelSelector: metav1.FormatLabelSelector(&labelSelector)})
+	if err != nil {
+		klog.Errorf("failed to list node with labelselector[%v], err=[%v]", labelSelector, err)
+		return nil, err
+	}
+	klog.V(4).Infof("listNodes node nums=[%d]", len(nodes.Items))
+	return nodes.Items, nil
+}
+
+// addResourceList add list resource quantity
+func addResourceList(list, req, limit v1.ResourceList) {
+	for name, quantity := range req {
+		if value, ok := list[name]; !ok {
+			list[name] = quantity.DeepCopy()
+		} else {
+			value.Add(quantity)
+			list[name] = value
+		}
+	}
+
+	if req != nil {
+		return
+	}
+
+	// If Requests is omitted for a container,
+	// it defaults to Limits if that is explicitly specified.
+	for name, quantity := range limit {
+		if value, ok := list[name]; !ok {
+			list[name] = quantity.DeepCopy()
+		} else {
+			value.Add(quantity)
+			list[name] = value
+		}
+	}
+}
+
+// CalcPodResources calculate podgroup minimum resource
+func CalcPodResources(pod *v1.Pod) *v1.ResourceList {
+	podRes := v1.ResourceList{}
+
+	for _, c := range pod.Spec.Containers {
+		addResourceList(podRes, c.Resources.Requests, c.Resources.Limits)
+	}
+
+	return &podRes
+}
+
+// GetEQuotaChildren get children of elasticResourceQuota
+func GetEQuotaChildren(client versioned.Interface, eQuotaName string) ([]schedulingv1beta1.ElasticResourceQuota, error) {
+	labelSelector := metav1.LabelSelector{}
+	labelSelector.MatchExpressions = append(labelSelector.MatchExpressions, metav1.LabelSelectorRequirement{
+		Key:      schedulingv1beta1.ElasticQuotaParentKey,
+		Operator: metav1.LabelSelectorOpIn,
+		Values:   []string{eQuotaName},
+	})
+	eQuotas, err := client.SchedulingV1beta1().ElasticResourceQuotas().List(context.TODO(),
+		metav1.ListOptions{LabelSelector: metav1.FormatLabelSelector(&labelSelector)})
+	if err != nil {
+		klog.Errorf("Failed to list children for elasticResourceQuotas[%s], err=[%+v]", eQuotaName, err)
+		return nil, err
+	}
+
+	return eQuotas.Items, nil
+}
+
+// GetEQuotaParent get parent of elasticResourceQuota
+func GetEQuotaParent(client versioned.Interface, eQuota *schedulingv1beta1.ElasticResourceQuota) (*schedulingv1beta1.ElasticResourceQuota, bool, error) {
+	if parentEQuotaName, exist := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]; exist {
+		parentEQuota, err := client.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentEQuotaName, metav1.GetOptions{})
+		if err != nil {
+			klog.Errorf("Failed to get parent elasticResourceQuota[%s], err=[%+v]", parentEQuotaName, err)
+			return nil, false, err
+		}
+		return parentEQuota, true, nil
+	}
+	return nil, false, nil
+}
+
+// GetEQuotaRoot get root of elasticResourceQuota
+func GetEQuotaRoot(client versioned.Interface, eQuotaName string) (*schedulingv1beta1.ElasticResourceQuota, bool, error) {
+	EQuota, err := client.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), eQuotaName, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("Failed to get elasticResourceQuota[%s], err=[%+v]", eQuotaName, err)
+		return nil, false, err
+	}
+	parentEQuota := EQuota.DeepCopy()
+	parentEQuotaName, exist := EQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	for exist && (len(parentEQuotaName) > 0) {
+		klog.Infof("get parent elasticResourceQuota[%s]", parentEQuotaName)
+		parentEQuota, err = client.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentEQuotaName, metav1.GetOptions{})
+		if err != nil {
+			klog.Errorf("Failed to get parent elasticResourceQuota[%s], err=[%+v]", parentEQuotaName, err)
+			return nil, false, err
+		}
+		parentEQuotaName, exist = parentEQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	}
+	return parentEQuota, true, nil
+}
+
+func StringToSlice(str string) []string {
+	var newSlice []string
+	if len(str) == 0 {
+		return newSlice
+	}
+	if err := json.Unmarshal([]byte(str), newSlice); err != nil {
+		klog.Errorf("Failed to convert string[%s] to slice, err=[%+v]", str, err)
+		return newSlice
+	}
+	return newSlice
+}
+
+func SliceToString(slice []string) string {
+	result := ""
+	if len(slice) == 0 {
+		return result
+	}
+	bytes, err := json.Marshal(slice)
+	if err != nil {
+		klog.Errorf("Failed to Marshal slice[%v]: err[%v].", slice, err)
+		return result
+	}
+	return string(bytes)
+}
+
+// CalcManagedNodesResources calculate sum resources of nodes which is managed by quota named quotaKey
+func CalcManagedNodesResources(kubeClient kubernetes.Interface, quotaKey string) (*Resource, error) {
+	nodes, err := ListNodesByQuotaLabel(kubeClient, quotaKey)
+	if err != nil {
+		return EmptyResource(), err
+	}
+	nodeSumResource := EmptyResource()
+	for _, node := range nodes {
+		nodeResource := NewResource(node.Status.Allocatable)
+		nodeSumResource.Add(nodeResource)
+	}
+	return nodeSumResource, nil
+}
diff --git a/pkg/controllers/apis/request.go b/pkg/controllers/apis/request.go
index cebf2f3..3013a5e 100644
--- a/pkg/controllers/apis/request.go
+++ b/pkg/controllers/apis/request.go
@@ -23,10 +23,14 @@ import (
 
 //Request struct.
 type Request struct {
-	Namespace string
-	JobName   string
-	TaskName  string
-	QueueName string
+	Namespace        string
+	JobName          string
+	TaskName         string
+	QueueName        string
+	// ElasticResourceQuota
+	EQuotaName       string
+	// Parent of ElasticResourceQuota
+	ParentEQuotaName string
 
 	Event      v1alpha1.Event
 	ExitCode   int32
diff --git a/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller.go b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller.go
new file mode 100644
index 0000000..2716561
--- /dev/null
+++ b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller.go
@@ -0,0 +1,268 @@
+package elasticresourcequota
+
+import (
+	"fmt"
+	"sync"
+	"time"
+
+	v1 "k8s.io/api/core/v1"
+	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
+	"k8s.io/apimachinery/pkg/util/wait"
+	"k8s.io/client-go/informers"
+	coreinformers "k8s.io/client-go/informers/core/v1"
+	"k8s.io/client-go/kubernetes"
+	corev1 "k8s.io/client-go/kubernetes/typed/core/v1"
+	corelisters "k8s.io/client-go/listers/core/v1"
+	"k8s.io/client-go/tools/cache"
+	"k8s.io/client-go/tools/record"
+	"k8s.io/client-go/util/workqueue"
+	"k8s.io/klog"
+
+	scheduling "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	vcclientset "volcano.sh/apis/pkg/client/clientset/versioned"
+	versionedscheme "volcano.sh/apis/pkg/client/clientset/versioned/scheme"
+	informerfactory "volcano.sh/apis/pkg/client/informers/externalversions"
+	schedulinginformer "volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1"
+	schedulinglister "volcano.sh/apis/pkg/client/listers/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+	"volcano.sh/volcano/pkg/controllers/apis"
+	"volcano.sh/volcano/pkg/controllers/framework"
+)
+
+func init() {
+	klog.Infof("framework.RegisterController elastic-resource-quota-controller")
+	framework.RegisterController(&elasticresourcequotacontroller{})
+}
+
+type elasticresourcequotacontroller struct {
+	kubeClient kubernetes.Interface
+	vcClient   vcclientset.Interface
+
+	// elastic resource quota Client
+	eqLister   schedulinglister.ElasticResourceQuotaLister
+	eqInformer schedulinginformer.ElasticResourceQuotaInformer
+	eqSynced   cache.InformerSynced
+
+	// queue Client
+	queueLister   schedulinglister.QueueLister
+	queueInformer schedulinginformer.QueueInformer
+	queueSynced   cache.InformerSynced
+
+	// pod Client
+	podInformer coreinformers.PodInformer
+	podLister   corelisters.PodLister
+	podSynced   func() bool
+
+	// node client
+	nodeInformer coreinformers.NodeInformer
+	nodeLister   corelisters.NodeLister
+	nodeSynced   func() bool
+
+	syncHandler func(req *apis.Request) error
+	// rateLimitQueue
+	queue workqueue.RateLimitingInterface
+
+	recorder      record.EventRecorder
+	maxRequeueNum int
+	// cache
+	resourceUsedCache map[string]*common.Resource
+	cacheMutex        sync.RWMutex
+}
+
+func (c *elasticresourcequotacontroller) Name() string {
+	return "elastic-resource-quota-controller"
+}
+
+func (c *elasticresourcequotacontroller) Initialize(opt *framework.ControllerOption) error {
+	c.vcClient = opt.VolcanoClient
+	c.kubeClient = opt.KubeClient
+
+	namespaceMap := make(map[string]struct{})
+	for _, ns := range opt.Namespaces {
+		namespaceMap[ns] = struct{}{}
+	}
+
+	// eventBroadcaster
+	eventBroadcaster := record.NewBroadcaster()
+	eventBroadcaster.StartLogging(klog.Infof)
+	eventBroadcaster.StartRecordingToSink(&corev1.EventSinkImpl{Interface: c.kubeClient.CoreV1().Events("")})
+	// informer lister
+	factory := informerfactory.NewSharedInformerFactory(c.vcClient, 0)
+	equotaInformer := factory.Scheduling().V1beta1().ElasticResourceQuotas()
+	c.eqInformer = equotaInformer
+	c.eqLister = equotaInformer.Lister()
+	c.eqSynced = equotaInformer.Informer().HasSynced
+
+	queueInformer := factory.Scheduling().V1beta1().Queues()
+	c.queueInformer = queueInformer
+	c.queueLister = queueInformer.Lister()
+	c.queueSynced = queueInformer.Informer().HasSynced
+
+	podInformer := opt.SharedInformerFactory.Core().V1().Pods()
+	c.podInformer = podInformer
+	c.podLister = podInformer.Lister()
+	c.podSynced = podInformer.Informer().HasSynced
+
+	c.nodeInformer = informers.NewSharedInformerFactory(c.kubeClient, 0).Core().V1().Nodes()
+	c.nodeLister = c.nodeInformer.Lister()
+	c.nodeSynced = c.nodeInformer.Informer().HasSynced
+
+	// request_event queue
+	c.queue = workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
+
+	c.maxRequeueNum = opt.MaxRequeueNum
+	if c.maxRequeueNum < 0 {
+		c.maxRequeueNum = -1
+	}
+	// AddEventHandler
+	equotaInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
+		AddFunc:    c.addElasticQuota,
+		DeleteFunc: c.deleteElasticQuota,
+	})
+	queueInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
+		AddFunc:    c.addQueue,
+		UpdateFunc: c.updateQueue,
+		DeleteFunc: c.deleteQueue,
+	})
+	podInformer.Informer().AddEventHandler(
+		cache.FilteringResourceEventHandler{
+			FilterFunc: func(obj interface{}) bool {
+				switch v := obj.(type) {
+				case *v1.Pod:
+					if len(opt.Namespaces) > 0 {
+						if _, ok := namespaceMap[v.Namespace]; !ok {
+							klog.Warningf("Pod <%s/%s> is not in watching namespace: %v, so skip it",
+								v.Namespace, v.Name, opt.Namespaces)
+							return false
+						}
+					}
+					if v.Spec.SchedulerName == opt.SchedulerName && v.Annotations != nil && v.Annotations[scheduling.KubeGroupNameAnnotationKey] != "" {
+						return true
+					}
+					return false
+				default:
+					return false
+				}
+			},
+			Handler: cache.ResourceEventHandlerFuncs{
+				AddFunc:    c.addPod,
+				UpdateFunc: c.updatePod,
+				DeleteFunc: c.deletePod,
+			},
+		})
+
+	// register node event handler
+	c.nodeInformer.Informer().AddEventHandler(
+		cache.FilteringResourceEventHandler{
+			FilterFunc: func(obj interface{}) bool {
+				switch obj.(type) {
+				case *v1.Node:
+					// manager all nodes, we just find resourceQuotas label
+					return common.IsControlledBy(obj.(*v1.Node))
+				default:
+					return false
+				}
+			},
+			Handler: cache.ResourceEventHandlerFuncs{
+				AddFunc:    c.addNode,
+				UpdateFunc: c.updateNode,
+				DeleteFunc: c.deleteNode,
+			},
+		})
+
+	// handler
+	c.syncHandler = c.handleElasticResourceQuotas
+	// recorder
+	c.recorder = eventBroadcaster.NewRecorder(versionedscheme.Scheme, v1.EventSource{Component: "vc-controller-manager"})
+	// init cache
+	c.resourceUsedCache = make(map[string]*common.Resource)
+	return nil
+}
+
+// Run starts resourcequotaController.
+func (c *elasticresourcequotacontroller) Run(stopCh <-chan struct{}) {
+	defer utilruntime.HandleCrash()
+	defer c.queue.ShutDown()
+
+	klog.Infof("Starting elastic-resource-quotas-controller.")
+	defer klog.Infof("Shutting down elastic-resource-quotas-controller.")
+
+	// begin to listen event
+	go c.eqInformer.Informer().Run(stopCh)
+	go c.queueInformer.Informer().Run(stopCh)
+	go c.podInformer.Informer().Run(stopCh)
+	go c.nodeInformer.Informer().Run(stopCh)
+	// wait until cache synced
+	if !cache.WaitForCacheSync(stopCh, c.eqSynced, c.queueSynced, c.podSynced, c.nodeSynced) {
+		klog.Errorf("unable to sync caches for elastic-resource-quotas-controller.")
+		return
+	}
+	// worker starting
+	go wait.Until(c.worker, 0, stopCh)
+
+	<-stopCh
+}
+
+// worker runs a worker thread that just dequeues items, processes them, and
+// marks them done. You may run as many of these in parallel as you wish; the
+// workqueue guarantees that they will not end up processing the same `elasticresourcequota`
+// at the same time.
+func (c *elasticresourcequotacontroller) worker() {
+	for c.processNextWorkItem() {
+	}
+}
+
+func (c *elasticresourcequotacontroller) processNextWorkItem() bool {
+	obj, shutdown := c.queue.Get()
+	if shutdown {
+		return false
+	}
+	defer c.queue.Done(obj)
+
+	req, ok := obj.(*apis.Request)
+	if !ok {
+		klog.Errorf("%v is not a valid elasticResourceQuota request struct.", obj)
+		return true
+	}
+
+	err := c.syncHandler(req)
+	c.handleElasticResourceQuotasErr(err, obj)
+
+	return true
+}
+
+func (c *elasticresourcequotacontroller) handleElasticResourceQuotas(req *apis.Request) error {
+	klog.V(4).Infof("handle elasticResourceQuota req[%+v].", req)
+	startTime := time.Now()
+	defer func() {
+		klog.V(4).Infof("Finished syncing elasticResourceQuota[%s] (%v).", req.EQuotaName, time.Since(startTime))
+	}()
+
+	// when informer notice controller, it can only sync info in elasticresourcequota
+	// because equota has been created/updated/deleted
+	switch req.Action {
+	case common.RemoveEQuotaAction:
+		return c.removeEQuota(req.EQuotaName, req.ParentEQuotaName)
+	case common.SyncEQuotaAction, common.SyncEQuotaUsedAction:
+		// .spec has nothing to be updated, only update .status
+		return c.syncEQuota(req.EQuotaName, req.Action)
+	default:
+		return fmt.Errorf("unknown elasticResourceQuota request[%+v]", req)
+	}
+}
+
+func (c *elasticresourcequotacontroller) handleElasticResourceQuotasErr(err error, obj interface{}) {
+	if err == nil {
+		c.queue.Forget(obj)
+		return
+	}
+
+	if c.maxRequeueNum == -1 || c.queue.NumRequeues(obj) < c.maxRequeueNum {
+		klog.V(4).Infof("Error syncing elasticResourceQuota request %v for %v.", obj, err)
+		c.queue.AddRateLimited(obj)
+		return
+	}
+
+	klog.V(2).Infof("Dropping elasticResourceQuota request %v out of the queue for %v.", obj, err)
+	c.queue.Forget(obj)
+}
diff --git a/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_action.go b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_action.go
new file mode 100644
index 0000000..0e538ad
--- /dev/null
+++ b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_action.go
@@ -0,0 +1,238 @@
+package elasticresourcequota
+
+import (
+	"context"
+	"reflect"
+
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/klog"
+
+	busv1alpha1 "volcano.sh/apis/pkg/apis/bus/v1alpha1"
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+)
+
+func (c *elasticresourcequotacontroller) removeEQuota(deletedEQuotaName string, parentEQuotaName string) error {
+	klog.V(4).Infof("Begin to remove elasticResourceQuota[%s] and sync elasticResourceQuota[%s].", deletedEQuotaName, parentEQuotaName)
+	defer klog.V(4).Infof("End to remove elasticResourceQuota[%s] and sync elasticResourceQuota[%s].", deletedEQuotaName, parentEQuotaName)
+	parentEQuota, err := c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentEQuotaName, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("Failed to get elasticResourceQuota[%s] when removed elasticResourceQuota[%s], err:[%v].", parentEQuotaName, deletedEQuotaName, err)
+		return err
+	}
+	oldIsLeaf := parentEQuota.Status.IsLeaf
+	// get children
+	eQuotaList, err := common.GetEQuotaChildren(c.vcClient, parentEQuotaName)
+	if err != nil {
+		klog.Errorf("Failed to list children for elasticResourceQuota[%s], err=[%+v]", parentEQuotaName, err)
+		return err
+	}
+
+	if len(eQuotaList) == 0 {
+		parentEQuota.Status.IsLeaf = true
+	} else {
+		parentEQuota.Status.IsLeaf = false
+	}
+
+	if oldIsLeaf != parentEQuota.Status.IsLeaf {
+		if _, err = c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().UpdateStatus(context.TODO(), parentEQuota, metav1.UpdateOptions{}); err != nil {
+			klog.Errorf("Failed to update status.IsLeaf of eQuota[%+v], err=[%+v]", parentEQuota, err)
+			return err
+		}
+	}
+	return nil
+}
+
+// syncEQuota sync spec.max, status.IsLeaf and status.Used of elasticResourceQuota
+func (c *elasticresourcequotacontroller) syncEQuota(eQuotaName string, action busv1alpha1.Action) error {
+	klog.V(4).Infof("Begin to sync elasticResourceQuota[%s].", eQuotaName)
+	defer klog.V(4).Infof("End to sync elasticResourceQuota[%s].", eQuotaName)
+	eQuota, err := c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), eQuotaName, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("elasticResourceQuota[%+v] get failed, error:%v.", eQuota, err)
+		return err
+	}
+	newEQuota := eQuota.DeepCopy()
+
+	if action == common.SyncEQuotaAction {
+		if err := c.syncEQuotaStatusIsLeaf(newEQuota); err != nil {
+			klog.Errorf("sync status.IsLeaf of elasticResourceQuota[%+v] failed, error:%v.", newEQuota, err)
+			return err
+		}
+		if err := c.syncEQuotaSpec(newEQuota); err != nil {
+			klog.Errorf("sync min or max of elasticResourceQuota[%+v] failed, error:%v.", newEQuota, err)
+			return err
+		}
+		if err := c.syncHardwareTypes(newEQuota); err != nil {
+			klog.Errorf("sync hardwareTypes of elasticResourceQuota[%+v] failed, error:%v.", newEQuota, err)
+			return err
+		}
+	}
+
+	if err := c.syncEQuotaStatusUsed(newEQuota); err != nil {
+		klog.Errorf("Failed to sync status.used for elasticResourceQuota[%s], err=[%+v]", eQuota.Name, err)
+		return err
+	}
+	// submit changes
+	if action == common.SyncEQuotaAction && !reflect.DeepEqual(eQuota, newEQuota) {
+		klog.V(4).Infof("changes in elasticResourceQuota[%s] is to be submit", eQuota.Name)
+		if _, err := c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().Update(context.TODO(), newEQuota, metav1.UpdateOptions{}); err != nil {
+			klog.Errorf("Failed to update elasticResourceQuota[%s], update from eQuota[%+v] to newEQuota[%+v], err=[%+v]",
+				eQuota.Name, eQuota, newEQuota, err)
+			return err
+		}
+	}
+	if !reflect.DeepEqual(eQuota.Status, newEQuota.Status) {
+		klog.V(4).Infof("changes in elasticResourceQuota[%s].status is to be submit", eQuota.Name)
+		if _, err := c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().UpdateStatus(context.TODO(), newEQuota, metav1.UpdateOptions{}); err != nil {
+			klog.Errorf("Failed to update elasticResourceQuota[%s], update from status[%+v] to status[%+v], err=[%+v]",
+				eQuota.Name, eQuota.Status, newEQuota.Status, err)
+			return err
+		}
+	}
+
+	return nil
+}
+
+// syncEQuotaSpec sync min/max to right value which it should be
+func (c *elasticresourcequotacontroller) syncEQuotaSpec(eQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("sync the spec of elasticResourceQuota[%s], spec=[%v].", eQuota.Name, eQuota.Spec)
+	if _, isChild := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]; isChild {
+		return nil
+	}
+
+	switch eQuota.Labels[schedulingv1beta1.QuotaTypeKey] {
+	case schedulingv1beta1.QuotaTypeLogical:
+		nodeList, err := common.ListPublicNodes(c.kubeClient)
+		if err != nil && !errors.IsNotFound(err) {
+			klog.Errorf("failed to list publicNodes, err=[%v]", err)
+			return err
+		}
+		// calculate the sum of nodes
+		nodeSumResource := common.EmptyResource()
+		for _, n := range nodeList {
+			nodeSumResource.Add(common.NewResource(n.Status.Allocatable))
+		}
+		eQuota.Spec.Min = common.NewResourceList(nodeSumResource)
+		klog.V(4).Infof("Logical quotas own nodeSumResource=[%s]", nodeSumResource.String())
+	case schedulingv1beta1.QuotaTypePhysical:
+		// sync spec.max of physical quota
+		nodeSumResource, err := common.CalcManagedNodesResources(c.kubeClient, eQuota.Name)
+		if err != nil {
+			klog.Errorf("CalcManagedNodesResources for elasticResourceQuota[%+v] failed, error:%v.", eQuota.Name, err)
+			return err
+		}
+		nodeResourceList := common.NewResourceList(nodeSumResource)
+		eQuota.Spec.Min = nodeResourceList
+		eQuota.Spec.Max = nodeResourceList
+		klog.V(4).Infof("Physical quotas own nodeSumResource=[%s]", nodeSumResource.String())
+	default:
+		klog.Errorf("Unsupported type of quotaType when sync elasticresourcequota[%s]", eQuota.Name)
+	}
+
+	return nil
+}
+
+func (c *elasticresourcequotacontroller) syncEQuotaStatusIsLeaf(eQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("sync the status.IsLeaf of elasticResourceQuota[%s], status.IsLeaf=[%v].", eQuota.Name, eQuota.Status.IsLeaf)
+	// find children
+	children, err := common.GetEQuotaChildren(c.vcClient, eQuota.Name)
+	if err != nil {
+		klog.Errorf("Failed to get children of elasticResourceQuota[%s], err=[%+v]", eQuota.Name, err)
+		return err
+	}
+	eQuota.Status.IsLeaf = len(children) == 0
+	klog.V(4).Infof("sync the status.IsLeaf of elasticResourceQuota[%s], new value of status.IsLeaf=[%v], children=[%+v].",
+		eQuota.Name, eQuota.Status.IsLeaf, children)
+	// update parent
+	parentEQuota, parentExist, err := common.GetEQuotaParent(c.vcClient, eQuota)
+	if err != nil {
+		klog.Errorf("Failed to get elasticResourceQuota[%s]'s parent, err=[%+v]", eQuota.Name, err)
+		return err
+	}
+	if parentExist && parentEQuota.Status.IsLeaf {
+		// update parent isLeaf
+		parentEQuota.Status.IsLeaf = false
+		if _, err = c.vcClient.SchedulingV1beta1().ElasticResourceQuotas().UpdateStatus(context.TODO(), parentEQuota, metav1.UpdateOptions{}); err != nil {
+			klog.Errorf("Failed to update status.IsLeaf of parentEQuota[%+v], err=[%+v]", parentEQuota, err)
+			return err
+		}
+	}
+
+	return nil
+}
+
+// syncEQuotaStatusUsed will sync status.Used of elasticResourceQuota, submit update in function when isUpdateNow is true
+func (c *elasticresourcequotacontroller) syncEQuotaStatusUsed(eQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("sync the status.Used of elasticResourceQuota[%+v].", eQuota)
+	if !eQuota.Status.IsLeaf {
+		klog.V(4).Infof("elasticResourceQuota[%s] is not leaf, passed the sync of status.", eQuota.Name)
+		return nil
+	}
+	// update .Status.Used
+	newestResource, exist := c.resourceUsedCache[eQuota.Name]
+	if exist {
+		klog.V(4).Infof("resourceUsedCache[%s]=[%s]", eQuota.Name, newestResource.String())
+	} else {
+		klog.V(4).Infof("resourceUsedCache[%s] is not exist", eQuota.Name)
+		newestResource = common.EmptyResource()
+	}
+
+	snapshotResource := common.NewResource(eQuota.Status.Used)
+	klog.V(4).Infof("elasticResourceQuota[%s].Status.used=[%s] actually=[%s].", eQuota.Name,
+		snapshotResource.String(), newestResource.String())
+	if reflect.DeepEqual(snapshotResource, newestResource) {
+		return nil
+	}
+
+	eQuota.Status.Used = common.NewResourceList(newestResource)
+
+	klog.V(4).Infof("syncEQuotaStatusUsed: after update, elasticResourceQuota=[%+v]", eQuota)
+	return nil
+}
+
+func (c *elasticresourcequotacontroller) syncHardwareTypes(eQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("sync the HardwareType of elasticResourceQuota[%s].", eQuota.Name)
+	hardwareTypes, err := c.getHardwareTypes(eQuota.Name)
+	if err != nil {
+		klog.Errorf("Failed to getHardwareTypes: %v.", err)
+		return err
+	}
+	if eQuota.Annotations == nil {
+		eQuota.Annotations = make(map[string]string)
+	}
+	if len(hardwareTypes) > 0 {
+		// Must check Annotations null or not while setting value
+		eQuota.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey] = common.SliceToString(hardwareTypes)
+	} else {
+		delete(eQuota.Annotations, schedulingv1beta1.QuotaHardwareTypeLabelKey)
+	}
+	return nil
+}
+
+func (c *elasticresourcequotacontroller) getHardwareTypes(eQuotaName string) ([]string, error) {
+	nodeItems, err := common.ListNodesByQuotaLabel(c.kubeClient, eQuotaName)
+	if err != nil {
+		klog.Errorf("getHardwareTypes: get Node.Items failed, err:[%+v]", err)
+		return nil, err
+	}
+
+	htMap := make(map[string]bool)
+	htInRP := []string{}
+
+	//e.g. paddleflow.baidu.com/hardware-type: V100
+	for _, node := range nodeItems {
+		// Ignore error here, tt can not occur.
+		if hardWareType, ok := node.Labels[schedulingv1beta1.QuotaHardwareTypeLabelKey]; ok && hardWareType != "" {
+			klog.V(4).Infof("node[%s] record hardWareType[%s]", node.Name, hardWareType)
+			htMap[hardWareType] = true
+		}
+	}
+
+	for ht := range htMap {
+		htInRP = append(htInRP, ht)
+	}
+	klog.V(6).Infof("getHardwareTypes: [%+v]", htInRP)
+	return htInRP, nil
+}
diff --git a/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_handler.go b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_handler.go
new file mode 100644
index 0000000..541326a
--- /dev/null
+++ b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_handler.go
@@ -0,0 +1,274 @@
+package elasticresourcequota
+
+import (
+	"fmt"
+	"reflect"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/client-go/tools/cache"
+	"k8s.io/klog"
+
+	batch "volcano.sh/apis/pkg/apis/batch/v1alpha1"
+	busv1alpha1 "volcano.sh/apis/pkg/apis/bus/v1alpha1"
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+	"volcano.sh/volcano/pkg/controllers/apis"
+)
+
+func (c *elasticresourcequotacontroller) enqueue(req *apis.Request) {
+	c.queue.Add(req)
+}
+
+// addElasticQuota sync tree stucture when addElasticQuota watched
+func (c *elasticresourcequotacontroller) addElasticQuota(obj interface{}) {
+	elasticQuota := obj.(*schedulingv1beta1.ElasticResourceQuota)
+	klog.V(4).Infof("Notice the event of add elasticResourceQuota[%s].", elasticQuota.Name)
+	req := &apis.Request{
+		EQuotaName: elasticQuota.Name,
+		Event:      busv1alpha1.OutOfSyncEvent,
+		Action:     common.SyncEQuotaAction,
+	}
+	c.enqueue(req)
+}
+
+// deleteElasticQuota sync tree stucture when deleteElasticQuota watched
+func (c *elasticresourcequotacontroller) deleteElasticQuota(obj interface{}) {
+	elasticQuota := obj.(*schedulingv1beta1.ElasticResourceQuota)
+	klog.V(4).Infof("Notice the delete event of elasticResourceQuota[%s].", elasticQuota.Name)
+	var parentElasticQuotaName string
+	if elasticQuota.Labels != nil {
+		parentElasticQuotaName = elasticQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	}
+	req := &apis.Request{
+		ParentEQuotaName: parentElasticQuotaName,
+		EQuotaName:       elasticQuota.Name,
+		Namespace:        elasticQuota.Spec.Namespace,
+		Event:            busv1alpha1.OutOfSyncEvent,
+		Action:           common.RemoveEQuotaAction,
+	}
+	c.enqueue(req)
+}
+
+func (c *elasticresourcequotacontroller) addQueue(obj interface{}) {
+}
+
+func (c *elasticresourcequotacontroller) updateQueue(oldObj, newObj interface{}) {
+}
+
+func (c *elasticresourcequotacontroller) deleteQueue(obj interface{}) {
+}
+
+func (c *elasticresourcequotacontroller) addPod(obj interface{}) {
+	pod, ok := obj.(*v1.Pod)
+	if !ok {
+		klog.Errorf("Failed to convert %v to v1.Pod", obj)
+		return
+	}
+	if pod.Status.Phase != v1.PodRunning {
+		return
+	}
+
+	queueName, eQuotaName, err := c.getBoundResourceKeyInPod(pod)
+	if err != nil {
+		klog.Errorf("get the name of queue and elasticresourcequota from pod[%s] failed, err=[%+v]", pod.Name, err)
+		return
+	}
+	klog.V(4).Infof("the added pod[%s] related to queue[%s] and elasticResourceQuota[%s].", pod.Name, queueName, eQuotaName)
+
+	// update cache
+	c.cacheMutex.Lock()
+	defer c.cacheMutex.Unlock()
+	used := common.EmptyResource()
+	if recordedResource, exist := c.resourceUsedCache[eQuotaName]; exist {
+		used = recordedResource
+	}
+	requestResource := common.NewResource(*common.CalcPodResources(pod))
+	sumResource := used.Add(requestResource)
+	c.resourceUsedCache[eQuotaName] = sumResource
+	klog.V(4).Infof("elasticResourceQuota[%s] request-resource[%s] of pod[%s] to be added, sum=[%s].", eQuotaName, requestResource.String(), pod.Name, sumResource.String())
+    // send request
+	req := &apis.Request{
+		QueueName:  queueName,
+		EQuotaName: eQuotaName,
+		Event:      busv1alpha1.OutOfSyncEvent,
+		Action:     common.SyncEQuotaUsedAction,
+	}
+	c.enqueue(req)
+
+}
+
+func (c *elasticresourcequotacontroller) updatePod(old, new interface{}) {
+	oldPod := old.(*v1.Pod)
+	newPod := new.(*v1.Pod)
+
+	if oldPod.Status.Phase != newPod.Status.Phase {
+		klog.V(4).Infof("Notice to update podGroup[%s] old_status=[%v] new_status=[%v].", newPod.Name, oldPod.Status.Phase, newPod.Status.Phase)
+		if newPod.Status.Phase == v1.PodRunning {
+			c.addPod(newPod)
+		}
+		if oldPod.Status.Phase == v1.PodRunning {
+			// param must not be newPod while its new Phase would be filtered by `deletePod`
+			c.deletePod(oldPod)
+		}
+	}
+}
+
+func (c *elasticresourcequotacontroller) deletePod(obj interface{}) {
+	pod, ok := obj.(*v1.Pod)
+	if !ok {
+		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
+		if !ok {
+			klog.Errorf("Couldn't get object from tombstone %#v.", obj)
+			return
+		}
+		pod, ok = tombstone.Obj.(*v1.Pod)
+		if !ok {
+			klog.Errorf("Tombstone contained object that is not a PodGroup: %#v.", obj)
+			return
+		}
+	}
+	if pod.Status.Phase != v1.PodRunning {
+		return
+	}
+	klog.V(4).Infof("Notice to delete pod[%s] status=[%v].", pod.Name, pod.Status.Phase)
+
+	queueName, eQuotaName, err := c.getBoundResourceKeyInPod(pod)
+	if err != nil {
+		klog.Errorf("get the name of queue and elasticresourcequota from pod[%s] failed, err=[%+v]", pod.Name, err)
+		return
+	}
+	klog.V(4).Infof("the deleted pod[%s] related to queue[%s] and elasticResourceQuota[%s].", pod.Name, queueName, eQuotaName)
+
+	// update cache
+	c.cacheMutex.Lock()
+	defer c.cacheMutex.Unlock()
+	used := common.EmptyResource()
+	if recordedResource, exist := c.resourceUsedCache[eQuotaName]; exist {
+		used = recordedResource
+	}
+	requestResource := common.NewResource(*common.CalcPodResources(pod))
+	if requestResource.LessEqual(used) {
+		sumResource := used.Sub(requestResource)
+		c.resourceUsedCache[eQuotaName] = sumResource
+		klog.V(4).Infof("elasticResourceQuota[%s] request-resource[%s] of pod[%s] to be deleted, left=[%s].",
+			eQuotaName, requestResource.String(), pod.Name, used.String())
+	} else {
+		klog.Warningf("elasticResourceQuota[%s] request-resource[%s] of pod[%s] to be deleted, but left=[%s].",
+			eQuotaName, requestResource.String(), pod.Name, used.String())
+	}
+
+	// send request
+	req := &apis.Request{
+		QueueName:  queueName,
+		EQuotaName: eQuotaName,
+		Event:      busv1alpha1.OutOfSyncEvent,
+		Action:     common.SyncEQuotaUsedAction,
+	}
+	c.enqueue(req)
+	klog.V(4).Infof("c.resourceUsedCache=[%+v]", c.resourceUsedCache)
+}
+
+// getBoundResourceKeyInPod parse bound resource key in pod, including queue.Name and elasticResourceQuota.Name
+func (c *elasticresourcequotacontroller) getBoundResourceKeyInPod(pod *v1.Pod) (queueName, eQuotaName string, err error) {
+	// find queue by pod
+	queueName, exist := pod.Annotations[batch.QueueNameKey]
+	if !exist {
+		return "", "", fmt.Errorf("Failed to get queue's name from pod[%s]. ", pod.Name)
+	}
+	// todo(zhongzichao) queue would be cached to improve the efficient
+	queue, err := c.queueLister.Get(queueName)
+	if err != nil {
+		klog.Errorf("queue cannot be found: err=[+v].", err)
+		return "", "", err
+	}
+	// find eQuotaName by queue, and regard it as key in cacheMap
+	eQuotaName, ok := queue.Labels[schedulingv1beta1.QueueBindingElasticQuotaKey]
+	if !ok {
+		klog.Infof("queue hasn't bound elasticResourceQuota: err=[+v].", queueName, err)
+		return "", "", fmt.Errorf("Failed to get elasticResourceQuota's name from pod[%s]. ", pod.Name)
+	}
+	return queueName, eQuotaName, nil
+}
+
+func (c *elasticresourcequotacontroller) addNode(obj interface{}) {
+	node := obj.(*v1.Node)
+	eQuotaName := common.GetEQuotaLabelByNode(node)
+
+	// filter unrelated case
+	if len(eQuotaName) == 0 || eQuotaName == schedulingv1beta1.NodeScaleDown {
+		return
+	}
+
+	// if node's namespace is scale-down
+	klog.V(4).Infof("Begin to add node[%+v] to elasticResourceQuota[%s].", node.Name, eQuotaName)
+	defer klog.V(4).Infof("End to add node, enqueue req")
+
+	c.addSyncRequest(eQuotaName)
+}
+
+// ,,
+func (c *elasticresourcequotacontroller) updateNode(oldObj, newObj interface{}) {
+	oldNode := oldObj.(*v1.Node)
+	newNode := newObj.(*v1.Node)
+
+	// Filter deleted node
+	if newNode.DeletionTimestamp != nil {
+		klog.V(6).Infof("node[%s] has been deleted at[%v], update passed", newNode.Name, newNode.DeletionTimestamp)
+		c.deleteNode(newObj)
+		return
+	}
+	if reflect.DeepEqual(oldNode.Labels, newNode.Labels) {
+		klog.V(6).Infof("node[%s] labels changed nothing, update passed", newNode.Name)
+		return
+	} else {
+		klog.V(4).Infof("updating node[%s]: old node's labels[%v] to new node's labels[%v].", newNode.Name, oldNode.Labels, newNode.Labels)
+	}
+
+	// Filter no changes node
+	oldHardwareType := oldNode.Labels[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	newHardwareType := newNode.Labels[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+
+	oldEQuotaName := common.GetEQuotaLabelByNode(oldNode)
+	newEQuotaName := common.GetEQuotaLabelByNode(newNode)
+
+	if oldHardwareType == newHardwareType && oldEQuotaName == newEQuotaName {
+		klog.V(6).Infof("hardwareType and quotaName of Node not changed, update passed")
+		return
+	}
+
+	if oldEQuotaName != "" && oldEQuotaName != schedulingv1beta1.NodeScaleDown {
+		// oldEQuota is physical type, sync it
+		c.addSyncRequest(oldEQuotaName)
+	} else {
+		// todo(zhongzichao) sync hardwareType of logical quota
+	}
+
+	if newEQuotaName != "" && newEQuotaName != schedulingv1beta1.NodeScaleDown {
+		// update/add new node
+		c.addSyncRequest(newEQuotaName)
+	} else {
+		// todo(zhongzichao) sync hardwareType of logical quota
+	}
+}
+
+func (c *elasticresourcequotacontroller) deleteNode(obj interface{}) {
+	node := obj.(*v1.Node)
+	eQuotaName := common.GetEQuotaLabelByNode(node)
+	klog.V(6).Infof("Begin to remove node[%v] from elasticResourceQuota[%s].", node, eQuotaName)
+	if eQuotaName == "" || eQuotaName == schedulingv1beta1.NodeScaleDown {
+		// todo(zhongzichao) sync logical-eQuota tree
+		return
+	}
+
+	c.addSyncRequest(eQuotaName)
+}
+
+// addSyncRequest construct simple sync-request that just specified equotaName
+func (c *elasticresourcequotacontroller) addSyncRequest(eQuotaName string) {
+	req := &apis.Request{
+		EQuotaName: eQuotaName,
+		Event:      busv1alpha1.OutOfSyncEvent,
+		Action:     common.SyncEQuotaAction,
+	}
+	c.enqueue(req)
+}
\ No newline at end of file
diff --git a/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_test.go b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_test.go
new file mode 100644
index 0000000..727c421
--- /dev/null
+++ b/pkg/controllers/elasticresourcequota/elastic_resource_quota_controller_test.go
@@ -0,0 +1,94 @@
+/*
+Copyright 2019 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package elasticresourcequota
+
+import (
+	"fmt"
+	"testing"
+
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	kubeclient "k8s.io/client-go/kubernetes/fake"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	vcclient "volcano.sh/apis/pkg/client/clientset/versioned/fake"
+	"volcano.sh/volcano/pkg/controllers/framework"
+)
+
+func newFakeController() *elasticresourcequotacontroller {
+	KubeBatchClientSet := vcclient.NewSimpleClientset()
+	KubeClientSet := kubeclient.NewSimpleClientset()
+
+	controller := &elasticresourcequotacontroller{}
+	opt := framework.ControllerOption{
+		VolcanoClient: KubeBatchClientSet,
+		KubeClient:    KubeClientSet,
+	}
+
+	controller.Initialize(&opt)
+
+	return controller
+}
+
+func TestAddQueue(t *testing.T) {
+	testCases := []struct {
+		Name        string
+		elasticQuota       *schedulingv1beta1.ElasticResourceQuota
+		ExpectValue int
+	}{
+		{
+			Name: "AddElasticQuota",
+			elasticQuota: &schedulingv1beta1.ElasticResourceQuota{
+				ObjectMeta: metav1.ObjectMeta{
+					Name: "c1",
+				},
+			},
+			ExpectValue: 1,
+		},
+	}
+
+	for i, testcase := range testCases {
+		c := newFakeController()
+
+		c.addElasticQuota(testcase.elasticQuota)
+
+		if testcase.ExpectValue != c.queue.Len() {
+			t.Errorf("case %d (%s): expected: %v, got %v ", i, testcase.Name, testcase.ExpectValue, c.queue.Len())
+		}
+	}
+}
+
+func TestProcessNextWorkItem(t *testing.T) {
+	testCases := []struct {
+		Name        string
+		ExpectValue int32
+	}{
+		{
+			Name:        "processNextWorkItem",
+			ExpectValue: 0,
+		},
+	}
+
+	for i, testcase := range testCases {
+		c := newFakeController()
+		c.queue.Add("test")
+		bVal := c.processNextWorkItem()
+		fmt.Println("The value of boolean is ", bVal)
+		if c.queue.Len() != 0 {
+			t.Errorf("case %d (%s): expected: %v, got %v ", i, testcase.Name, testcase.ExpectValue, c.queue.Len())
+		}
+	}
+}
diff --git a/pkg/controllers/framework/interface.go b/pkg/controllers/framework/interface.go
index 13b1b98..c9858af 100644
--- a/pkg/controllers/framework/interface.go
+++ b/pkg/controllers/framework/interface.go
@@ -30,6 +30,7 @@ type ControllerOption struct {
 	SchedulerName         string
 	WorkerNum             uint32
 	MaxRequeueNum         int
+	Namespaces            []string
 }
 
 // Controller is the interface of all controllers.
diff --git a/pkg/controllers/job/job_controller.go b/pkg/controllers/job/job_controller.go
index 1914f4f..3b32e91 100644
--- a/pkg/controllers/job/job_controller.go
+++ b/pkg/controllers/job/job_controller.go
@@ -38,6 +38,7 @@ import (
 
 	batchv1alpha1 "volcano.sh/apis/pkg/apis/batch/v1alpha1"
 	busv1alpha1 "volcano.sh/apis/pkg/apis/bus/v1alpha1"
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
 	vcclientset "volcano.sh/apis/pkg/client/clientset/versioned"
 	vcscheme "volcano.sh/apis/pkg/client/clientset/versioned/scheme"
 	informerfactory "volcano.sh/apis/pkg/client/informers/externalversions"
@@ -141,12 +142,35 @@ func (cc *jobcontroller) Initialize(opt *framework.ControllerOption) error {
 		cc.queueList[i] = workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
 	}
 
+	namespaceMap := make(map[string]struct{})
+	for _, ns := range opt.Namespaces {
+		namespaceMap[ns] = struct{}{}
+	}
+
 	cc.jobInformer = informerfactory.NewSharedInformerFactory(cc.vcClient, 0).Batch().V1alpha1().Jobs()
-	cc.jobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
-		AddFunc:    cc.addJob,
-		UpdateFunc: cc.updateJob,
-		DeleteFunc: cc.deleteJob,
-	})
+	cc.jobInformer.Informer().AddEventHandler(
+		cache.FilteringResourceEventHandler{
+			FilterFunc: func(obj interface{}) bool {
+				switch v := obj.(type) {
+				case *batchv1alpha1.Job:
+					if len(opt.Namespaces) > 0 {
+						if _, ok := namespaceMap[v.Namespace]; !ok {
+							klog.Warningf("Job <%s/%s> is not in watching namespace: %v, so skip it",
+								v.Namespace, v.Name, opt.Namespaces)
+							return false
+						}
+					}
+					return true
+				default:
+					return false
+				}
+			},
+			Handler: cache.ResourceEventHandlerFuncs{
+				AddFunc:    cc.addJob,
+				UpdateFunc: cc.updateJob,
+				DeleteFunc: cc.deleteJob,
+			},
+		})
 	cc.jobLister = cc.jobInformer.Lister()
 	cc.jobSynced = cc.jobInformer.Informer().HasSynced
 
@@ -176,10 +200,27 @@ func (cc *jobcontroller) Initialize(opt *framework.ControllerOption) error {
 	cc.cmdSynced = cc.cmdInformer.Informer().HasSynced
 
 	cc.podInformer = sharedInformers.Core().V1().Pods()
-	cc.podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
-		AddFunc:    cc.addPod,
-		UpdateFunc: cc.updatePod,
-		DeleteFunc: cc.deletePod,
+	cc.podInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{
+		FilterFunc: func(obj interface{}) bool {
+			switch v := obj.(type) {
+			case *v1.Pod:
+				if len(opt.Namespaces) > 0 {
+					if _, ok := namespaceMap[v.Namespace]; !ok {
+						klog.Warningf("Pod <%s/%s> is not in watching namespace: %v, so skip it",
+							v.Namespace, v.Name, opt.Namespaces)
+						return false
+					}
+				}
+				return true
+			default:
+				return false
+			}
+		},
+		Handler: cache.ResourceEventHandlerFuncs{
+			AddFunc:    cc.addPod,
+			UpdateFunc: cc.updatePod,
+			DeleteFunc: cc.deletePod,
+		},
 	})
 
 	cc.podLister = cc.podInformer.Lister()
@@ -194,9 +235,27 @@ func (cc *jobcontroller) Initialize(opt *framework.ControllerOption) error {
 	cc.svcSynced = cc.svcInformer.Informer().HasSynced
 
 	cc.pgInformer = informerfactory.NewSharedInformerFactory(cc.vcClient, 0).Scheduling().V1beta1().PodGroups()
-	cc.pgInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
-		UpdateFunc: cc.updatePodGroup,
-	})
+	cc.pgInformer.Informer().AddEventHandler(
+		cache.FilteringResourceEventHandler{
+			FilterFunc: func(obj interface{}) bool {
+				switch v := obj.(type) {
+				case *schedulingv1beta1.PodGroup:
+					if len(opt.Namespaces) > 0 {
+						if _, ok := namespaceMap[v.Namespace]; !ok {
+							klog.Warningf("PodGroup <%s/%s> is not in watching namespace: %v, so skip it",
+								v.Namespace, v.Name, opt.Namespaces)
+							return false
+						}
+					}
+					return true
+				default:
+					return false
+				}
+			},
+			Handler: cache.ResourceEventHandlerFuncs{
+				UpdateFunc: cc.updatePodGroup,
+			},
+		})
 	cc.pgLister = cc.pgInformer.Lister()
 	cc.pgSynced = cc.pgInformer.Informer().HasSynced
 
diff --git a/pkg/controllers/podgroup/pg_controller.go b/pkg/controllers/podgroup/pg_controller.go
index cb72906..11122e3 100644
--- a/pkg/controllers/podgroup/pg_controller.go
+++ b/pkg/controllers/podgroup/pg_controller.go
@@ -67,6 +67,11 @@ func (pg *pgcontroller) Initialize(opt *framework.ControllerOption) error {
 	pg.kubeClient = opt.KubeClient
 	pg.vcClient = opt.VolcanoClient
 
+	namespaceMap := make(map[string]struct{})
+	for _, ns := range opt.Namespaces {
+		namespaceMap[ns] = struct{}{}
+	}
+
 	pg.queue = workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
 
 	pg.podInformer = opt.SharedInformerFactory.Core().V1().Pods()
@@ -77,6 +82,13 @@ func (pg *pgcontroller) Initialize(opt *framework.ControllerOption) error {
 			FilterFunc: func(obj interface{}) bool {
 				switch v := obj.(type) {
 				case *v1.Pod:
+					if len(opt.Namespaces) > 0 {
+						if _, ok := namespaceMap[v.Namespace]; !ok {
+							klog.Warningf("Pod <%s/%s> is not in watching namespace: %v, so skip it",
+								v.Namespace, v.Name, opt.Namespaces)
+							return false
+						}
+					}
 					if v.Spec.SchedulerName == opt.SchedulerName &&
 						(v.Annotations == nil || v.Annotations[scheduling.KubeGroupNameAnnotationKey] == "") {
 						return true
diff --git a/pkg/controllers/podgroup/pg_controller_handler.go b/pkg/controllers/podgroup/pg_controller_handler.go
index 76fdf32..1b15003 100644
--- a/pkg/controllers/podgroup/pg_controller_handler.go
+++ b/pkg/controllers/podgroup/pg_controller_handler.go
@@ -25,8 +25,10 @@ import (
 	"k8s.io/apimachinery/pkg/runtime/schema"
 	"k8s.io/klog"
 
+	batch "volcano.sh/apis/pkg/apis/batch/v1alpha1"
 	"volcano.sh/apis/pkg/apis/helpers"
 	scheduling "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
 )
 
 type podRequest struct {
@@ -92,9 +94,10 @@ func (pg *pgcontroller) createNormalPodPGIfNotExist(pod *v1.Pod) error {
 			Spec: scheduling.PodGroupSpec{
 				MinMember:         1,
 				PriorityClassName: pod.Spec.PriorityClassName,
+				MinResources:      common.CalcPodResources(pod),
 			},
 		}
-		if queueName, ok := pod.Annotations[scheduling.QueueNameAnnotationKey]; ok {
+		if queueName, ok := pod.Annotations[batch.QueueNameKey]; ok {
 			obj.Spec.Queue = queueName
 		}
 
diff --git a/pkg/scheduler/actions/allocate/allocate.go b/pkg/scheduler/actions/allocate/allocate.go
index 2720c98..7d8f7e9 100644
--- a/pkg/scheduler/actions/allocate/allocate.go
+++ b/pkg/scheduler/actions/allocate/allocate.go
@@ -58,7 +58,8 @@ func (alloc *Action) Execute(ssn *framework.Session) {
 	jobsMap := map[api.NamespaceName]map[api.QueueID]*util.PriorityQueue{}
 
 	for _, job := range ssn.Jobs {
-		if job.PodGroup.Status.Phase == scheduling.PodGroupPending {
+		klog.V(4).Infof("PodGroup <%s/%s> Status %+v", job.PodGroup.Namespace, job.PodGroup.Name, job.PodGroup.Status)
+		if job.PodGroup.Status.Phase == scheduling.PodGroupPending || job.PodGroup.Status.Phase == "" {
 			continue
 		}
 		if vr := ssn.JobValid(job); vr != nil && !vr.Pass {
@@ -182,7 +183,9 @@ func (alloc *Action) Execute(ssn *framework.Session) {
 						task.Namespace, task.Name)
 					continue
 				}
-
+				// print task affinity
+				klog.V(3).Infof("Task <%v/%v> is on queue[%+v], and affinity is [%+v].",
+					task.Namespace, task.Name, queue.Name, task.Pod.Spec.Affinity)
 				tasks.Push(task)
 			}
 			pendingTasks[job.UID] = tasks
diff --git a/pkg/scheduler/actions/backfill/backfill.go b/pkg/scheduler/actions/backfill/backfill.go
index 0add143..d225a2b 100644
--- a/pkg/scheduler/actions/backfill/backfill.go
+++ b/pkg/scheduler/actions/backfill/backfill.go
@@ -43,7 +43,7 @@ func (alloc *Action) Execute(ssn *framework.Session) {
 
 	// TODO (k82cn): When backfill, it's also need to balance between Queues.
 	for _, job := range ssn.Jobs {
-		if job.PodGroup.Status.Phase == scheduling.PodGroupPending {
+		if job.PodGroup.Status.Phase == scheduling.PodGroupPending || job.PodGroup.Status.Phase == "" {
 			continue
 		}
 		if vr := ssn.JobValid(job); vr != nil && !vr.Pass {
diff --git a/pkg/scheduler/actions/enqueue/enqueue.go b/pkg/scheduler/actions/enqueue/enqueue.go
index 980813f..4f9b003 100644
--- a/pkg/scheduler/actions/enqueue/enqueue.go
+++ b/pkg/scheduler/actions/enqueue/enqueue.go
@@ -94,6 +94,7 @@ func (enqueue *Action) Execute(ssn *framework.Session) {
 		if job.PodGroup.Spec.MinResources == nil || ssn.JobEnqueueable(job) {
 			job.PodGroup.Status.Phase = scheduling.PodGroupInqueue
 			ssn.Jobs[job.UID] = job
+			klog.V(3).Infof("Enqueue PodGroup<%s/%s>.", job.PodGroup.Namespace, job.PodGroup.Name)
 		}
 
 		// Added Queue back until no job in Queue.
diff --git a/pkg/scheduler/actions/factory.go b/pkg/scheduler/actions/factory.go
index 6f8de96..531c3f7 100644
--- a/pkg/scheduler/actions/factory.go
+++ b/pkg/scheduler/actions/factory.go
@@ -22,6 +22,7 @@ import (
 	"volcano.sh/volcano/pkg/scheduler/actions/elect"
 	"volcano.sh/volcano/pkg/scheduler/actions/enqueue"
 	"volcano.sh/volcano/pkg/scheduler/actions/preempt"
+	"volcano.sh/volcano/pkg/scheduler/actions/preempt-pf"
 	"volcano.sh/volcano/pkg/scheduler/actions/reclaim"
 	"volcano.sh/volcano/pkg/scheduler/actions/reserve"
 	"volcano.sh/volcano/pkg/scheduler/framework"
@@ -35,4 +36,5 @@ func init() {
 	framework.RegisterAction(enqueue.New())
 	framework.RegisterAction(elect.New())
 	framework.RegisterAction(reserve.New())
+	framework.RegisterAction(preemptpf.New())
 }
diff --git a/pkg/scheduler/actions/preempt-pf/candidate.go b/pkg/scheduler/actions/preempt-pf/candidate.go
new file mode 100644
index 0000000..15a5de9
--- /dev/null
+++ b/pkg/scheduler/actions/preempt-pf/candidate.go
@@ -0,0 +1,73 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package preemptpf
+
+import (
+	"volcano.sh/volcano/pkg/scheduler/api"
+)
+
+// Victims represents:
+//   tasks:  a group of pods expected to be preempted.
+type Victims struct {
+	Tasks map[api.TaskID]*api.TaskInfo
+}
+
+type Pipelines struct {
+	Tasks map[api.TaskID]*api.TaskInfo
+}
+
+// Candidate represents a nominated node on which the preemptor can be scheduled,
+// along with the list of victims that should be evicted for the preemptor to fit the node.
+type Candidate interface {
+	// Victims wraps a list of to-be-preempted Tasks and the number of PDB violation.
+	Victims() *Victims
+	// Pipelines wraps a list of to-be-pipelined Tasks
+	Pipelines() *Pipelines
+	// Name returns the target node name where the preemptor gets nominated to run.
+	Name() string
+}
+
+type candidate struct {
+	victims   *Victims
+	pipelines *Pipelines
+	name      string
+}
+
+// Victims returns s.victims.
+func (c *candidate) Victims() *Victims {
+	return c.victims
+}
+
+// Pipelines returns s.pipelines.
+func (c *candidate) Pipelines() *Pipelines {
+	return c.pipelines
+}
+
+// Name returns s.name.
+func (c *candidate) Name() string {
+	return c.name
+}
+
+func NewCandidate(nodeName string, tasksMap map[api.TaskID]*api.TaskInfo) candidate {
+	victims := Victims{
+		Tasks: tasksMap,
+	}
+	return candidate{
+		victims: &victims,
+		name:    nodeName,
+	}
+}
diff --git a/pkg/scheduler/actions/preempt-pf/cross_node_preemption.go b/pkg/scheduler/actions/preempt-pf/cross_node_preemption.go
new file mode 100644
index 0000000..5731c02
--- /dev/null
+++ b/pkg/scheduler/actions/preempt-pf/cross_node_preemption.go
@@ -0,0 +1,243 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package preemptpf
+
+import (
+	"fmt"
+	"sort"
+
+	"k8s.io/klog"
+
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/util"
+)
+
+type candidateJob struct {
+	JobInfo  *api.JobInfo
+	NodeTask map[string]*api.Resource
+	Cost     float64
+	Reward   float64
+}
+
+func NewCandidateJob(jobInfo *api.JobInfo) *candidateJob {
+	candJob := &candidateJob{
+		JobInfo:  jobInfo,
+		NodeTask: make(map[string]*api.Resource),
+	}
+	for _, task := range jobInfo.Tasks {
+		if nodeResource, ok := candJob.NodeTask[task.NodeName]; !ok {
+			candJob.NodeTask[task.NodeName] = task.Resreq.Clone()
+		} else {
+			candJob.NodeTask[task.NodeName] = nodeResource.Add(task.Resreq)
+		}
+	}
+	return candJob
+}
+
+func (cj *candidateJob) String() string {
+	cjStr := fmt.Sprintf("candidat job info: [%v], Cost:[%f], Reward[%f]. tasks on node info: \n", cj.JobInfo, cj.Cost, cj.Reward)
+	for nodeName, rs := range cj.NodeTask {
+		cjStr += fmt.Sprintf("node name: [%s], allocated resources: [%+v]", nodeName, rs)
+	}
+	return cjStr
+}
+
+func SelectCandidates(
+	ssn *framework.Session,
+	preemptorJob *api.JobInfo,
+	preemptors []*api.TaskInfo,
+	victimJobs []*candidateJob,
+	candNodes []*api.NodeInfo) []Candidate {
+
+	if len(preemptors) == 0 || preemptorJob == nil {
+		return nil
+	}
+
+	candidateNodeMap := make(map[string]*api.NodeInfo)
+	for _, node := range candNodes {
+		klog.V(3).Infof("candidate node <%+v>", node)
+		candidateNodeMap[node.Name] = node
+	}
+
+	candidateNodes := make(map[string]*api.Resource)
+	for _, job := range victimJobs {
+		klog.V(3).Infof("victim job on queue <%s>, candidate job info <%+v>", job.JobInfo.Queue, job)
+		for name, allocated := range job.NodeTask {
+			if _, find := candidateNodeMap[name]; !find {
+				continue
+			}
+			if _, find := candidateNodes[name]; !find {
+				candidateNodes[name] = ssn.Nodes[name].Idle
+			}
+			candidateNodes[name] = candidateNodes[name].Add(allocated)
+		}
+		if candidateNodeFit(candidateNodes, preemptorJob, preemptors) {
+			klog.V(4).Infof("candidate node is enough for preempt job <%s/%s>",
+				candidateNodes, preemptorJob.Namespace, preemptorJob.Name)
+			break
+		}
+	}
+	klog.V(4).Infof("candidate nodes info <%+v>", candidateNodes)
+
+	var bestCandidate []Candidate
+	klog.V(3).Infof("best candidates [%+v]", bestCandidate)
+	return bestCandidate
+}
+
+func candidateNodeFit(
+	candidateNodes map[string]*api.Resource,
+	preemptorJob *api.JobInfo,
+	preemptors []*api.TaskInfo) bool {
+
+	// TODO(dongzezhao) check candidateNodeFit
+	return true
+}
+
+func victimJobOrderFn(preemptorJob *api.JobInfo, victimJobs []*api.JobInfo) []*api.JobInfo {
+	preemptorVictimJobs := &PreemptorVictims{
+		PreemptorJob: preemptorJob,
+		VictimJobs:   victimJobs,
+		// TODO(dongzezhao): make these weight configurable
+		RunTimeThreshold:    1.0,
+		GpusFactorWeight:    1.0,
+		PodsFactorWeight:    1.0,
+		RunTimeFactorWeight: 1.0,
+	}
+	sort.Sort(preemptorVictimJobs)
+	return preemptorVictimJobs.VictimJobs
+}
+
+func KMSelectCandidates(
+	ssn *framework.Session,
+	stmt *framework.Statement,
+	preemptorJob *api.JobInfo,
+	victimJobToTasks map[api.JobID][]*api.TaskInfo,
+	candidateNodes map[string]*api.NodeInfo) {
+
+	var victimJobs []*api.JobInfo
+	for jobID := range victimJobToTasks {
+		if vjob, found := ssn.Jobs[jobID]; found {
+			victimJobs = append(victimJobs, vjob)
+		}
+	}
+
+	sortedVictimJobs := victimJobOrderFn(preemptorJob, victimJobs)
+	victimIdx := len(sortedVictimJobs) - 1
+	for {
+		// 1. pending task
+		preemptorTasks := util.NewPriorityQueue(ssn.TaskOrderFn)
+		for _, task := range preemptorJob.TaskStatusIndex[api.Pending] {
+			preemptorTasks.Push(task)
+		}
+		// 2. 
+		if len(candidateNodes) != 0 {
+			for {
+				// jobminavailable, 
+				if !ssn.JobStarving(preemptorJob) {
+					break
+				}
+				if preemptorTasks.Empty() {
+					klog.V(3).Infof("No preemptor task in job <%s/%s>.",
+						preemptorJob.Namespace, preemptorJob.Name)
+					break
+				}
+				preemptor := preemptorTasks.Pop().(*api.TaskInfo)
+				if node := SchedulePreemptorTask(ssn, stmt, preemptor, candidateNodes); node != nil {
+					if err := stmt.Pipeline(preemptor, node.Name); err != nil {
+						// Ignore pipeline error, will be corrected in next scheduling loop.
+						klog.Errorf("Failed to pipeline Task <%s/%s> on Node <%s>",
+							preemptor.Namespace, preemptor.Name, node.Name)
+					}
+				}
+			}
+		}
+		// 3. 
+		if !ssn.JobStarving(preemptorJob) {
+			break
+		}
+		// 4. 
+		if victimIdx == -1 {
+			break
+		}
+		victimJob := sortedVictimJobs[victimIdx]
+		victimIdx--
+		// 5. jobtask
+		msg := fmt.Sprintf("preempted by %s/%s", preemptorJob.Namespace, preemptorJob.Name)
+		subCandidateNodes := stmt.EvictJob(victimJobToTasks[victimJob.UID], msg)
+		for _, nodeName := range subCandidateNodes {
+			if _, exist := candidateNodes[nodeName]; !exist {
+				if node, found := ssn.Nodes[nodeName]; found {
+					candidateNodes[nodeName] = node
+				}
+			}
+		}
+	}
+}
+
+// SchedulePreemptorTask try to schedule preemptor task.
+func SchedulePreemptorTask(
+	ssn *framework.Session,
+	stmt *framework.Statement,
+	preemptor *api.TaskInfo,
+	candidateNodes map[string]*api.NodeInfo,
+) *api.NodeInfo {
+	var allNodes []*api.NodeInfo
+	if len(candidateNodes) == 0 {
+		return nil
+	}
+	allNodes = util.GetNodeList(candidateNodes)
+	predicateNodes, _ := util.PredicateNodes(preemptor, allNodes, ssn.PredicateFn)
+	if len(predicateNodes) == 0 {
+		return nil
+	}
+	nodeScores := util.PrioritizeNodes(preemptor, predicateNodes, ssn.BatchNodeOrderFn, ssn.NodeOrderMapFn, ssn.NodeOrderReduceFn)
+	node := ssn.BestNodeFn(preemptor, nodeScores)
+	if node == nil {
+		node = util.SelectBestNode(nodeScores)
+	}
+	return node
+}
+
+func PrepareCandidate(
+	ssn *framework.Session,
+	stmt *framework.Statement,
+	preemptorJob *api.JobInfo,
+	candidates []Candidate) error {
+
+	for _, candidateNode := range candidates {
+		nodeName := candidateNode.Name()
+		victims := candidateNode.Victims().Tasks
+		klog.V(3).Infof("begin to evict victim[%+v] for preemptor[%s/%s]", victims, preemptorJob.Namespace, preemptorJob.Name)
+		for _, victim := range victims {
+			if err := stmt.Evict(victim, "preempt"); err != nil {
+				klog.Errorf("Failed to preempt Task <%s/%s> for Job <%s/%s>: %v",
+					victim.Namespace, victim.Name, preemptorJob.Namespace, preemptorJob.Name, err)
+				continue
+			}
+		}
+		pipelines := candidateNode.Pipelines().Tasks
+		for _, preemptor := range pipelines {
+			if err := stmt.Pipeline(preemptor, nodeName); err != nil {
+				klog.Errorf("Failed to pipeline Task <%s/%s> on Node <%s>",
+					preemptor.Namespace, preemptor.Name, nodeName)
+				return fmt.Errorf("preemptor failed on node[%s]", nodeName)
+			}
+		}
+	}
+	return nil
+}
diff --git a/pkg/scheduler/actions/preempt-pf/preempt_pf.go b/pkg/scheduler/actions/preempt-pf/preempt_pf.go
new file mode 100644
index 0000000..1d671fd
--- /dev/null
+++ b/pkg/scheduler/actions/preempt-pf/preempt_pf.go
@@ -0,0 +1,317 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package preemptpf
+
+import (
+	"context"
+	"fmt"
+	"sync"
+
+	"k8s.io/client-go/util/workqueue"
+	"k8s.io/klog"
+	"volcano.sh/apis/pkg/apis/scheduling"
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/metrics"
+	"volcano.sh/volcano/pkg/scheduler/util"
+)
+
+type Action struct{}
+
+func New() *Action {
+	return &Action{}
+}
+
+func (alloc *Action) Name() string {
+	return "preemptpf"
+}
+
+func (alloc *Action) Initialize() {}
+
+func (alloc *Action) Execute(ssn *framework.Session) {
+	klog.V(3).Infof("Enter preempt-pf with capacity scheduling ...")
+	defer klog.V(3).Infof("Leaving preempt-pf with capacity scheduling ...")
+
+	preemptorsMap := map[api.QueueID]*util.PriorityQueue{}
+	preemptorTasks := map[api.JobID]*util.PriorityQueue{}
+
+	var underRequest []*api.JobInfo
+	queues := map[api.QueueID]*api.QueueInfo{}
+
+	// victim Jobs from overused queue
+	victimJobsMap := map[api.QueueID]*util.PriorityQueue{}
+	for _, job := range ssn.Jobs {
+		if job.PodGroup.Status.Phase == scheduling.PodGroupPending || job.PodGroup.Status.Phase == "" {
+			continue
+		}
+		if vr := ssn.JobValid(job); vr != nil && !vr.Pass {
+			klog.V(4).Infof("Job <%s/%s> Queue <%s> skip preemption, reason: %v, message %v", job.Namespace, job.Name, job.Queue, vr.Reason, vr.Message)
+			continue
+		}
+		if queue, found := ssn.Queues[job.Queue]; !found {
+			continue
+		} else if _, existed := queues[queue.UID]; !existed {
+			klog.V(3).Infof("Added Queue <%s> for Job <%s/%s>",
+				queue.Name, job.Namespace, job.Name)
+			queues[queue.UID] = queue
+		}
+		// record running job
+		if job.PodGroup.Status.Phase == scheduling.PodGroupRunning {
+			if _, found := victimJobsMap[job.Queue]; !found {
+				victimJobsMap[job.Queue] = util.NewPriorityQueue(VictimJobOrderFn)
+			}
+			victimJobsMap[job.Queue].Push(job)
+		}
+
+		// check capacity of queue
+		queue := queues[job.Queue]
+		if queue.OverUsed(job.TotalRequest, queue.Max) {
+			klog.V(3).Infof("Queue <%s> for Job <%s/%s> is over max, max resource[%v], used resource[%v], and job total request[%v]",
+				queue.Name, job.Namespace, job.Name, *queue.Max, *queue.Used, *job.TotalRequest)
+			continue
+		}
+
+		// skip preempt in queue
+		if queue.OverUsed(job.TotalRequest, queue.Min) {
+			// If Job.Request + Queue.Used > Queue.Min:
+			// It means that its guaranteed isn't borrowed by other queues, then preempt in this queue
+			klog.V(3).Infof("Queue <%s> for Job <%s/%s> is over min, min resource[%v], used resource[%v], and job total request[%v]",
+				queue.Name, job.Namespace, job.Name, *queue.Min, *queue.Used, *job.TotalRequest)
+			continue
+		}
+
+		// check job if starting for more resources.
+		if ssn.JobStarving(job) {
+			if _, found := preemptorsMap[job.Queue]; !found {
+				preemptorsMap[job.Queue] = util.NewPriorityQueue(ssn.JobOrderFn)
+			}
+			preemptorsMap[job.Queue].Push(job)
+			underRequest = append(underRequest, job)
+			preemptorTasks[job.UID] = util.NewPriorityQueue(ssn.TaskOrderFn)
+			for _, task := range job.TaskStatusIndex[api.Pending] {
+				preemptorTasks[job.UID].Push(task)
+			}
+		}
+	}
+
+	victimJobs := make(map[api.JobID][]*api.TaskInfo)
+	// get victim jobs from overused queue
+	for _, queue := range queues {
+		if !queue.OverUsed(api.EmptyResource(), queue.Min) {
+			klog.V(5).Infof("Queue <%s> is not overused, skip.", queue.Name)
+			continue
+		}
+		releaseResource := api.EmptyResource()
+		usedResource := queue.Used.Clone()
+		victimJobsOnQueue := victimJobsMap[queue.UID]
+		for {
+			if victimJobsOnQueue == nil || victimJobsOnQueue.Empty() {
+				klog.V(4).Infof("No victim jobs in Queue <%s>, break.", queue.Name)
+				break
+			}
+			job := victimJobsOnQueue.Pop().(*api.JobInfo)
+			if _, found := victimJobs[job.UID]; !found {
+				victimJobs[job.UID] = make([]*api.TaskInfo, 0)
+			}
+			for _, task := range job.Tasks {
+				victimJobs[job.UID] = append(victimJobs[job.UID], task)
+			}
+			releaseResource.Add(job.Allocated)
+			usedResource = usedResource.Sub(job.Allocated)
+
+			klog.V(5).Infof("releasing job <%+v>", job)
+			if usedResource.LessEqualStrict(queue.Min) {
+				klog.V(4).Infof("After release job <%s/%s>, queue <%s> is not overused, break.",
+					job.Namespace, job.Name, queue.Name)
+				break
+			}
+		}
+		klog.V(4).Infof("Queue <%s> min resource <%+v>, used resource <%+v>, release resource <%+v>",
+			queue.Name, queue.Min, queue.Used, releaseResource)
+	}
+	klog.V(3).Infof("these are %d victim jobs from overused queue, victim jobs[%v]", len(victimJobs), victimJobs)
+
+	// Preemption between queues.
+	for _, queue := range queues {
+		for {
+			preemptorsOnQueue := preemptorsMap[queue.UID]
+
+			// If no preemptor, no preemption.
+			if preemptorsOnQueue == nil || preemptorsOnQueue.Empty() {
+				klog.V(4).Infof("No preemptors in Queue <%s>, break.", queue.Name)
+				break
+			}
+
+			preemptorJob := preemptorsOnQueue.Pop().(*api.JobInfo)
+			if len(preemptorJob.TaskStatusIndex[api.Pending]) == 0 {
+				continue
+			}
+
+			stmt := framework.NewStatement(ssn)
+			candidateNodes := map[string]*api.NodeInfo{}
+			KMSelectCandidates(ssn, stmt, preemptorJob, victimJobs, candidateNodes)
+
+			// Commit changes only if job is pipelined, otherwise try next job.
+			if ssn.JobPipelined(preemptorJob) {
+				stmt.Commit()
+			} else {
+				stmt.Discard()
+			}
+		}
+
+	}
+
+	// call victimTasksFn to evict tasks
+	victimTasks(ssn)
+}
+
+func (alloc *Action) UnInitialize() {}
+
+func victimTasks(ssn *framework.Session) {
+	stmt := framework.NewStatement(ssn)
+	victimTasks := ssn.VictimTasks()
+	for _, victim := range victimTasks {
+		if err := stmt.Evict(victim.Clone(), "evict"); err != nil {
+			klog.Errorf("Failed to evict Task <%s/%s>: %v",
+				victim.Namespace, victim.Name, err)
+			continue
+		}
+	}
+	stmt.Commit()
+}
+
+// VictimJobOrderFn sort job in queue
+func VictimJobOrderFn(l, r interface{}) bool {
+
+	lv := l.(*api.JobInfo)
+	rv := r.(*api.JobInfo)
+
+	// Compare job's priority
+	if lv.Priority != rv.Priority {
+		return lv.Priority < rv.Priority
+	}
+	// Compare job's resource
+	return lv.Allocated.LessEqualStrict(rv.Allocated)
+
+	// TODO(dongzezhao) compare job's running time
+}
+
+func preempt(
+	ssn *framework.Session,
+	stmt *framework.Statement,
+	victimJobs []*candidateJob,
+	preemptorJob *api.JobInfo,
+	preemptors []*api.TaskInfo,
+	filter func(*api.TaskInfo) bool,
+) (bool, error) {
+	if len(preemptors) == 0 || preemptorJob == nil {
+		return false, fmt.Errorf("no preempt task")
+	}
+
+	// 1. Select all preemption candidate nodes for preempt job
+	candidateNodes, err := FindCandidateNodes(ssn, preemptors[0])
+	if err != nil {
+		klog.Errorf("can not find any candidate node for preempt job <%s/%s>", preemptorJob.Namespace, preemptorJob.Name)
+		return false, err
+	}
+
+	// 2. Find the best candidates
+	bestCandidates := SelectCandidates(ssn, preemptorJob, preemptors, victimJobs, candidateNodes)
+	if bestCandidates == nil || len(bestCandidates) == 0 {
+		return false, fmt.Errorf("failed to find the test candidates")
+	}
+
+	// 3. PrepareCandidate
+	if err := PrepareCandidate(ssn, stmt, preemptorJob, bestCandidates); err != nil {
+		return false, fmt.Errorf("prepare candidate failed: %v\n", err)
+	}
+	return true, nil
+}
+
+func FindCandidateNodes(
+	ssn *framework.Session,
+	preemptor *api.TaskInfo) ([]*api.NodeInfo, error) {
+	// get potentialNodes to be preempted
+	potentialNodes := nodesWherePreemptionMightHelp(ssn, preemptor)
+	if len(potentialNodes) == 0 {
+		return nil, fmt.Errorf("can not find any candidate node")
+	}
+	return potentialNodes, nil
+}
+
+// nodesWherePreemptionMightHelp
+func nodesWherePreemptionMightHelp(ssn *framework.Session, preemptor *api.TaskInfo) []*api.NodeInfo {
+	allNodes := util.GetNodeList(ssn.Nodes)
+	predicateNodes, _ := util.PredicateNodes(preemptor, allNodes, ssn.PredicateFn)
+	return predicateNodes
+}
+
+// dryRunPreemption simulates Preemption logic on <potentialNodes> in parallel,
+// and returns all possible preemption candidates.
+func dryRunPreemption(ssn *framework.Session, preemptor *api.TaskInfo,
+	potentialNodes []*api.NodeInfo, filter func(*api.TaskInfo) bool) []candidate {
+	var resultLock sync.Mutex
+	var candidates []candidate
+	checkNode := func(i int) {
+		nodeInfoCopy := potentialNodes[i].Clone()
+
+		tasks, err := selectVictimsOnNode(ssn, preemptor, nodeInfoCopy, filter)
+		klog.V(3).Infof("selectVictimsOnNode [%d] tasks[%+v], err[%v].", len(tasks), tasks, err)
+		if err == nil && len(tasks) > 0 {
+			tasksMap := make(map[api.TaskID]*api.TaskInfo)
+			for _, task := range tasks {
+				tasksMap[task.UID] = task
+			}
+			c := NewCandidate(nodeInfoCopy.Name, tasksMap)
+			klog.V(3).Infof("append candidate name [%s], victim tasks [%+v].", c.Name(), c.Victims().Tasks)
+			resultLock.Lock()
+			candidates = append(candidates, c)
+			resultLock.Unlock()
+		}
+	}
+	workqueue.ParallelizeUntil(context.TODO(), 16, len(potentialNodes), checkNode)
+	klog.V(3).Infof("Candidate length is %d, info [%+v].", len(candidates), candidates)
+	return candidates
+}
+
+func selectVictimsOnNode(
+	ssn *framework.Session,
+	preemptor *api.TaskInfo,
+	nodeInfo *api.NodeInfo,
+	filter func(*api.TaskInfo) bool) ([]*api.TaskInfo, error) {
+	klog.V(3).Infof("Considering preemptor <%s/%s> on Node <%s>.",
+		preemptor.Namespace, preemptor.Name, nodeInfo.Name)
+
+	var preemptees []*api.TaskInfo
+	for _, task := range nodeInfo.Tasks {
+		if filter == nil {
+			preemptees = append(preemptees, task.Clone())
+		} else if filter(task) {
+			preemptees = append(preemptees, task.Clone())
+		}
+	}
+	//victims := ssn.Preemptable(preemptor, preemptees)
+	klog.V(5).Infof("victims on Node <%s> with filter, is:[%v]", nodeInfo.Name, preemptees)
+	metrics.UpdatePreemptionVictimsCount(len(preemptees))
+
+	if err := util.ValidateVictims(preemptor, nodeInfo, preemptees); err != nil {
+		klog.V(3).Infof("No validated victims on Node <%s>: %v", nodeInfo.Name, err)
+		return nil, err
+	}
+	klog.V(5).Infof("victims on Node <%s> after validate victims, is:[%v]", nodeInfo.Name, preemptees)
+	return preemptees, nil
+}
diff --git a/pkg/scheduler/actions/preempt-pf/preempt_pf_test.go b/pkg/scheduler/actions/preempt-pf/preempt_pf_test.go
new file mode 100644
index 0000000..a747c99
--- /dev/null
+++ b/pkg/scheduler/actions/preempt-pf/preempt_pf_test.go
@@ -0,0 +1,321 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package preemptpf
+
+import (
+	"testing"
+	"time"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/api/scheduling/v1beta1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/tools/record"
+
+	schedulingv1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/cmd/scheduler/app/options"
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/cache"
+	"volcano.sh/volcano/pkg/scheduler/conf"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/plugins/conformance"
+	"volcano.sh/volcano/pkg/scheduler/plugins/gang"
+	"volcano.sh/volcano/pkg/scheduler/util"
+)
+
+func TestPreempt(t *testing.T) {
+	framework.RegisterPluginBuilder("conformance", conformance.New)
+	framework.RegisterPluginBuilder("gang", gang.New)
+	options.ServerOpts = &options.ServerOption{
+		MinNodesToFind:             100,
+		MinPercentageOfNodesToFind: 5,
+		PercentageOfNodesToFind:    100,
+	}
+	defer framework.CleanupPluginBuilders()
+
+	tests := []struct {
+		name      string
+		podGroups []*schedulingv1.PodGroup
+		pods      []*v1.Pod
+		nodes     []*v1.Node
+		queues    []*schedulingv1.Queue
+		expected  int
+	}{
+		{
+			name: "do not preempt if there are enough idle resources",
+			podGroups: []*schedulingv1.PodGroup{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg1",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember: 3,
+						Queue:     "q1",
+					},
+				},
+			},
+			pods: []*v1.Pod{
+				util.BuildPod("c1", "preemptee1", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee2", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptor1", "", v1.PodPending, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+			},
+			// If there are enough idle resources on the node, then there is no need to preempt anything.
+			nodes: []*v1.Node{
+				util.BuildNode("n1", util.BuildResourceList("10", "10G"), make(map[string]string)),
+			},
+			queues: []*schedulingv1.Queue{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "q1",
+					},
+					Spec: schedulingv1.QueueSpec{
+						Weight: 1,
+					},
+				},
+			},
+			expected: 0,
+		},
+		{
+			name: "do not preempt if job is pipelined",
+			podGroups: []*schedulingv1.PodGroup{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg1",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember: 1,
+						Queue:     "q1",
+					},
+				},
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg2",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember: 1,
+						Queue:     "q1",
+					},
+				},
+			},
+			// Both pg1 and pg2 jobs are pipelined, because enough pods are already running.
+			pods: []*v1.Pod{
+				util.BuildPod("c1", "preemptee1", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee2", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee3", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg2", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptor2", "", v1.PodPending, util.BuildResourceList("1", "1G"), "pg2", make(map[string]string), make(map[string]string)),
+			},
+			// All resources on the node will be in use.
+			nodes: []*v1.Node{
+				util.BuildNode("n1", util.BuildResourceList("3", "3G"), make(map[string]string)),
+			},
+			queues: []*schedulingv1.Queue{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "q1",
+					},
+					Spec: schedulingv1.QueueSpec{
+						Weight: 1,
+					},
+				},
+			},
+			expected: 0,
+		},
+		{
+			name: "preempt one task of different job to fit both jobs on one node",
+			podGroups: []*schedulingv1.PodGroup{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg1",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember:         1,
+						Queue:             "q1",
+						PriorityClassName: "low-priority",
+					},
+				},
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg2",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember:         1,
+						Queue:             "q1",
+						PriorityClassName: "high-priority",
+					},
+				},
+			},
+
+			pods: []*v1.Pod{
+				util.BuildPod("c1", "preemptee1", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee2", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptor1", "", v1.PodPending, util.BuildResourceList("1", "1G"), "pg2", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptor2", "", v1.PodPending, util.BuildResourceList("1", "1G"), "pg2", make(map[string]string), make(map[string]string)),
+			},
+			nodes: []*v1.Node{
+				util.BuildNode("n1", util.BuildResourceList("2", "2G"), make(map[string]string)),
+			},
+			queues: []*schedulingv1.Queue{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "q1",
+					},
+					Spec: schedulingv1.QueueSpec{
+						Weight: 1,
+					},
+				},
+			},
+			expected: 1,
+		},
+		{
+			name: "preempt enough tasks to fit large task of different job",
+			podGroups: []*schedulingv1.PodGroup{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg1",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember:         1,
+						Queue:             "q1",
+						PriorityClassName: "low-priority",
+					},
+				},
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name:      "pg2",
+						Namespace: "c1",
+					},
+					Spec: schedulingv1.PodGroupSpec{
+						MinMember:         1,
+						Queue:             "q1",
+						PriorityClassName: "high-priority",
+					},
+				},
+			},
+			// There are 3 cpus and 3G of memory idle and 3 tasks running each consuming 1 cpu and 1G of memory.
+			// Big task requiring 5 cpus and 5G of memory should preempt 2 of 3 running tasks to fit into the node.
+			pods: []*v1.Pod{
+				util.BuildPod("c1", "preemptee1", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee2", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptee3", "n1", v1.PodRunning, util.BuildResourceList("1", "1G"), "pg1", make(map[string]string), make(map[string]string)),
+				util.BuildPod("c1", "preemptor1", "", v1.PodPending, util.BuildResourceList("5", "5G"), "pg2", make(map[string]string), make(map[string]string)),
+			},
+			nodes: []*v1.Node{
+				util.BuildNode("n1", util.BuildResourceList("6", "6G"), make(map[string]string)),
+			},
+			queues: []*schedulingv1.Queue{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "q1",
+					},
+					Spec: schedulingv1.QueueSpec{
+						Weight: 1,
+					},
+				},
+			},
+			expected: 2,
+		},
+	}
+
+	preempt := New()
+
+	for _, test := range tests {
+		t.Run(test.name, func(t *testing.T) {
+			binder := &util.FakeBinder{
+				Binds:   map[string]string{},
+				Channel: make(chan string),
+			}
+			evictor := &util.FakeEvictor{
+				Channel: make(chan string),
+			}
+			schedulerCache := &cache.SchedulerCache{
+				Nodes:           make(map[string]*api.NodeInfo),
+				Jobs:            make(map[api.JobID]*api.JobInfo),
+				Queues:          make(map[api.QueueID]*api.QueueInfo),
+				Binder:          binder,
+				Evictor:         evictor,
+				StatusUpdater:   &util.FakeStatusUpdater{},
+				VolumeBinder:    &util.FakeVolumeBinder{},
+				PriorityClasses: make(map[string]*v1beta1.PriorityClass),
+
+				Recorder: record.NewFakeRecorder(100),
+			}
+			schedulerCache.PriorityClasses["high-priority"] = &v1beta1.PriorityClass{
+				Value: 100000,
+			}
+			schedulerCache.PriorityClasses["low-priority"] = &v1beta1.PriorityClass{
+				Value: 10,
+			}
+			for _, node := range test.nodes {
+				schedulerCache.AddNode(node)
+			}
+			for _, pod := range test.pods {
+				schedulerCache.AddPod(pod)
+			}
+
+			for _, ss := range test.podGroups {
+				schedulerCache.AddPodGroupV1beta1(ss)
+			}
+
+			for _, q := range test.queues {
+				schedulerCache.AddQueueV1beta1(q)
+			}
+
+			trueValue := true
+			ssn := framework.OpenSession(schedulerCache, []conf.Tier{
+				{
+					Plugins: []conf.PluginOption{
+						{
+							Name:               "conformance",
+							EnabledPreemptable: &trueValue,
+						},
+						{
+							Name:                "gang",
+							EnabledPreemptable:  &trueValue,
+							EnabledJobPipelined: &trueValue,
+							EnabledJobStarving:  &trueValue,
+						},
+					},
+				},
+			}, nil)
+			defer framework.CloseSession(ssn)
+
+			preempt.Execute(ssn)
+
+			for i := 0; i < test.expected; i++ {
+				select {
+				case <-evictor.Channel:
+				case <-time.After(time.Second):
+					t.Errorf("not enough evictions")
+				}
+			}
+			select {
+			case key, opened := <-evictor.Channel:
+				if opened {
+					t.Errorf("unexpected eviction: %s", key)
+				}
+			case <-time.After(50 * time.Millisecond):
+				// TODO: Active waiting here is not optimal, but there is no better way currently.
+				//	 Ideally we would like to wait for evict and bind request goroutines to finish first.
+			}
+		})
+	}
+}
diff --git a/pkg/scheduler/actions/preempt-pf/victim_jobs.go b/pkg/scheduler/actions/preempt-pf/victim_jobs.go
new file mode 100644
index 0000000..49b4d93
--- /dev/null
+++ b/pkg/scheduler/actions/preempt-pf/victim_jobs.go
@@ -0,0 +1,205 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package preemptpf
+
+import (
+	"math"
+	"strconv"
+	"strings"
+	"time"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/klog"
+
+	"volcano.sh/volcano/pkg/scheduler/api"
+)
+
+// PreemptorVictims 
+type PreemptorVictims struct {
+	PreemptorJob        *api.JobInfo
+	RunTimeThreshold    float64
+	PodsFactorWeight    float64
+	GpusFactorWeight    float64
+	RunTimeFactorWeight float64
+	VictimJobs          []*api.JobInfo
+}
+
+func (preemptorVictims *PreemptorVictims) Len() int {
+	return len(preemptorVictims.VictimJobs)
+}
+func (preemptorVictims *PreemptorVictims) Swap(i, j int) {
+	preemptorVictims.VictimJobs[i], preemptorVictims.VictimJobs[j] = preemptorVictims.VictimJobs[j], preemptorVictims.VictimJobs[i]
+}
+
+func (preemptorVictims *PreemptorVictims) Less(i, j int) bool {
+	jobI := preemptorVictims.VictimJobs[i]
+	jobJ := preemptorVictims.VictimJobs[j]
+	// GPU
+	preemptorGPUPodNum, _, preemptorGPUNum := GetGPUTypeAndNum(preemptorVictims.PreemptorJob)
+	costTimeI, gpuPodNumI, GPUNumI := GetResourceInfo(jobI)
+	costTimeJ, gpuPodNumJ, GPUNumJ := GetResourceInfo(jobJ)
+	if jobI.Priority > jobJ.Priority { // 
+		return true
+	} else if jobI.Priority == jobJ.Priority {
+		// 
+		N := preemptorGPUPodNum
+		G := preemptorGPUNum
+		scoreI := preemptorVictims.PodsFactorWeight*numFactorScore(gpuPodNumI, float64(N)) +
+			preemptorVictims.GpusFactorWeight*numFactorScore(GPUNumI, float64(G)) +
+			preemptorVictims.RunTimeFactorWeight*float64(2.0)*timeFactorScore(costTimeI, preemptorVictims.RunTimeThreshold, float64(N))
+		scoreJ := preemptorVictims.PodsFactorWeight*numFactorScore(gpuPodNumJ, float64(N)) +
+			preemptorVictims.GpusFactorWeight*numFactorScore(GPUNumJ, float64(G)) +
+			preemptorVictims.RunTimeFactorWeight*float64(2.0)*timeFactorScore(costTimeJ, preemptorVictims.RunTimeThreshold, float64(N))
+		// 
+		klog.V(5).Infof("NameI: %s, costTimeI: %v, gpuPodNumI: %v, GPUNumI: %v, ScoreI: %v", jobI.Name, costTimeI, gpuPodNumI, GPUNumI, scoreI)
+		klog.V(5).Infof("NameJ: %s, costTimeJ: %v, gpuPodNumJ: %v, GPUNumJ: %v, ScoreJ: %v", jobJ.Name, costTimeJ, gpuPodNumJ, GPUNumJ, scoreJ)
+		if scoreI > scoreJ {
+			return true
+		}
+	}
+	return false
+}
+
+func numFactorScore(n float64, N float64) float64 {
+	if n < N {
+		return 2.0*N + 2.0 - n
+	} else if n > N {
+		return (N+1.0)/(N-n) + N + 2.0
+	} else {
+		return 0
+	}
+}
+
+func timeFactorScore(t float64, T float64, N float64) float64 {
+	l := float64((2.0*N + 2.0) * t / T)
+	r := float64(t - T + 2.0*N + 2.0)
+	return math.Max(l, r)
+}
+
+func CalculateJobScore(job *api.JobInfo, T float64, N float64, G float64, w1 float64, w2 float64, w3 float64) float64 {
+	t := GetPendingTime(job)
+	l := t / T
+	r := t - T + float64(1)
+	timeFactorScore := float64(2) * math.Max(l, r)
+	_, n, g := GetResourceInfo(job)
+	podNumsFactorScore := (N - n) / N
+	podGpusFactorScore := (G - g) / G
+	klog.V(5).Infof("Parameter: T %v, N %v, G %v, t %v, n %v, g %v.",
+		T, N, G, t, n, g)
+	klog.V(5).Infof("Result: timeFactorScore %v, podNumsFactorScore %v, podGpusFactorScore %v.",
+		timeFactorScore, podNumsFactorScore, podGpusFactorScore)
+	return w1*timeFactorScore + w2*podNumsFactorScore + w3*podGpusFactorScore
+}
+
+func GetPendingTime(jobInfo *api.JobInfo) float64 {
+	pendingTime := int64(0)
+	timeNow := time.Now().Unix()
+	timeStart := jobInfo.CreationTimestamp.Unix()
+	if timeStart != int64(0) {
+		pendingTime = timeNow - timeStart
+	}
+	return float64(pendingTime)
+}
+
+func GetResourceInfo(jobInfo *api.JobInfo) (float64, float64, float64) {
+	timeMax := int64(0)
+	timeNow := time.Now().Unix()
+	gpuPodNum := 0
+	gpuNum := float64(0)
+	// namespace := ""
+	if len(jobInfo.Tasks) > 0 {
+		// namespace = jobInfo.Namespace
+		for _, taskInfo := range jobInfo.Tasks {
+			var taskGpuCount float64 = 0
+			var taskGPUCore float64 = 0
+			var taskGPUMemory float64 = 0
+			for rName, value := range taskInfo.Resreq.ScalarResources {
+				if strings.HasSuffix(rName.String(), api.CGPUCore) && value > 0 {
+					taskGPUCore = value / 1000
+				} else if strings.HasSuffix(rName.String(), api.CGPUMemory) && value > 0 {
+					taskGPUMemory = value / 1000
+				} else if (strings.HasSuffix(rName.String(), api.CGPU) || strings.HasPrefix(rName.String(), api.RESOURCE_NVIDIA_GPU) ||
+					strings.HasPrefix(rName.String(), api.RESOURCE_BAIDU_GPU)) && value > 0 {
+					taskGpuCount = value / 1000
+				}
+			}
+			if taskGpuCount == 0 {
+				continue
+			}
+			if taskGPUCore == 0 && taskGPUMemory == 0 {
+				gpuNum = taskGpuCount
+			} else {
+				if len(taskInfo.Pod.Annotations) > 0 {
+					devGPUCore, devGPUCoreExist := taskInfo.Pod.Annotations[api.GPUCoreByDev]
+					devGPUMemory, devGPUMemoryExist := taskInfo.Pod.Annotations[api.GPUMemByDev]
+					var gpuNumByCore float64 = 0
+					var gpuNumByMemory float64 = 0
+					if devGPUCoreExist && taskGPUCore > 0 {
+						devGPUCoreUint, e := strconv.Atoi(devGPUCore)
+						if e != nil || devGPUCoreUint == 0 {
+							continue
+						}
+						gpuNumByCore = taskGPUCore / taskGpuCount / float64(devGPUCoreUint)
+					}
+					if devGPUMemoryExist && taskGPUMemory > 0 {
+						devGPUMemoryUint, e := strconv.Atoi(devGPUMemory)
+						if e != nil || devGPUMemoryUint == 0 {
+							continue
+						}
+						gpuNumByMemory = taskGPUMemory / taskGpuCount / float64(devGPUMemoryUint)
+					}
+					gpuNum = math.Max(gpuNumByCore, gpuNumByMemory) * taskGpuCount
+				}
+			}
+			gpuPodNum += 1
+			if taskInfo.Pod.Status.StartTime != nil {
+				timeStart := taskInfo.Pod.Status.StartTime.Time.Unix()
+				klog.V(5).Infof("%v start time %v %v", taskInfo.Name, taskInfo.Pod.Status.StartTime, timeStart)
+				if timeNow-timeStart > timeMax {
+					timeMax = timeNow - timeStart
+				}
+			}
+		}
+	}
+	return float64(timeMax), float64(gpuPodNum), gpuNum
+}
+
+// GetGPUTypeAndNum Job  MinAvailable  Task AGPU
+func GetGPUTypeAndNum(jobInfo *api.JobInfo) (float64, v1.ResourceName, float64) {
+	var gpuResource v1.ResourceName
+	var gpuNum float64
+	var gpuPodNum float64
+	tasks := jobInfo.Tasks
+	minTaskNums := 0
+	for _, taskInfo := range tasks {
+		//  MinAvailable  Task
+		minTaskNums += 1
+		if minTaskNums > int(jobInfo.MinAvailable) {
+			break
+		}
+		for rName, value := range taskInfo.Resreq.ScalarResources {
+			if (strings.HasSuffix(rName.String(), api.CGPU) || strings.HasPrefix(rName.String(), api.RESOURCE_NVIDIA_GPU) ||
+				strings.HasPrefix(rName.String(), api.RESOURCE_BAIDU_GPU)) && value > 0 {
+				gpuPodNum += 1
+				gpuResource = rName
+				gpuNum = value / 1000
+				break
+			}
+		}
+	}
+	return gpuPodNum, gpuResource, gpuNum
+}
diff --git a/pkg/scheduler/actions/preempt/preempt.go b/pkg/scheduler/actions/preempt/preempt.go
index ca9f820..053dee2 100644
--- a/pkg/scheduler/actions/preempt/preempt.go
+++ b/pkg/scheduler/actions/preempt/preempt.go
@@ -49,7 +49,7 @@ func (alloc *Action) Execute(ssn *framework.Session) {
 	queues := map[api.QueueID]*api.QueueInfo{}
 
 	for _, job := range ssn.Jobs {
-		if job.PodGroup.Status.Phase == scheduling.PodGroupPending {
+		if job.PodGroup.Status.Phase == scheduling.PodGroupPending || job.PodGroup.Status.Phase == "" {
 			continue
 		}
 		if vr := ssn.JobValid(job); vr != nil && !vr.Pass {
diff --git a/pkg/scheduler/api/const.go b/pkg/scheduler/api/const.go
new file mode 100644
index 0000000..d716095
--- /dev/null
+++ b/pkg/scheduler/api/const.go
@@ -0,0 +1,41 @@
+/*
+Copyright 2020 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package api
+
+const (
+	GPUCountName     = "kubernetes.io/baidu-cgpu.gpu-count"
+	GPUTopo          = "kubernetes.io/baidu-cgpu.gpu-topo"
+	NvidiaDriverLib  = "kubernetes.io/baidu-cgpu.nvidia-driver-lib"
+	GPUIndex         = "BAIDU_COM_GPU_IDX"
+	GPUMemByPod      = "BAIDU_COM_GPU_MEM_POD"
+	GPUCoreByPod     = "BAIDU_COM_GPU_CORE_POD"
+	GPUMemByDev      = "BAIDU_COM_GPU_MEM_DEVICE"
+	GPUCoreByDev     = "BAIDU_COM_GPU_CORE_DEVICE"
+	AssignedFlag     = "BAIDU_COM_GPU_ASSIGNED"
+	PredicateTime    = "BAIDU_COM_GPU_ASSUME_TIME"
+	GPUCoreIsolation = "BAIDU_COM_GPU_CORE_ISOLATION"
+	CGPU             = "cgpu"
+	CGPUMemory       = "cgpu_memory"
+	CGPUCore         = "cgpu_core"
+
+	RESOURCE_CPU               = "cpu"
+	RESOURCE_MEMORY            = "memory"
+	RESOURCE_STORAGE           = "storage"
+	RESOURCE_EPHEMERAL_STORAGE = "ephemeral-storage"
+	RESOURCE_NVIDIA_GPU        = "nvidia.com/gpu"
+	RESOURCE_BAIDU_GPU         = "baidu/gpu"
+)
diff --git a/pkg/scheduler/api/device_info.go b/pkg/scheduler/api/device_info.go
index 11edae2..f7975a6 100644
--- a/pkg/scheduler/api/device_info.go
+++ b/pkg/scheduler/api/device_info.go
@@ -17,7 +17,10 @@ limitations under the License.
 package api
 
 import (
+	"strings"
+
 	v1 "k8s.io/api/core/v1"
+	"k8s.io/klog"
 )
 
 // GPUDevice include gpu id, memory and the pods that are sharing it.
@@ -28,31 +31,170 @@ type GPUDevice struct {
 	PodMap map[string]*v1.Pod
 	// memory per card
 	Memory uint
+	// core per card
+	Core uint
 }
 
 // NewGPUDevice creates a device
-func NewGPUDevice(id int, mem uint) *GPUDevice {
+func NewGPUDevice(id int, core uint, mem uint) *GPUDevice {
 	return &GPUDevice{
 		ID:     id,
+		Core:   core,
 		Memory: mem,
 		PodMap: map[string]*v1.Pod{},
 	}
 }
 
+// isUsedGPU check the device used or not.
+func (g *GPUDevice) isUsedGPU() bool {
+	for _, pod := range g.PodMap {
+		if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
+			continue
+		} else {
+			return true
+		}
+	}
+	return false
+}
+
+// IsCGPUCoreIsolation check pod on device is core isolation or not.
+func (g *GPUDevice) IsCGPUCoreIsolation() bool {
+	for _, pod := range g.PodMap {
+		if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
+			continue
+		}
+		if isCGPUCoreIsolation(pod) {
+			return true
+		}
+	}
+	return false
+}
+
+func isCGPUCoreIsolation(pod *v1.Pod) bool {
+	isIsolation := false
+	for _, c := range pod.Spec.Containers {
+		for resType := range c.Resources.Requests {
+			if strings.HasSuffix(resType.String(), CGPUCore) {
+				isIsolation = true
+			}
+		}
+	}
+	return isIsolation
+}
+
 // getUsedGPUMemory calculates the used memory of the device.
 func (g *GPUDevice) getUsedGPUMemory() uint {
 	res := uint(0)
+	klog.V(4).Infof("getUsedGPUMemory pods num: %+v", len(g.PodMap))
 	for _, pod := range g.PodMap {
 		if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
 			continue
 		} else {
-			gpuRequest := GetGPUResourceOfPod(pod)
-			res += gpuRequest
+			gpuRequest := GetGPUMemoryOfPod(pod)
+			if gpuRequest == uint(0) {
+				res += g.Memory
+			} else {
+				res += gpuRequest
+			}
+			klog.V(4).Infof("pod %s gpuRequest mem: %+v", pod.Name, gpuRequest)
+
 		}
 	}
 	return res
 }
 
+// getUsedGPUCore calculates the used core of the device.
+func (g *GPUDevice) getUsedGPUCore() uint {
+	res := uint(0)
+	klog.V(4).Infof("getUsedGPUCore pods num: %+v", len(g.PodMap))
+	for _, pod := range g.PodMap {
+		if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
+			continue
+		} else {
+			gpuRequest := GetGPUCoreOfPod(pod)
+			if gpuRequest == uint(0) {
+				if !isCGPUMemoryIsolation(pod) {
+					res += g.Core
+				}
+			} else {
+				res += gpuRequest
+			}
+			klog.V(4).Infof("pod %s gpuRequest core: %+v", pod.Name, gpuRequest)
+		}
+	}
+	return res
+}
+
+func isCGPUMemoryIsolation(pod *v1.Pod) bool {
+	isIsolation := false
+	for _, c := range pod.Spec.Containers {
+		for resType := range c.Resources.Requests {
+			if strings.HasSuffix(resType.String(), CGPUMemory) {
+				isIsolation = true
+			}
+		}
+	}
+	return isIsolation
+}
+
+// GetGPUCountOfPod gets GPU Count of the Pod
+func GetGPUCountOfPod(pod *v1.Pod) uint {
+	var total uint
+	containers := pod.Spec.Containers
+	for _, container := range containers {
+		resourceName := getPodResourceNameByKey(pod, CGPU)
+		if val, ok := container.Resources.Requests[resourceName]; ok {
+			total += uint(val.Value())
+		}
+	}
+	klog.V(4).Infof("pod %s in ns %s has gpu count: %d", pod.Name, pod.Namespace, total)
+	return total
+}
+
+// GetGPUCoreOfPod returns the GPU core required by the pod.
+func GetGPUCoreOfPod(pod *v1.Pod) uint {
+	var core uint
+	resourceName := getPodResourceNameByKey(pod, CGPUCore)
+	for _, container := range pod.Spec.Containers {
+		if val, ok := container.Resources.Requests[resourceName]; ok {
+			core += uint(val.Value())
+		}
+	}
+	return core
+}
+
+// GetGPUMemoryOfPod returns the GPU memory required by the pod.
+func GetGPUMemoryOfPod(pod *v1.Pod) uint {
+	var mem uint
+	resourceName := getPodResourceNameByKey(pod, CGPUMemory)
+	for _, container := range pod.Spec.Containers {
+		if val, ok := container.Resources.Requests[resourceName]; ok {
+			mem += uint(val.Value())
+		}
+	}
+	return mem
+}
+
+func getPodResourceNameByKey(pod *v1.Pod, nameKey string) v1.ResourceName {
+	for _, name := range GetPodResourceName(pod) {
+		if strings.HasSuffix(name.String(), nameKey) {
+			return name
+		}
+	}
+	return v1.ResourceName("")
+}
+
+// GetPodResourceName get resource name from pod
+func GetPodResourceName(pod *v1.Pod) []v1.ResourceName {
+	var resourceNameList []v1.ResourceName
+	for _, container := range pod.Spec.Containers {
+		for name := range container.Resources.Requests {
+			resourceNameList = append(resourceNameList, name)
+		}
+	}
+	return resourceNameList
+}
+
 // GetGPUResourceOfPod returns the GPU resource required by the pod.
 func GetGPUResourceOfPod(pod *v1.Pod) uint {
 	var mem uint
diff --git a/pkg/scheduler/api/elastic_resource_quota_info.go b/pkg/scheduler/api/elastic_resource_quota_info.go
new file mode 100644
index 0000000..c09af14
--- /dev/null
+++ b/pkg/scheduler/api/elastic_resource_quota_info.go
@@ -0,0 +1,70 @@
+/*
+Copyright 2021 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package api
+
+import (
+	"k8s.io/apimachinery/pkg/types"
+	"volcano.sh/apis/pkg/apis/scheduling"
+)
+
+// ElasticResourceQuotaID is UID type, serves as unique ID for each ElasticResourceQuota
+type ElasticResourceQuotaID types.UID
+
+// ElasticResourceQuotaInfo will have all details about ElasticResourceQuota
+type ElasticResourceQuotaInfo struct {
+	UID       ElasticResourceQuotaID
+	Name      string
+	Namespace string
+
+	// ElasticResourceQuota Max
+	Max           *Resource
+	Min           *Resource
+	Used          *Resource
+	HardwareTypes []string
+
+	ElasticResourceQuota *scheduling.ElasticResourceQuota
+}
+
+// NewElasticResourceQuotaInfo creates new ElasticResourceQuotaInfo object
+func NewElasticResourceQuotaInfo(elasticQuota *scheduling.ElasticResourceQuota) *ElasticResourceQuotaInfo {
+	return &ElasticResourceQuotaInfo{
+		UID:       ElasticResourceQuotaID(elasticQuota.Name),
+		Name:      elasticQuota.Name,
+		Namespace: elasticQuota.Namespace,
+
+		Max:  NewResource(elasticQuota.Spec.Max),
+		Min:  NewResource(elasticQuota.Spec.Min),
+		Used: NewResource(elasticQuota.Status.Used),
+		// TODO(dongzezhao): get hardware types from annotation
+		HardwareTypes:        []string{},
+		ElasticResourceQuota: elasticQuota,
+	}
+}
+
+// Clone is used to clone ElasticResourceQuotaInfo object
+func (erq *ElasticResourceQuotaInfo) Clone() *ElasticResourceQuotaInfo {
+	return &ElasticResourceQuotaInfo{
+		UID:                  erq.UID,
+		Name:                 erq.Name,
+		Namespace:            erq.Namespace,
+		Max:                  erq.Max,
+		Min:                  erq.Min,
+		Used:                 erq.Used,
+		HardwareTypes:        erq.HardwareTypes,
+		ElasticResourceQuota: erq.ElasticResourceQuota,
+	}
+}
diff --git a/pkg/scheduler/api/job_info.go b/pkg/scheduler/api/job_info.go
index 580b89d..c7af1ba 100644
--- a/pkg/scheduler/api/job_info.go
+++ b/pkg/scheduler/api/job_info.go
@@ -583,3 +583,10 @@ func (ji *JobInfo) Ready() bool {
 
 	return occupied >= ji.MinAvailable
 }
+
+// Pipelined returns whether the number of ready and pipelined task is enough
+func (ji *JobInfo) Pipelined() bool {
+	occupied := ji.WaitingTaskNum() + ji.ReadyTaskNum()
+
+	return occupied >= ji.MinAvailable
+}
diff --git a/pkg/scheduler/api/node_info.go b/pkg/scheduler/api/node_info.go
index dc591e2..68a4234 100644
--- a/pkg/scheduler/api/node_info.go
+++ b/pkg/scheduler/api/node_info.go
@@ -18,6 +18,8 @@ package api
 
 import (
 	"fmt"
+	"strconv"
+	"strings"
 
 	v1 "k8s.io/api/core/v1"
 	"k8s.io/klog"
@@ -172,26 +174,40 @@ func (ni *NodeInfo) setNodeGPUInfo(node *v1.Node) {
 	if node == nil {
 		return
 	}
-	memory, ok := node.Status.Capacity[VolcanoGPUResource]
-	if !ok {
-		return
-	}
+	resourceName := ni.getNodeResourceNameByKey(CGPUCore)
+	core, _ := node.Status.Capacity[resourceName]
+	totalCore := core.Value()
+
+	resourceName = ni.getNodeResourceNameByKey(CGPUMemory)
+	memory, _ := node.Status.Capacity[resourceName]
 	totalMemory := memory.Value()
 
-	res, ok := node.Status.Capacity[VolcanoGPUNumber]
-	if !ok {
+	gpuCount := ni.GetGPUCount(node)
+	if gpuCount == 0 {
+		klog.Warningf("invalid %s=%d on node %s.", GPUCountName, gpuCount, node.Name)
 		return
 	}
-	gpuNumber := res.Value()
-	if gpuNumber == 0 {
-		klog.Warningf("invalid %s=%s", VolcanoGPUNumber, res.String())
-		return
+	klog.V(4).Infof("Get gpu info from node %s, totalCore: %d, totalMemory: %d, gpuCount: %d", node.Name, totalCore, totalMemory, gpuCount)
+	corePerCard := uint(totalCore / gpuCount)
+	memoryPerCard := uint(totalMemory / gpuCount)
+	for i := 0; i < int(gpuCount); i++ {
+		ni.GPUDevices[i] = NewGPUDevice(i, corePerCard, memoryPerCard)
 	}
+}
 
-	memoryPerCard := uint(totalMemory / gpuNumber)
-	for i := 0; i < int(gpuNumber); i++ {
-		ni.GPUDevices[i] = NewGPUDevice(i, memoryPerCard)
+// GetGPUCount get gpu count from node annotations
+func (ni *NodeInfo) GetGPUCount(node *v1.Node) int64 {
+	count := int64(0)
+	if node != nil && len(node.ObjectMeta.Annotations) > 0 {
+		if value, found := node.ObjectMeta.Annotations[GPUCountName]; found {
+			if gpuCount, err := strconv.ParseInt(value, 10, 64); err == nil {
+				count = gpuCount
+			} else {
+				klog.Warningf("Failed to parse %s from annotations on node %s", GPUCountName, node.Name)
+			}
+		}
 	}
+	return count
 }
 
 // SetNode sets kubernetes node object to nodeInfo object
@@ -363,10 +379,98 @@ func (ni *NodeInfo) Pods() (pods []*v1.Pod) {
 	return
 }
 
+// GetDeviceGPUMem return gpu memory per card
+func (ni *NodeInfo) GetDeviceGPUMem() int {
+	mem := 0
+	for _, dev := range ni.GPUDevices {
+		if dev != nil {
+			mem = int(dev.Memory)
+			break
+		}
+	}
+	return mem
+}
+
+// GetDeviceGPUCore return gpu core per card
+func (ni *NodeInfo) GetDeviceGPUCore() int {
+	core := 0
+	for _, dev := range ni.GPUDevices {
+		if dev != nil {
+			core = int(dev.Core)
+			break
+		}
+	}
+	return core
+}
+
+func (ni *NodeInfo) getNodeResourceNameByKey(nameKey string) v1.ResourceName {
+	for _, name := range ni.getNodeResourceName() {
+		if strings.HasSuffix(name.String(), nameKey) {
+			return name
+		}
+	}
+	return v1.ResourceName("")
+}
+
+func (ni *NodeInfo) getNodeResourceName() []v1.ResourceName {
+	var resourceNameList []v1.ResourceName
+	for name := range ni.Capability.ScalarResources {
+		resourceNameList = append(resourceNameList, name)
+	}
+	return resourceNameList
+}
+
+// GetEmptyDevices get empty devices
+func (ni *NodeInfo) GetEmptyDevices() map[int]bool {
+	res := map[int]bool{}
+	idleGPUs := ni.GetDevicesIdleGPU()
+	if len(idleGPUs) > 0 {
+		for _, id := range idleGPUs {
+			res[id] = true
+		}
+	}
+	return res
+}
+
+// GetDevicesIdleGPU returns all the idle GPU id.
+func (ni *NodeInfo) GetDevicesIdleGPU() []int {
+	res := make([]int, 0)
+	devicesAllGPU := ni.getDevicesAllGPU()
+	klog.V(4).Infof("getDevicesAllGPU: %+v on node: %s", devicesAllGPU, ni.Name)
+	deviceUsedGPU := ni.getDevicesUsedGPU()
+	klog.V(4).Infof("getDevicesUsedGPU: %+v on node: %s", deviceUsedGPU, ni.Name)
+	for id := range devicesAllGPU {
+		if used, found := deviceUsedGPU[id]; found && used {
+			continue
+		} else {
+			res = append(res, id)
+		}
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesAllGPU() map[int]bool {
+	res := map[int]bool{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = false
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesUsedGPU() map[int]bool {
+	res := map[int]bool{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.isUsedGPU()
+	}
+	return res
+}
+
 // GetDevicesIdleGPUMemory returns all the idle GPU memory by gpu card.
 func (ni *NodeInfo) GetDevicesIdleGPUMemory() map[int]uint {
 	devicesAllGPUMemory := ni.getDevicesAllGPUMemory()
+	klog.V(4).Infof("getDevicesAllGPUMemory: %+v on node: %s", devicesAllGPUMemory, ni.Name)
 	devicesUsedGPUMemory := ni.getDevicesUsedGPUMemory()
+	klog.V(4).Infof("devicesUsedGPUMemory: %+v on node: %s", devicesUsedGPUMemory, ni.Name)
 	res := map[int]uint{}
 	for id, allMemory := range devicesAllGPUMemory {
 		if usedMemory, found := devicesUsedGPUMemory[id]; found {
@@ -392,24 +496,76 @@ func (ni *NodeInfo) getDevicesAllGPUMemory() map[int]uint {
 	return res
 }
 
+// GetCoreIsolationDevices get core isolation devices on node
+func (ni *NodeInfo) GetCoreIsolationDevices() map[int]bool {
+	res := map[int]bool{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.IsCGPUCoreIsolation()
+	}
+	return res
+}
+
+// GetDevicesIdleGPUCore returns all the idle GPU core by gpu card.
+func (ni *NodeInfo) GetDevicesIdleGPUCore() map[int]uint {
+	devicesAllGPUCore := ni.getDevicesAllGPUCore()
+	klog.V(4).Infof("devicesAllGPUCore: %+v on node: %s", devicesAllGPUCore, ni.Name)
+	devicesUsedGPUCore := ni.getDevicesUsedGPUCore()
+	klog.V(4).Infof("devicesUsedGPUCore: %+v on node: %s", devicesUsedGPUCore, ni.Name)
+	res := map[int]uint{}
+	for id, allCore := range devicesAllGPUCore {
+		if usedCore, found := devicesUsedGPUCore[id]; found {
+			res[id] = allCore - usedCore
+		}
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesUsedGPUCore() map[int]uint {
+	res := map[int]uint{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.getUsedGPUCore()
+	}
+	return res
+}
+
+func (ni *NodeInfo) getDevicesAllGPUCore() map[int]uint {
+	res := map[int]uint{}
+	for _, device := range ni.GPUDevices {
+		res[device.ID] = device.Core
+	}
+	return res
+}
+
 // AddGPUResource adds the pod to GPU pool if it is assigned
 func (ni *NodeInfo) AddGPUResource(pod *v1.Pod) {
-	gpuRes := GetGPUResourceOfPod(pod)
+	gpuRes := GetGPUCountOfPod(pod)
 	if gpuRes > 0 {
-		id := GetGPUIndex(pod)
-		if dev := ni.GPUDevices[id]; dev != nil {
-			dev.PodMap[string(pod.UID)] = pod
+		ids, err := GetGPUIndexs(pod)
+		if err != nil {
+			klog.Errorf("Get pod %s gpu index failed, err: %v", pod.Name, err)
+			return
+		}
+		for _, id := range ids {
+			if dev := ni.GPUDevices[id]; dev != nil {
+				dev.PodMap[string(pod.UID)] = pod
+			}
 		}
 	}
 }
 
 // SubGPUResource frees the gpu hold by the pod
 func (ni *NodeInfo) SubGPUResource(pod *v1.Pod) {
-	gpuRes := GetGPUResourceOfPod(pod)
+	gpuRes := GetGPUCountOfPod(pod)
 	if gpuRes > 0 {
-		id := GetGPUIndex(pod)
-		if dev := ni.GPUDevices[id]; dev != nil {
-			delete(dev.PodMap, string(pod.UID))
+		ids, err := GetGPUIndexs(pod)
+		if err != nil {
+			klog.Errorf("Get pod %s gpu index failed, err: %v", pod.Name, err)
+			return
+		}
+		for _, id := range ids {
+			if dev := ni.GPUDevices[id]; dev != nil {
+				delete(dev.PodMap, string(pod.UID))
+			}
 		}
 	}
 }
diff --git a/pkg/scheduler/api/pod_info.go b/pkg/scheduler/api/pod_info.go
index 0b7890f..27bc18a 100644
--- a/pkg/scheduler/api/pod_info.go
+++ b/pkg/scheduler/api/pod_info.go
@@ -144,6 +144,27 @@ func GetGPUIndex(pod *v1.Pod) int {
 	return -1
 }
 
+// GetGPUIndexs returns the ID list of GPU
+func GetGPUIndexs(pod *v1.Pod) ([]int, error) {
+	gpuIndex := make([]int, 0)
+	if len(pod.Annotations) > 0 {
+		value, found := pod.Annotations[GPUIndex]
+		if found {
+			idxSlice := strings.Split(value, ",")
+			for _, idx := range idxSlice {
+				id, err := strconv.Atoi(idx)
+				if err != nil {
+					klog.Errorf("invalid %s=%s", GPUIndex, idx)
+					return nil, fmt.Errorf("invalid %s=%s on pod %s/%s", GPUIndex, idx, pod.Namespace, pod.Name)
+				}
+				gpuIndex = append(gpuIndex, id)
+			}
+			return gpuIndex, nil
+		}
+	}
+	return nil, fmt.Errorf("Failed to get GPU index %s on pod %s/%s", GPUIndex, pod.Namespace, pod.Name)
+}
+
 func escapeJSONPointer(p string) string {
 	// Escaping reference name using https://tools.ietf.org/html/rfc6901
 	p = strings.Replace(p, "~", "~0", -1)
@@ -164,3 +185,43 @@ func RemoveGPUIndexPatch() string {
 	return fmt.Sprintf(`[{"op": "remove", "path": "/metadata/annotations/%s"},`+
 		`{"op": "remove", "path": "/metadata/annotations/%s"]`, escapeJSONPointer(PredicateTime), escapeJSONPointer(GPUIndex))
 }
+
+// AddGPUInfoPatch returns the patch adding GPU info
+func AddGPUInfoPatch(node *NodeInfo, pod *v1.Pod, ids []int) string {
+	var devIds []string
+	for _, id := range ids {
+		devIds = append(devIds, strconv.Itoa(id))
+	}
+	return fmt.Sprintf(`[{"op": "add", "path": "/metadata/annotations/%s", "value":"%d"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%s"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%s"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%d"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%d"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%d"},`+
+		`{"op": "add", "path": "/metadata/annotations/%s", "value": "%d"}]`,
+		escapeJSONPointer(PredicateTime), time.Now().UnixNano(),
+		escapeJSONPointer(GPUIndex), strings.Join(devIds, ","),
+		escapeJSONPointer(AssignedFlag), "false",
+		escapeJSONPointer(GPUMemByPod), GetGPUMemoryOfPod(pod),
+		escapeJSONPointer(GPUCoreByPod), GetGPUCoreOfPod(pod),
+		escapeJSONPointer(GPUMemByDev), node.GetDeviceGPUMem(),
+		escapeJSONPointer(GPUCoreByDev), node.GetDeviceGPUCore())
+}
+
+// RemoveGPUInfoPatch returns the patch removing GPU info
+func RemoveGPUInfoPatch() string {
+	return fmt.Sprintf(`[{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"},`+
+		`{"op": "remove", "path": "/metadata/annotations/%s"}]`,
+		escapeJSONPointer(PredicateTime),
+		escapeJSONPointer(GPUIndex),
+		escapeJSONPointer(AssignedFlag),
+		escapeJSONPointer(GPUMemByPod),
+		escapeJSONPointer(GPUCoreByPod),
+		escapeJSONPointer(GPUMemByDev),
+		escapeJSONPointer(GPUCoreByDev))
+}
diff --git a/pkg/scheduler/api/queue_info.go b/pkg/scheduler/api/queue_info.go
index 3e9eaf9..562eac6 100644
--- a/pkg/scheduler/api/queue_info.go
+++ b/pkg/scheduler/api/queue_info.go
@@ -18,6 +18,7 @@ package api
 
 import (
 	"k8s.io/apimachinery/pkg/types"
+	"volcano.sh/volcano/pkg/common"
 
 	"volcano.sh/apis/pkg/apis/scheduling"
 	"volcano.sh/apis/pkg/apis/scheduling/v1beta1"
@@ -41,11 +42,23 @@ type QueueInfo struct {
 	// path from the root to the node itself.
 	Hierarchy string
 
-	Queue *scheduling.Queue
+	// HardwareTypes used in scheduling job
+	HardwareTypes []string
+
+	// Resource range of queue
+	Max  *Resource
+	Min  *Resource
+	Used *Resource
+
+	Queue                *scheduling.Queue
+	ElasticResourceQuota *scheduling.ElasticResourceQuota
 }
 
 // NewQueueInfo creates new queueInfo object
-func NewQueueInfo(queue *scheduling.Queue) *QueueInfo {
+func NewQueueInfo(queue *scheduling.Queue, elasticQuota *scheduling.ElasticResourceQuota) *QueueInfo {
+	hardwareTypesStr := queue.Annotations[v1beta1.QuotaHardwareTypeLabelKey]
+	hardwareTypes := common.StringToSlice(hardwareTypesStr)
+
 	return &QueueInfo{
 		UID:  QueueID(queue.Name),
 		Name: queue.Name,
@@ -54,19 +67,31 @@ func NewQueueInfo(queue *scheduling.Queue) *QueueInfo {
 		Hierarchy: queue.Annotations[v1beta1.KubeHierarchyAnnotationKey],
 		Weights:   queue.Annotations[v1beta1.KubeHierarchyWeightAnnotationKey],
 
-		Queue: queue,
+		HardwareTypes:     hardwareTypes,
+
+		Max:  NewResource(elasticQuota.Spec.Max),
+		Min:  NewResource(elasticQuota.Spec.Min),
+		Used: NewResource(elasticQuota.Status.Used),
+
+		Queue:                queue,
+		ElasticResourceQuota: elasticQuota,
 	}
 }
 
 // Clone is used to clone queueInfo object
 func (q *QueueInfo) Clone() *QueueInfo {
 	return &QueueInfo{
-		UID:       q.UID,
-		Name:      q.Name,
-		Weight:    q.Weight,
-		Hierarchy: q.Hierarchy,
-		Weights:   q.Weights,
-		Queue:     q.Queue,
+		UID:                  q.UID,
+		Name:                 q.Name,
+		Weight:               q.Weight,
+		Hierarchy:            q.Hierarchy,
+		Weights:              q.Weights,
+		Queue:                q.Queue,
+		HardwareTypes:        q.HardwareTypes,
+		Max:                  q.Max,
+		Min:                  q.Min,
+		Used:                 q.Used,
+		ElasticResourceQuota: q.ElasticResourceQuota,
 	}
 }
 
@@ -86,3 +111,8 @@ func (q *QueueInfo) Reclaimable() bool {
 
 	return *q.Queue.Spec.Reclaimable
 }
+
+func (q *QueueInfo) OverUsed(request, resource *Resource) bool {
+	usedResource := q.Used.Clone()
+	return !usedResource.Add(request).LessEqualStrict(resource)
+}
diff --git a/pkg/scheduler/api/resource_info.go b/pkg/scheduler/api/resource_info.go
index 42897b9..b70abb0 100644
--- a/pkg/scheduler/api/resource_info.go
+++ b/pkg/scheduler/api/resource_info.go
@@ -26,6 +26,20 @@ import (
 	"volcano.sh/volcano/pkg/scheduler/util/assert"
 )
 
+const (
+	minResource float64 = 0.1
+)
+
+// DimensionDefaultValue means default value for black resource dimension
+type DimensionDefaultValue int
+
+const (
+	// Zero means resource dimension not defined will be treated as zero
+	Zero DimensionDefaultValue = 0
+	// Infinity means resource dimension not defined will be treated as infinity
+	Infinity DimensionDefaultValue = -1
+)
+
 // Resource struct defines all the resource type
 type Resource struct {
 	MilliCPU float64
@@ -292,6 +306,37 @@ func (r *Resource) LessEqualStrict(rr *Resource) bool {
 	return true
 }
 
+// LessEqual returns true only on condition that all dimensions of resources in r are less than or equal with that of rr,
+// Otherwise returns false.
+// @param defaultValue "default value for resource dimension not defined in ScalarResources. Its value can only be one of 'Zero' and 'Infinity'"
+func (r *Resource) LessEqualNew(rr *Resource, defaultValue DimensionDefaultValue) bool {
+	lessEqualFunc := func(l, r, diff float64) bool {
+		if l < r || math.Abs(l-r) < diff {
+			return true
+		}
+		return false
+	}
+
+	if !lessEqualFunc(r.MilliCPU, rr.MilliCPU, minResource) {
+		return false
+	}
+	if !lessEqualFunc(r.Memory, rr.Memory, minResource) {
+		return false
+	}
+
+	for resourceName, leftValue := range r.ScalarResources {
+		rightValue, ok := rr.ScalarResources[resourceName]
+		if !ok && defaultValue == Infinity {
+			continue
+		}
+
+		if !lessEqualFunc(leftValue, rightValue, minResource) {
+			return false
+		}
+	}
+	return true
+}
+
 // LessEqual checks whether a resource is less than other resource
 func (r *Resource) LessEqual(rr *Resource) bool {
 	lessEqualFunc := func(l, r, diff float64) bool {
diff --git a/pkg/scheduler/api/well_known_labels.go b/pkg/scheduler/api/well_known_labels.go
index c412d24..34c0eba 100644
--- a/pkg/scheduler/api/well_known_labels.go
+++ b/pkg/scheduler/api/well_known_labels.go
@@ -24,7 +24,7 @@ const (
 	VolcanoGPUNumber = "volcano.sh/gpu-number"
 
 	// PredicateTime is the key of predicate time
-	PredicateTime = "volcano.sh/predicate-time"
+	//PredicateTime = "volcano.sh/predicate-time"
 	// GPUIndex is the key of gpu index
-	GPUIndex = "volcano.sh/gpu-index"
+	//GPUIndex = "volcano.sh/gpu-index"
 )
diff --git a/pkg/scheduler/cache/cache.go b/pkg/scheduler/cache/cache.go
index 5ff0e1f..50b5551 100644
--- a/pkg/scheduler/cache/cache.go
+++ b/pkg/scheduler/cache/cache.go
@@ -27,6 +27,7 @@ import (
 	apierrors "k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/apimachinery/pkg/types"
 	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
 	"k8s.io/apimachinery/pkg/util/wait"
 	"k8s.io/client-go/informers"
@@ -83,6 +84,7 @@ type SchedulerCache struct {
 	nodeInformer               infov1.NodeInformer
 	podGroupInformerV1beta1    vcinformerv1.PodGroupInformer
 	queueInformerV1beta1       vcinformerv1.QueueInformer
+	elasticQuotaInformerV1beta1 vcinformerv1.ElasticResourceQuotaInformer
 	pvInformer                 infov1.PersistentVolumeInformer
 	pvcInformer                infov1.PersistentVolumeClaimInformer
 	scInformer                 storagev1.StorageClassInformer
@@ -101,6 +103,8 @@ type SchedulerCache struct {
 	Jobs                 map[schedulingapi.JobID]*schedulingapi.JobInfo
 	Nodes                map[string]*schedulingapi.NodeInfo
 	Queues               map[schedulingapi.QueueID]*schedulingapi.QueueInfo
+	ElasticResourceQuotas map[schedulingapi.ElasticResourceQuotaID]*schedulingapi.ElasticResourceQuotaInfo
+
 	PriorityClasses      map[string]*v1beta1.PriorityClass
 	defaultPriorityClass *v1beta1.PriorityClass
 	defaultPriority      int32
@@ -300,6 +304,8 @@ func newSchedulerCache(config *rest.Config, schedulerName string, defaultQueue s
 		Jobs:            make(map[schedulingapi.JobID]*schedulingapi.JobInfo),
 		Nodes:           make(map[string]*schedulingapi.NodeInfo),
 		Queues:          make(map[schedulingapi.QueueID]*schedulingapi.QueueInfo),
+		ElasticResourceQuotas: make(map[schedulingapi.ElasticResourceQuotaID]*schedulingapi.ElasticResourceQuotaInfo),
+
 		PriorityClasses: make(map[string]*v1beta1.PriorityClass),
 		errTasks:        workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),
 		deletedJobs:     workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),
@@ -422,6 +428,23 @@ func newSchedulerCache(config *rest.Config, schedulerName string, defaultQueue s
 		DeleteFunc: sc.DeleteQueueV1beta1,
 	})
 
+	// create informer(v1beta1) for ElasticResourceQuota information
+	sc.elasticQuotaInformerV1beta1 = vcinformers.Scheduling().V1beta1().ElasticResourceQuotas()
+	sc.elasticQuotaInformerV1beta1.Informer().AddEventHandler(cache.FilteringResourceEventHandler{
+		FilterFunc: func(obj interface{}) bool {
+			switch v := obj.(type) {
+			case *vcv1beta1.ElasticResourceQuota:
+				return v.Status.IsLeaf
+			default:
+				return false
+			}
+		},
+		Handler: cache.ResourceEventHandlerFuncs{
+			AddFunc:    sc.AddElasticResourceQuotaV1beta1,
+			UpdateFunc: sc.UpdateElasticResourceQuotaV1beta1,
+			DeleteFunc: sc.DeleteElasticResourceQuotaV1beta1,
+		},
+	})
 	return sc
 }
 
@@ -434,6 +457,7 @@ func (sc *SchedulerCache) Run(stopCh <-chan struct{}) {
 	go sc.pvcInformer.Informer().Run(stopCh)
 	go sc.scInformer.Informer().Run(stopCh)
 	go sc.queueInformerV1beta1.Informer().Run(stopCh)
+	go sc.elasticQuotaInformerV1beta1.Informer().Run(stopCh)
 	go sc.quotaInformer.Informer().Run(stopCh)
 
 	if options.ServerOpts.EnablePriorityClass {
@@ -459,6 +483,7 @@ func (sc *SchedulerCache) WaitForCacheSync(stopCh <-chan struct{}) bool {
 				sc.pvcInformer.Informer().HasSynced,
 				sc.scInformer.Informer().HasSynced,
 				sc.queueInformerV1beta1.Informer().HasSynced,
+				sc.elasticQuotaInformerV1beta1.Informer().HasSynced,
 				sc.quotaInformer.Informer().HasSynced,
 			}
 			if options.ServerOpts.EnablePriorityClass {
@@ -486,6 +511,33 @@ func (sc *SchedulerCache) findJobAndTask(taskInfo *schedulingapi.TaskInfo) (*sch
 	return job, task, nil
 }
 
+// EvictJob
+func (sc *SchedulerCache) EvictJob(jobInfo *schedulingapi.JobInfo, msg string) error {
+	sc.Mutex.Lock()
+	defer sc.Mutex.Unlock()
+	job, found := sc.Jobs[jobInfo.UID]
+	if !found {
+		return fmt.Errorf("failed to find Job %v", jobInfo.UID)
+	}
+	podgroup := &vcv1beta1.PodGroup{}
+	if err := schedulingscheme.Scheme.Convert(&job.PodGroup.PodGroup, podgroup, nil); err != nil {
+		klog.Errorf("Error while converting PodGroup to v1alpha1.PodGroup with error: %v", err)
+		return err
+	}
+	klog.Infof("patch pod group <%+v> with message %s", podgroup, msg)
+	// patch podgroup preempted
+	go func() {
+		_, err := sc.vcClient.SchedulingV1beta1().PodGroups(podgroup.Namespace).Patch(context.TODO(),
+			podgroup.Name, types.MergePatchType, []byte(fmt.Sprintf(`{"metadata":{"annotations":{"Preempted":"%s"}}}`, msg)), metav1.PatchOptions{})
+		if err != nil {
+			klog.Errorf("Patch PodGroup with preempted annotation error: %v", err)
+		}
+	}()
+
+	sc.Recorder.Eventf(podgroup, v1.EventTypeWarning, "Evict", msg)
+	return nil
+}
+
 // Evict will evict the pod.
 //
 // If error occurs both task and job are guaranteed to be in the original state.
diff --git a/pkg/scheduler/cache/event_handlers.go b/pkg/scheduler/cache/event_handlers.go
index adf7fac..fc332cf 100644
--- a/pkg/scheduler/cache/event_handlers.go
+++ b/pkg/scheduler/cache/event_handlers.go
@@ -555,7 +555,20 @@ func (sc *SchedulerCache) DeleteQueueV1beta1(obj interface{}) {
 }
 
 func (sc *SchedulerCache) addQueue(queue *scheduling.Queue) {
-	qi := schedulingapi.NewQueueInfo(queue)
+	newElasticQuota := &scheduling.ElasticResourceQuota{}
+	elasticQuotaName, ok := queue.Labels[schedulingv1.QueueBindingElasticQuotaKey]
+	if ok && len(elasticQuotaName) > 0 {
+		eq, err := sc.vcClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), elasticQuotaName, metav1.GetOptions{})
+		if err != nil {
+			klog.Errorf("get elastic resource quota[%s] for queue[%s] failed, err: %v", elasticQuotaName, queue.Name, err)
+			return
+		}
+		if err := scheme.Scheme.Convert(eq, newElasticQuota, nil); err != nil {
+			klog.Errorf("Failed to convert elastic resource quota from %T to %T", eq, newElasticQuota)
+			return
+		}
+	}
+	qi := schedulingapi.NewQueueInfo(queue, newElasticQuota)
 	sc.Queues[qi.UID] = qi
 }
 
@@ -567,6 +580,105 @@ func (sc *SchedulerCache) deleteQueue(id schedulingapi.QueueID) {
 	delete(sc.Queues, id)
 }
 
+// AddElasticResourceQuotaV1beta1 add elastic resource quota to scheduler cache
+func (sc *SchedulerCache) AddElasticResourceQuotaV1beta1(obj interface{}) {
+	ss, ok := obj.(*schedulingv1.ElasticResourceQuota)
+	if !ok {
+		klog.Errorf("Cannot convert to *schedulingv1.ElasticResourceQuota: %v", obj)
+		return
+	}
+
+	elasticQuota := &scheduling.ElasticResourceQuota{}
+	if err := scheme.Scheme.Convert(ss, elasticQuota, nil); err != nil {
+		klog.Errorf("Failed to convert elastic resource quota from %T to %T", ss, elasticQuota)
+		return
+	}
+
+	sc.Mutex.Lock()
+	defer sc.Mutex.Unlock()
+
+	klog.V(4).Infof("Add elastic resource quota (%s) into cache, spec(%#v), status(%#v)", ss.Name, elasticQuota.Spec, elasticQuota.Status)
+	sc.addElasticResourceQuota(elasticQuota)
+}
+
+// UpdateElasticResourceQuotaV1beta1 update elastic resource quota to scheduler cache
+func (sc *SchedulerCache) UpdateElasticResourceQuotaV1beta1(oldObj, newObj interface{}) {
+	oldSS, ok := oldObj.(*schedulingv1.ElasticResourceQuota)
+	if !ok {
+		klog.Errorf("Cannot convert oldObj to *schedulingv1.ElasticResourceQuota: %v", oldObj)
+		return
+	}
+	newSS, ok := newObj.(*schedulingv1.ElasticResourceQuota)
+	if !ok {
+		klog.Errorf("Cannot convert newObj to *schedulingv1.ElasticResourceQuota: %v", newObj)
+		return
+	}
+
+	if oldSS.ResourceVersion == newSS.ResourceVersion {
+		return
+	}
+
+	newElasticQuota := &scheduling.ElasticResourceQuota{}
+	if err := scheme.Scheme.Convert(newSS, newElasticQuota, nil); err != nil {
+		klog.Errorf("Failed to convert elastic resource quota from %T to %T", newSS, newElasticQuota)
+		return
+	}
+
+	sc.Mutex.Lock()
+	defer sc.Mutex.Unlock()
+	sc.updateElasticResourceQuota(newElasticQuota)
+}
+
+// DeleteQueueV1beta1 delete queue from the scheduler cache
+func (sc *SchedulerCache) DeleteElasticResourceQuotaV1beta1(obj interface{}) {
+	var ss *schedulingv1.ElasticResourceQuota
+	switch t := obj.(type) {
+	case *schedulingv1.ElasticResourceQuota:
+		ss = t
+	case cache.DeletedFinalStateUnknown:
+		var ok bool
+		ss, ok = t.Obj.(*schedulingv1.ElasticResourceQuota)
+		if !ok {
+			klog.Errorf("Cannot convert to *schedulingv1.ElasticResourceQuota: %v", t.Obj)
+			return
+		}
+	default:
+		klog.Errorf("Cannot convert to *schedulingv1.ElasticResourceQuota: %v", t)
+		return
+	}
+
+	sc.Mutex.Lock()
+	defer sc.Mutex.Unlock()
+	sc.deleteElasticResourceQuota(schedulingapi.ElasticResourceQuotaID(ss.Name))
+}
+
+func (sc *SchedulerCache) addElasticResourceQuota(elasticQuota *scheduling.ElasticResourceQuota) {
+	if len(elasticQuota.Status.QueueName) > 0 {
+		queue, err := sc.vcClient.SchedulingV1beta1().Queues().Get(context.TODO(), elasticQuota.Status.QueueName, metav1.GetOptions{})
+		if err != nil {
+			klog.Errorf("get elastic resource quota[%s] bind queue[%s] failed, err: %v", elasticQuota.Name, elasticQuota.Status.QueueName, err)
+			return
+		}
+		newQueue := &scheduling.Queue{}
+		if err := scheme.Scheme.Convert(queue, newQueue, nil); err != nil {
+			klog.Errorf("Failed to convert queue from %T to %T", queue, newQueue)
+			return
+		}
+		qi := schedulingapi.NewQueueInfo(newQueue, elasticQuota)
+		sc.Queues[qi.UID] = qi
+	}
+	eqi := schedulingapi.NewElasticResourceQuotaInfo(elasticQuota)
+	sc.ElasticResourceQuotas[eqi.UID] = eqi
+}
+
+func (sc *SchedulerCache) updateElasticResourceQuota(elasticQuota *scheduling.ElasticResourceQuota) {
+	sc.addElasticResourceQuota(elasticQuota)
+}
+
+func (sc *SchedulerCache) deleteElasticResourceQuota(id schedulingapi.ElasticResourceQuotaID) {
+	delete(sc.ElasticResourceQuotas, id)
+}
+
 //DeletePriorityClass delete priorityclass from the scheduler cache
 func (sc *SchedulerCache) DeletePriorityClass(obj interface{}) {
 	var ss *v1beta1.PriorityClass
diff --git a/pkg/scheduler/cache/interface.go b/pkg/scheduler/cache/interface.go
index 093cff2..1d174e9 100644
--- a/pkg/scheduler/cache/interface.go
+++ b/pkg/scheduler/cache/interface.go
@@ -40,6 +40,9 @@ type Cache interface {
 	// TODO(jinzhej): clean up expire Tasks.
 	Bind(task *api.TaskInfo, hostname string) error
 
+	// EvictJob evict the job to release resources.
+	EvictJob(job *api.JobInfo, reason string) error
+
 	// Evict evicts the task to release resources.
 	Evict(task *api.TaskInfo, reason string) error
 
diff --git a/pkg/scheduler/framework/statement.go b/pkg/scheduler/framework/statement.go
index 3956fdf..fe821eb 100644
--- a/pkg/scheduler/framework/statement.go
+++ b/pkg/scheduler/framework/statement.go
@@ -32,8 +32,16 @@ const (
 	Evict = iota
 	// Pipeline op
 	Pipeline
+	// UnPipeline op
+	UnPipeline
 	// Allocate op
 	Allocate
+	// UnAllocate op
+	UnAllocate
+	// EvictJob op
+	EvictJob
+	// EvictResource op
+	EvictResource
 )
 
 type operation struct {
@@ -55,6 +63,260 @@ func NewStatement(ssn *Session) *Statement {
 	}
 }
 
+// EvictJob evict job
+func (s *Statement) EvictJob(reclaimees []*api.TaskInfo, reason string) []string {
+	candidateNodes := make([]string, 0)
+	for _, task := range reclaimees {
+		switch {
+		case task.Status == api.Allocated:
+			if err := s.UnAllocate(task); err != nil {
+				klog.Errorf("Failed to unallocate Task <%s/%s>: %v",
+					task.Namespace, task.Name, err)
+			}
+
+		case task.Status == api.Pipelined:
+			if err := s.UnPipeline(task); err != nil {
+				klog.Errorf("Failed to unpipeline Task <%s/%s>: %v",
+					task.Namespace, task.Name, err)
+			}
+		case api.AllocatedStatus(task.Status):
+			if err := s.EvictResource(task, "preempt"); err != nil {
+				klog.Errorf("Failed to evictResource Task <%s/%s>: %v",
+					task.Namespace, task.Name, err)
+			}
+		}
+		candidateNodes = append(candidateNodes, task.NodeName)
+		s.operations = append(s.operations, operation{
+			name:   Evict,
+			task:   task,
+			reason: reason,
+		})
+	}
+	s.operations = append(s.operations, operation{
+		name:   EvictJob,
+		task:   reclaimees[0],
+		reason: reason,
+	})
+	return candidateNodes
+}
+
+// evictJob evict job
+func (s *Statement) evictJob(reclaimee *api.JobInfo, reason string) error {
+	if err := s.ssn.cache.EvictJob(reclaimee, reason); err != nil {
+		klog.Errorf("Faled to evict job <%v/%v>: %v.",
+			reclaimee.Namespace, reclaimee.Name, err)
+		return err
+	}
+	return nil
+}
+
+func (s *Statement) unevictJob(reclaimee *api.JobInfo) {
+}
+
+// Allocate the task to node
+func (s *Statement) UnAllocate(task *api.TaskInfo) error {
+	// Update status in session
+	job, found := s.ssn.Jobs[task.Job]
+	if found {
+		if err := job.UpdateTaskStatus(task, api.Pending); err != nil {
+			klog.Errorf("Failed to update task <%v/%v> status to %v in Session <%v>: %v",
+				task.Namespace, task.Name, api.Pending, s.ssn.UID, err)
+		}
+	} else {
+		klog.Errorf("Failed to find Job <%s> in Session <%s> index when unallocating.",
+			task.Job, s.ssn.UID)
+	}
+
+	if node, found := s.ssn.Nodes[task.NodeName]; found {
+		klog.V(3).Infof("Remove Task <%v> on node <%v>", task.Name, task.NodeName)
+		err := node.RemoveTask(task)
+		if err != nil {
+			klog.Errorf("Failed to remove Task <%v> on node <%v>: %s", task.Name, task.NodeName, err.Error())
+		}
+	}
+
+	for _, eh := range s.ssn.eventHandlers {
+		if eh.DeallocateFunc != nil {
+			eh.DeallocateFunc(&Event{
+				Task: task,
+			})
+		}
+	}
+	s.operations = append(s.operations, operation{
+		name:   UnAllocate,
+		task:   task,
+		reason: task.NodeName,
+	})
+	// task.NodeName = ""
+
+	return nil
+}
+
+func (s *Statement) RollBackAllocate(task *api.TaskInfo) error {
+	// Only update status in session
+	job, found := s.ssn.Jobs[task.Job]
+	if found {
+		if err := job.UpdateTaskStatus(task, api.Allocated); err != nil {
+			klog.Errorf("Failed to update task <%v/%v> status to %v in Session <%v>: %v",
+				task.Namespace, task.Name, api.Allocated, s.ssn.UID, err)
+			return err
+		}
+	} else {
+		klog.Errorf("Failed to found Job <%s> in Session <%s> index when binding.",
+			task.Job, s.ssn.UID)
+		return fmt.Errorf("failed to find job %s", task.Job)
+	}
+
+	// task.NodeName = hostname
+	hostname := task.NodeName
+	// Callbacks
+	for _, eh := range s.ssn.eventHandlers {
+		if eh.AllocateFunc != nil {
+			eh.AllocateFunc(&Event{
+				Task: task,
+			})
+		}
+	}
+	if node, found := s.ssn.Nodes[task.NodeName]; found {
+		if err := node.AddTask(task); err != nil {
+			klog.Errorf("Failed to add task <%v/%v> to node <%v> in Session <%v>: %v",
+				task.Namespace, task.Name, hostname, s.ssn.UID, err)
+			return err
+		}
+		klog.V(3).Infof("After allocated Task <%v/%v> to Node <%v>: idle <%v>, used <%v>, releasing <%v>",
+			task.Namespace, task.Name, node.Name, node.Idle, node.Used, node.Releasing)
+	} else {
+		klog.Errorf("Failed to found Node <%s> in Session <%s> index when binding.",
+			hostname, s.ssn.UID)
+		return fmt.Errorf("failed to find node %s", hostname)
+	}
+
+	return nil
+}
+
+func (s *Statement) UnPipeline(task *api.TaskInfo) error {
+	job, found := s.ssn.Jobs[task.Job]
+	if found {
+		if err := job.UpdateTaskStatus(task, api.Pending); err != nil {
+			klog.Errorf("Failed to update task <%v/%v> status to %v in Session <%v>: %v",
+				task.Namespace, task.Name, api.Pipelined, s.ssn.UID, err)
+		}
+	} else {
+		klog.Errorf("Failed to found Job <%s> in Session <%s> index when binding.",
+			task.Job, s.ssn.UID)
+	}
+
+	hostname := task.NodeName
+	// task.NodeName = ""
+
+	if node, found := s.ssn.Nodes[hostname]; found {
+		if err := node.RemoveTask(task); err != nil {
+			klog.Errorf("Failed to unpipeline task <%v/%v> to node <%v> in Session <%v>: %v",
+				task.Namespace, task.Name, hostname, s.ssn.UID, err)
+		}
+		klog.V(3).Infof("After unpipelined Task <%v/%v> to Node <%v>: idle <%v>, used <%v>, releasing <%v>",
+			task.Namespace, task.Name, node.Name, node.Idle, node.Used, node.Releasing)
+	} else {
+		klog.Errorf("Failed to found Node <%s> in Session <%s> index when binding.",
+			hostname, s.ssn.UID)
+	}
+
+	for _, eh := range s.ssn.eventHandlers {
+		if eh.DeallocateFunc != nil {
+			eh.DeallocateFunc(&Event{
+				Task: task,
+			})
+		}
+	}
+	s.operations = append(s.operations, operation{
+		name:   UnPipeline,
+		task:   task,
+		reason: hostname,
+	})
+
+	return nil
+}
+
+func (s *Statement) RollBackPipeline(task *api.TaskInfo) error {
+	job, found := s.ssn.Jobs[task.Job]
+	if found {
+		if err := job.UpdateTaskStatus(task, api.Pipelined); err != nil {
+			klog.Errorf("Failed to update task <%v/%v> status to %v in Session <%v>: %v",
+				task.Namespace, task.Name, api.Pipelined, s.ssn.UID, err)
+		}
+	} else {
+		klog.Errorf("Failed to found Job <%s> in Session <%s> index when binding.",
+			task.Job, s.ssn.UID)
+	}
+
+	// task.NodeName = hostname
+	hostname := task.NodeName
+	for _, eh := range s.ssn.eventHandlers {
+		if eh.AllocateFunc != nil {
+			eh.AllocateFunc(&Event{
+				Task: task,
+			})
+		}
+	}
+
+	if node, found := s.ssn.Nodes[hostname]; found {
+		if err := node.AddTask(task); err != nil {
+			klog.Errorf("Failed to pipeline task <%v/%v> to node <%v> in Session <%v>: %v",
+				task.Namespace, task.Name, hostname, s.ssn.UID, err)
+		}
+		klog.V(3).Infof("After pipelined Task <%v/%v> to Node <%v>: idle <%v>, used <%v>, releasing <%v>",
+			task.Namespace, task.Name, node.Name, node.Idle, node.Used, node.Releasing)
+	} else {
+		klog.Errorf("Failed to found Node <%s> in Session <%s> index when binding.",
+			hostname, s.ssn.UID)
+	}
+
+	return nil
+}
+
+// EvictResource
+func (s *Statement) EvictResource(reclaimee *api.TaskInfo, reason string) error {
+	// Update status in session
+	if job, found := s.ssn.Jobs[reclaimee.Job]; found {
+		if err := job.UpdateTaskStatus(reclaimee, api.Releasing); err != nil {
+			klog.Errorf("Failed to update task <%v/%v> status to %v in Session <%v>: %v",
+				reclaimee.Namespace, reclaimee.Name, api.Releasing, s.ssn.UID, err)
+		}
+	} else {
+		klog.Errorf("Failed to found Job <%s> in Session <%s> index when binding.",
+			reclaimee.Job, s.ssn.UID)
+	}
+
+	// Update task in node.
+	if node, found := s.ssn.Nodes[reclaimee.NodeName]; found {
+		err := node.UpdateTask(reclaimee)
+		if err != nil {
+			klog.Errorf("Failed to update task <%v/%v> in node %v for: %s",
+				reclaimee.Namespace, reclaimee.Name, reclaimee.NodeName, err.Error())
+			return err
+		}
+	}
+
+	for _, eh := range s.ssn.eventHandlers {
+		if eh.DeallocateFunc != nil {
+			eh.DeallocateFunc(&Event{
+				Task: reclaimee,
+			})
+		}
+	}
+
+	s.operations = append(s.operations, operation{
+		name:   EvictResource,
+		task:   reclaimee,
+		reason: reason,
+	})
+
+	return nil
+}
+
+func (s *Statement) evictResource(task *api.TaskInfo) {
+}
+
 // Evict the pod
 func (s *Statement) Evict(reclaimee *api.TaskInfo, reason string) error {
 	// Update status in session
@@ -354,7 +616,7 @@ func (s *Statement) Discard() {
 	for i := len(s.operations) - 1; i >= 0; i-- {
 		op := s.operations[i]
 		switch op.name {
-		case Evict:
+		case EvictResource, Evict:
 			err := s.unevict(op.task)
 			if err != nil {
 				klog.Errorf("Failed to unevict task: %s", err.Error())
@@ -364,11 +626,21 @@ func (s *Statement) Discard() {
 			if err != nil {
 				klog.Errorf("Failed to unpipeline task: %s", err.Error())
 			}
+		case UnPipeline:
+			err := s.RollBackPipeline(op.task)
+			if err != nil {
+				klog.Errorf("Failed to pipeline task: %s", err.Error())
+			}
 		case Allocate:
 			err := s.unallocate(op.task)
 			if err != nil {
 				klog.Errorf("Failed to unallocate task: %s", err.Error())
 			}
+		case UnAllocate:
+			err := s.RollBackAllocate(op.task)
+			if err != nil {
+				klog.Errorf("Failed to allocate task: %s", err.Error())
+			}
 		}
 	}
 }
@@ -378,6 +650,14 @@ func (s *Statement) Commit() {
 	klog.V(3).Info("Committing operations ...")
 	for _, op := range s.operations {
 		switch op.name {
+		case EvictJob:
+			klog.V(3).Info("Begin Evict job <%+v> ", op.task.Job)
+			if job, found := s.ssn.Jobs[op.task.Job]; found {
+				err := s.evictJob(job, op.reason)
+				if err != nil {
+					klog.Errorf("Failed to evict job: %s", err.Error())
+				}
+			}
 		case Evict:
 			err := s.evict(op.task, op.reason)
 			if err != nil {
diff --git a/pkg/scheduler/plugins/factory.go b/pkg/scheduler/plugins/factory.go
index c7aa808..646f3c5 100644
--- a/pkg/scheduler/plugins/factory.go
+++ b/pkg/scheduler/plugins/factory.go
@@ -22,11 +22,13 @@ import (
 	"volcano.sh/volcano/pkg/scheduler/plugins/conformance"
 	"volcano.sh/volcano/pkg/scheduler/plugins/drf"
 	"volcano.sh/volcano/pkg/scheduler/plugins/gang"
+	"volcano.sh/volcano/pkg/scheduler/plugins/kmpredicates"
 	"volcano.sh/volcano/pkg/scheduler/plugins/nodeorder"
 	"volcano.sh/volcano/pkg/scheduler/plugins/overcommit"
 	"volcano.sh/volcano/pkg/scheduler/plugins/predicates"
 	"volcano.sh/volcano/pkg/scheduler/plugins/priority"
 	"volcano.sh/volcano/pkg/scheduler/plugins/proportion"
+	"volcano.sh/volcano/pkg/scheduler/plugins/proportion-pf"
 	"volcano.sh/volcano/pkg/scheduler/plugins/reservation"
 	"volcano.sh/volcano/pkg/scheduler/plugins/sla"
 	"volcano.sh/volcano/pkg/scheduler/plugins/task-topology"
@@ -37,6 +39,7 @@ func init() {
 	// Plugins for Jobs
 	framework.RegisterPluginBuilder(drf.PluginName, drf.New)
 	framework.RegisterPluginBuilder(gang.PluginName, gang.New)
+	framework.RegisterPluginBuilder(kmpredicates.PluginName, kmpredicates.New)
 	framework.RegisterPluginBuilder(predicates.PluginName, predicates.New)
 	framework.RegisterPluginBuilder(priority.PluginName, priority.New)
 	framework.RegisterPluginBuilder(nodeorder.PluginName, nodeorder.New)
@@ -50,4 +53,5 @@ func init() {
 
 	// Plugins for Queues
 	framework.RegisterPluginBuilder(proportion.PluginName, proportion.New)
+	framework.RegisterPluginBuilder(proportionpf.PluginName, proportionpf.New)
 }
diff --git a/pkg/scheduler/plugins/kmpredicates/gpu.go b/pkg/scheduler/plugins/kmpredicates/gpu.go
new file mode 100644
index 0000000..968e179
--- /dev/null
+++ b/pkg/scheduler/plugins/kmpredicates/gpu.go
@@ -0,0 +1,339 @@
+/*
+Copyright 2020 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package kmpredicates
+
+import (
+	"encoding/json"
+	"fmt"
+	"math/rand"
+	"sort"
+	"strings"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/klog"
+
+	"volcano.sh/volcano/pkg/scheduler/api"
+)
+
+// checkNodeGPUPredicate checks if a gpu pod can be scheduled on a node.
+func checkNodeGPUPredicate(task *api.TaskInfo, nodeInfo *api.NodeInfo) (bool, error) {
+	klog.V(4).Infof("checkNodeGPUPredicate: %+v", nodeInfo)
+	// node
+	available := isNodeAvailable(task, nodeInfo)
+	if !available {
+		return false, fmt.Errorf("scalar resources on pod %s does not available on node %s", task.Pod.Name, nodeInfo.Name)
+	}
+	allocatable := assume(task.Pod, nodeInfo)
+	if !allocatable {
+		return false, fmt.Errorf("no enough gpu resource on devices of node %s", nodeInfo.Name)
+	}
+	return true, nil
+}
+
+func isNodeAvailable(task *api.TaskInfo, nodeInfo *api.NodeInfo) bool {
+	reqRes := getGPUResource(task.Resreq.ScalarResources)
+	nodeRes := getGPUResource(nodeInfo.Allocatable.ScalarResources)
+	klog.V(3).Infof("isNodeAvailable, req resource: [%+v], node resource:[%+v]", reqRes, nodeRes)
+	for resType := range reqRes {
+		resValue, exist := nodeRes[resType]
+		if !exist {
+			return false
+		}
+		if resValue == 0 {
+			return false
+		}
+	}
+	return true
+}
+
+func getGPUResource(scalarResources map[v1.ResourceName]float64) map[v1.ResourceName]float64 {
+	gpuRes := make(map[v1.ResourceName]float64)
+	for resType := range scalarResources {
+		if strings.Contains(string(resType), api.CGPU) {
+			gpuRes[resType] = scalarResources[resType]
+		}
+	}
+	return gpuRes
+}
+
+func assume(pod *v1.Pod, node *api.NodeInfo) bool {
+	reqGPUCount := api.GetGPUCountOfPod(pod)
+	reqGPUMem := api.GetGPUMemoryOfPod(pod) / reqGPUCount
+	reqGPUCore := api.GetGPUCoreOfPod(pod) / reqGPUCount
+	klog.V(4).Infof("predicates with gpu, pod %s/%s request GPUCount :%v, GPUMem: %v, GPUCore: %v", pod.Namespace, pod.Name, reqGPUCount, reqGPUMem, reqGPUCore)
+	if reqGPUMem == 0 && reqGPUCore == 0 {
+		// 
+		availableGPUs := node.GetDevicesIdleGPU()
+		klog.V(4).Infof("Assume alone job, availableGPUs %+v on node: %+v", availableGPUs, node.Name)
+		if len(availableGPUs) < int(reqGPUCount) {
+			return false
+		}
+	} else {
+		allocatableGPUIds := getAllocatableGPUIds(node, reqGPUMem, reqGPUCore)
+		klog.V(4).Infof("Assume mix job, availableGPUs %+v on node: %+v", allocatableGPUIds, node.Name)
+		if len(allocatableGPUIds) < int(reqGPUCount) {
+			return false
+		}
+	}
+	return true
+}
+
+func getAllocatableGPUIds(node *api.NodeInfo, reqGPUMem uint, reqGPUCore uint) map[int]bool {
+	// filter cgpu memory dev
+	availableGPUsMem := node.GetDevicesIdleGPUMemory()
+	klog.V(4).Infof("Assume mix job, availableGPUsMem %+v on node: %+v", availableGPUsMem, node.Name)
+	availableGPUIds := assumeGPUMemOrCore(availableGPUsMem, reqGPUMem)
+	klog.V(4).Infof("After assume cgpu memory, availableGPUIds %+v on node: %+v", availableGPUIds, node.Name)
+
+	availableGPUsCore := node.GetDevicesIdleGPUCore()
+	klog.V(4).Infof("Assume mix job, availableGPUsCore %+v on node: %+v", availableGPUsCore, node.Name)
+
+	if reqGPUCore == 0 {
+		for id := range availableGPUIds {
+			if coreVal, found := availableGPUsCore[id]; found && coreVal == 100 {
+				continue
+			}
+			delete(availableGPUIds, id)
+		}
+		klog.V(4).Infof("Assume mix job, allocatable core share gpus %+v on node: %+v", availableGPUIds, node.Name)
+	} else {
+		availableGPUCoreIds := assumeGPUMemOrCore(availableGPUsCore, reqGPUCore)
+		klog.V(4).Infof("After assume cgpu core, availableGPUIds %+v on node: %+v", availableGPUCoreIds, node.Name)
+
+		coreIsolationDevs := node.GetCoreIsolationDevices()
+		klog.V(4).Infof("Assume mix job, coreIsolationDevs %+v on node: %+v", coreIsolationDevs, node.Name)
+		emptyDevs := node.GetEmptyDevices()
+		klog.V(4).Infof("Assume mix job, emptyDevs %+v on node: %+v", emptyDevs, node.Name)
+		for id := range availableGPUCoreIds {
+			if isolation, found := coreIsolationDevs[id]; found && isolation {
+				continue
+			}
+			if empty, found := emptyDevs[id]; found && empty {
+				continue
+			}
+			delete(availableGPUCoreIds, id)
+		}
+		for id := range availableGPUIds {
+			if available, found := availableGPUCoreIds[id]; found && available {
+				continue
+			} else {
+				delete(availableGPUIds, id)
+			}
+		}
+		klog.V(4).Infof("Assume mix job, allocatable core isolation gpus %+v on node: %+v", availableGPUIds, node.Name)
+	}
+	return availableGPUIds
+}
+
+func assumeGPUMemOrCore(availableGPUs map[int]uint, reqGPURes uint) map[int]bool {
+	availableGPUIds := make(map[int]bool)
+	for id, res := range availableGPUs {
+		if res >= reqGPURes {
+			availableGPUIds[id] = true
+		}
+	}
+	return availableGPUIds
+}
+
+// allocateGPUs allocate the GPU IDs to the pod
+func allocateGPUs(pod *v1.Pod, node *api.NodeInfo, gpuTopoEnable bool) []int {
+	reqGPUCount := api.GetGPUCountOfPod(pod)
+	reqGPUMem := api.GetGPUMemoryOfPod(pod) / reqGPUCount
+	reqGPUCore := api.GetGPUCoreOfPod(pod) / reqGPUCount
+	candidateDevIDs := make([]int, 0)
+	binPackCandidateDevIDs := make([][]int, 0)
+
+	klog.V(4).Infof("allocate device ids, pod %s/%s request GPUCount :%v, GPUMem: %v, GPUCore: %v", pod.Namespace, pod.Name, reqGPUCount, reqGPUMem, reqGPUCore)
+	if reqGPUMem == 0 && reqGPUCore == 0 {
+		// 
+		availableGPUs := node.GetDevicesIdleGPU()
+		klog.V(4).Infof("Alone job, availableGPUs: %+v on node: %s.", availableGPUs, node.Name)
+		binPackCandidateDevIDs = combine(availableGPUs, reqGPUCount)
+		klog.V(4).Infof("Alone job, after dfs combine: %+v, on node: %s.", binPackCandidateDevIDs, node.Name)
+	} else {
+		availableGPUsMem := node.GetDevicesIdleGPUMemory()
+		allocatableGPUIds := getAllocatableGPUIds(node, reqGPUMem, reqGPUCore)
+		for id := range availableGPUsMem {
+			if allocatable, found := allocatableGPUIds[id]; found && allocatable {
+				continue
+			} else {
+				delete(availableGPUsMem, id)
+			}
+		}
+		if len(availableGPUsMem) > 0 {
+			binPackCandidateDevIDs = binPackAllocate(availableGPUsMem, reqGPUCount)
+			if len(binPackCandidateDevIDs) == 0 {
+				klog.Errorf("The node %s can't place the pod %s [reqGPUCount: %v, reqGPUMem: %v] in ns %s with gpu binpack strategy.",
+					node.Name,
+					pod.Name,
+					reqGPUCount,
+					reqGPUMem,
+					pod.Namespace)
+				return candidateDevIDs
+			}
+		}
+	}
+	candidateDevIDs = binPackCandidateDevIDs[rand.Intn(len(binPackCandidateDevIDs))]
+	if reqGPUCount > 1 && gpuTopoEnable {
+		gpuTopoAllocateDevIDs, err := gpuTopoAllocate(binPackCandidateDevIDs, node)
+		if err == nil {
+			candidateDevIDs = gpuTopoAllocateDevIDs
+		}
+	}
+	klog.V(4).Infof("Allocate GPUs: %+v on node %+v", candidateDevIDs, node.Name)
+
+	return candidateDevIDs
+}
+
+func combine(availableGPUs []int, reqGPUCount uint) (ans [][]int) {
+	temp := []int{}
+	var dfs func(int)
+	dfs = func(cur int) {
+		// temp  [cur, len(availableGPUs)]  reqGPUCount reqGPUCount  temp
+		if len(temp)+(len(availableGPUs)-cur) < int(reqGPUCount) {
+			return
+		}
+		// 
+		if len(temp) == int(reqGPUCount) {
+			comb := make([]int, reqGPUCount)
+			copy(comb, temp)
+			ans = append(ans, comb)
+			return
+		}
+		// 
+		temp = append(temp, availableGPUs[cur])
+		dfs(cur + 1)
+		temp = temp[:len(temp)-1]
+		// 
+		dfs(cur + 1)
+	}
+	dfs(0)
+	return
+}
+
+func binPackAllocate(availableGPUsMem map[int]uint, reqGPUCount uint) [][]int {
+	found := false
+	candidateGpuSet := make([][]int, 0)
+	devSlice := []int{}
+	for idx := range availableGPUsMem {
+		devSlice = append(devSlice, idx)
+	}
+	sort.Slice(devSlice, func(x, y int) bool {
+		gpuX := availableGPUsMem[devSlice[x]]
+		gpuY := availableGPUsMem[devSlice[y]]
+		return gpuX < gpuY
+
+	})
+	// cgpu memory
+	bestGpuSetMemSum := uint(0)
+	allocatableCount := uint(0)
+	for _, idx := range devSlice {
+		if availableGPUMem, ok := availableGPUsMem[idx]; ok {
+			allocatableCount++
+			bestGpuSetMemSum += availableGPUMem
+			if allocatableCount == reqGPUCount {
+				found = true
+				break
+			}
+		}
+	}
+	if found {
+		// binpackgpu
+		if reqGPUCount == 1 {
+			for _, idx := range devSlice {
+				availableGPUMem, ok := availableGPUsMem[idx]
+				if ok && availableGPUMem == bestGpuSetMemSum {
+					candidateGpuSet = append(candidateGpuSet, []int{idx})
+				}
+			}
+		} else {
+			used := make(map[int]bool)
+			for idx := range availableGPUsMem {
+				used[idx] = false
+			}
+			candidateGpuSet = reqGPUSum(availableGPUsMem, devSlice, used, bestGpuSetMemSum, len(availableGPUsMem)-1, int(reqGPUCount))
+		}
+	}
+	return candidateGpuSet
+}
+
+func reqGPUSum(availableGPUsMem map[int]uint, devSlice []int, used map[int]bool, remainder uint, loop int, reqGPUCount int) [][]int {
+	gpuSet := make([][]int, 0)
+	if remainder == 0 {
+		temp := make([]int, 0)
+		for idx := range used {
+			if used[idx] {
+				temp = append(temp, idx)
+			}
+		}
+		if len(temp) == reqGPUCount {
+			gpuSet = append(gpuSet, temp)
+		}
+		return gpuSet
+	}
+	for i := loop; i >= 0; i-- {
+		if !used[devSlice[i]] && (remainder-availableGPUsMem[devSlice[i]]) >= 0 {
+			used[devSlice[i]] = true
+			tempSet := reqGPUSum(availableGPUsMem, devSlice, used, remainder-availableGPUsMem[devSlice[i]], i-1, reqGPUCount)
+			if len(tempSet) != 0 {
+				gpuSet = append(gpuSet, tempSet...)
+			}
+			used[devSlice[i]] = false
+		}
+	}
+	return gpuSet
+}
+
+func gpuTopoAllocate(binPackCandidateDevIDs [][]int, node *api.NodeInfo) ([]int, error) {
+	nodeGPUDevices := []*Device{}
+	if node.Node.ObjectMeta.Annotations != nil {
+		gpuTopo := node.Node.ObjectMeta.Annotations[api.GPUTopo]
+		if err := json.Unmarshal([]byte(gpuTopo), &nodeGPUDevices); err != nil {
+			klog.Errorf("Failed to unmarshal GpuTopo, error: %v", err)
+			return nil, err
+		}
+	} else {
+		errMsg := "Failed to get GpuTopo, node.ObjectMeta.Annotations is nil."
+		klog.Errorf(errMsg)
+		return nil, fmt.Errorf(errMsg)
+	}
+	bestSet := make([]int, 0)
+	bestScore := 0
+	for _, binPackCandidate := range binPackCandidateDevIDs {
+		gpuSet := getBinPackCandidateDevs(nodeGPUDevices, binPackCandidate)
+		score := calculateGPUSetScore(gpuSet)
+		klog.V(4).Infof("calculateGPUSetScore: gpuSet: %+v, score: %+v.", gpuSet, score)
+		if score > bestScore {
+			bestSet = binPackCandidate
+			bestScore = score
+		}
+	}
+	klog.V(4).Infof("gpuTopoAllocate bestSet: %+v on node %+v", bestSet, node.Name)
+	return bestSet, nil
+}
+
+func getBinPackCandidateDevs(nodeGPUDevices []*Device, binPackCandidate []int) []*Device {
+	devs := make([]*Device, 0)
+	for _, idx := range binPackCandidate {
+		for _, nodeGPUDevice := range nodeGPUDevices {
+			if idx == nodeGPUDevice.Index {
+				devs = append(devs, nodeGPUDevice)
+			}
+		}
+	}
+	return devs
+}
diff --git a/pkg/scheduler/plugins/kmpredicates/gputopo.go b/pkg/scheduler/plugins/kmpredicates/gputopo.go
new file mode 100644
index 0000000..8cc8c9a
--- /dev/null
+++ b/pkg/scheduler/plugins/kmpredicates/gputopo.go
@@ -0,0 +1,160 @@
+package kmpredicates
+
+import (
+	"fmt"
+
+	nvml "volcano.sh/volcano/pkg/scheduler/plugins/util/nvidia"
+)
+
+// NodeGPUDevices represents GPU device list
+type NodeGPUDevices struct {
+	Devices []*Device `json:"devices"`
+}
+
+// Device represents a GPU device as reported by NVML, including all of its
+// Point-to-Point link information.
+type Device struct {
+	*nvml.Device
+	Index int
+	Links map[int][]P2PLink
+}
+
+// P2PLink represents a Point-to-Point link between two GPU devices. The link
+// is between the Device struct this struct is embedded in and the GPU Device
+// contained in the P2PLink struct itself.
+type P2PLink struct {
+	Index int
+	// GPU  *Device
+	Type nvml.P2PLinkType
+}
+
+// Get the total score of a set of GPUs. The score is calculated as the sum of
+// the scores calculated for each pair of GPUs in the set.
+func calculateGPUSetScore(gpuSet []*Device) int {
+	score := 0
+
+	iterateGPUSets(gpuSet, 2, func(gpus []*Device) {
+		score += calculateGPUPairScore(gpus[0], gpus[1])
+	})
+
+	return score
+}
+
+// Iterate through all GPU sets of size 'size', applying a callback function to them.
+// This function is implemented using an iterative solution for efficiency.
+func iterateGPUSets(devices []*Device, size int, callback func([]*Device)) {
+	if size <= 0 {
+		return
+	}
+
+	if size > len(devices) {
+		return
+	}
+
+	// The logic below is a simple unrolling of the recursive loops:
+	//
+	// n := len(devices)
+	// for i := 0; i < n; i++
+	//     for j := i+1; j < n; j++
+	//         for k := j+1; k < n; k++
+	//             ...
+	//             for z := y+1; z < n; z++
+	//                 callback({devices[i], devices[j], devices[k], ..., devices[z]})
+	//
+	// Where 'size' represents how many logical 'for' loops there are, 'level'
+	// represents how many 'for' loops deep we are, 'indices' holds the loop
+	// index at each level, and 'set' builds out the list of devices to pass to
+	// the callback each time the bottom most level is reached.
+	level := 0
+	indices := make([]int, size)
+	set := make([]*Device, size)
+
+	for {
+		if indices[level] == len(devices) {
+			if level == 0 {
+				break
+			}
+
+			level--
+			indices[level]++
+			continue
+		}
+
+		set[level] = devices[indices[level]]
+
+		if level < (size - 1) {
+			level++
+			indices[level] = indices[level-1] + 1
+			continue
+		}
+
+		callback(set)
+		indices[level]++
+	}
+}
+
+// Calculate a "link" score for a pair of GPUs.
+// The score is based on the "closeness" of the two GPUs in relation to one
+// another in terms of the communication links they have with another, as well
+// as the PCIe hierarchy they are in. GPUs connected by an NVLINK receive 100
+// points for each link connecting them. GPUs in the PCIe hierarchy receive
+// points relative to how close they are to one another.
+func calculateGPUPairScore(gpu0 *Device, gpu1 *Device) int {
+	if gpu0 == nil || gpu1 == nil {
+		return 0
+	}
+
+	if gpu0 == gpu1 {
+		return 0
+	}
+
+	if len(gpu0.Links[gpu1.Index]) != len(gpu1.Links[gpu0.Index]) {
+		err := fmt.Errorf("Internal error in bestEffort GPU allocator: all P2PLinks between 2 GPUs should be bidirectional")
+		panic(err)
+	}
+
+	score := 0
+
+	for _, link := range gpu0.Links[gpu1.Index] {
+		switch link.Type {
+		case nvml.P2PLinkCrossCPU:
+			score += 10
+		case nvml.P2PLinkSameCPU:
+			score += 20
+		case nvml.P2PLinkHostBridge:
+			score += 30
+		case nvml.P2PLinkMultiSwitch:
+			score += 40
+		case nvml.P2PLinkSingleSwitch:
+			score += 50
+		case nvml.P2PLinkSameBoard:
+			score += 60
+		case nvml.SingleNVLINKLink:
+			score += 100
+		case nvml.TwoNVLINKLinks:
+			score += 200
+		case nvml.ThreeNVLINKLinks:
+			score += 300
+		case nvml.FourNVLINKLinks:
+			score += 400
+		case nvml.FiveNVLINKLinks:
+			score += 500
+		case nvml.SixNVLINKLinks:
+			score += 600
+		case nvml.SevenNVLINKLinks:
+			score += 700
+		case nvml.EightNVLINKLinks:
+			score += 800
+		case nvml.NineNVLINKLinks:
+			score += 900
+		case nvml.TenNVLINKLinks:
+			score += 1000
+		case nvml.ElevenNVLINKLinks:
+			score += 1100
+		case nvml.TwelveNVLINKLinks:
+			score += 1200
+		}
+	}
+
+	return score
+}
diff --git a/pkg/scheduler/plugins/kmpredicates/kmpredicates.go b/pkg/scheduler/plugins/kmpredicates/kmpredicates.go
new file mode 100644
index 0000000..50b5a42
--- /dev/null
+++ b/pkg/scheduler/plugins/kmpredicates/kmpredicates.go
@@ -0,0 +1,263 @@
+/*
+Copyright 2018 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package kmpredicates
+
+import (
+	"context"
+	"fmt"
+
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/labels"
+	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/klog"
+	"k8s.io/kubernetes/pkg/scheduler/apis/config"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/interpodaffinity"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeaffinity"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeports"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeunschedulable"
+	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/tainttoleration"
+	k8sframework "k8s.io/kubernetes/pkg/scheduler/framework/v1alpha1"
+
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/plugins/util"
+	"volcano.sh/volcano/pkg/scheduler/plugins/util/k8s"
+)
+
+const (
+	// PluginName indicates name of volcano scheduler plugin.
+	PluginName = "kmpredicates"
+
+	// GPUTopoPredicate is the key for enableing GPU Topo Predicate in YAML
+	GPUTopoPredicate = "kmpredicate.GPUTopoEnable"
+)
+
+type predicatesPlugin struct {
+	// Arguments given for the plugin
+	pluginArguments framework.Arguments
+}
+
+// New return predicate plugin
+func New(arguments framework.Arguments) framework.Plugin {
+	return &predicatesPlugin{pluginArguments: arguments}
+}
+
+func (pp *predicatesPlugin) Name() string {
+	return PluginName
+}
+
+type predicateEnable struct {
+	gpuTopoEnable bool
+}
+
+func enablePredicate(args framework.Arguments) predicateEnable {
+
+	/*
+	   User Should give predicatesEnable in this format(predicate.GPUSharingEnable).
+	   Currently supported only GPUSharing predicate checks.
+
+	   actions: "reclaim, allocate, backfill, preempt"
+	   tiers:
+	   - plugins:
+	     - name: priority
+	     - name: gang
+	     - name: conformance
+	   - plugins:
+	     - name: drf
+	     - name: kmpredicates
+	     - name: proportion
+	     - name: nodeorder
+	*/
+
+	predicate := predicateEnable{
+		gpuTopoEnable: false,
+	}
+
+	args.GetBool(&predicate.gpuTopoEnable, GPUTopoPredicate)
+
+	return predicate
+}
+
+func (pp *predicatesPlugin) OnSessionOpen(ssn *framework.Session) {
+	pl := util.NewPodLister(ssn)
+	pods, _ := pl.List(labels.NewSelector())
+	nodeMap, nodeSlice := util.GenerateNodeMapAndSlice(ssn.Nodes)
+
+	predicate := enablePredicate(pp.pluginArguments)
+
+	kubeClient := ssn.KubeClient()
+	// Register event handlers to update task info in PodLister & nodeMap
+	ssn.AddEventHandler(&framework.EventHandler{
+		AllocateFunc: func(event *framework.Event) {
+			pod := pl.UpdateTask(event.Task, event.Task.NodeName)
+
+			nodeName := event.Task.NodeName
+			node, found := nodeMap[nodeName]
+			if !found {
+				klog.Errorf("predicates, update pod %s/%s allocate to NOT EXIST node [%s]", pod.Namespace, pod.Name, nodeName)
+				return
+			}
+
+			if api.GetGPUCountOfPod(pod) > 0 {
+				nodeInfo, _ := ssn.Nodes[nodeName]
+				ids := allocateGPUs(pod, nodeInfo, predicate.gpuTopoEnable)
+				if len(ids) < 0 {
+					klog.Errorf("The node %s can't place the pod %s in ns %s", pod.Spec.NodeName, pod.Name, pod.Namespace)
+					return
+				}
+				patch := api.AddGPUInfoPatch(nodeInfo, pod, ids)
+				pod, err := kubeClient.CoreV1().Pods(pod.Namespace).Patch(context.TODO(), pod.Name, types.JSONPatchType, []byte(patch), metav1.PatchOptions{})
+				if err != nil {
+					klog.Errorf("Patch pod %s failed with patch %s: %v", pod.Name, patch, err)
+					return
+				}
+				// update device info
+				klog.V(4).Infof("predicates pod %s gpu ids: %+v", pod.Name, ids)
+				for _, id := range ids {
+					dev, _ := nodeInfo.GPUDevices[id]
+					dev.PodMap[string(pod.UID)] = pod
+					klog.V(4).Infof("sync podMap, pod %s gpu id: %+v", pod.Name, id)
+				}
+				klog.V(4).Infof("predicates with gpu sharing, update pod %s/%s allocate to node [%s]", pod.Namespace, pod.Name, nodeName)
+			}
+
+			node.AddPod(pod)
+			klog.V(4).Infof("predicates, update pod %s/%s allocate to node [%s]", pod.Namespace, pod.Name, nodeName)
+		},
+		DeallocateFunc: func(event *framework.Event) {
+			pod := pl.UpdateTask(event.Task, "")
+			nodeName := event.Task.NodeName
+			node, found := nodeMap[nodeName]
+			if !found {
+				klog.Errorf("predicates, update pod %s/%s allocate from NOT EXIST node [%s]", pod.Namespace, pod.Name, nodeName)
+				return
+			}
+
+			if api.GetGPUCountOfPod(pod) > 0 {
+				// deallocate pod gpu id
+				ids, err := api.GetGPUIndexs(pod)
+				if err != nil {
+					klog.Errorf("Get pod %s gpu index failed, err: %v", pod.Name, err)
+					return
+				}
+				patch := api.RemoveGPUInfoPatch()
+				_, err = kubeClient.CoreV1().Pods(pod.Namespace).Patch(context.TODO(), pod.Name, types.JSONPatchType, []byte(patch), metav1.PatchOptions{})
+				if err != nil {
+					klog.Errorf("Patch pod %s failed with patch %s: %v", pod.Name, patch, err)
+					return
+				}
+
+				nodeInfo, _ := ssn.Nodes[nodeName]
+				for _, id := range ids {
+					if dev, ok := nodeInfo.GPUDevices[id]; ok {
+						delete(dev.PodMap, string(pod.UID))
+					}
+				}
+
+				klog.V(4).Infof("predicates with gpu sharing, update pod %s/%s deallocate from node [%s]", pod.Namespace, pod.Name, nodeName)
+			}
+
+			node.RemovePod(pod)
+			klog.V(4).Infof("predicates, update pod %s/%s deallocate from node [%s]", pod.Namespace, pod.Name, nodeName)
+
+		},
+	})
+
+	// Initialize k8s plugins
+	// TODO: Add more predicates, k8s.io/kubernetes/pkg/scheduler/framework/plugins/legacy_registry.go
+	handle := k8s.NewFrameworkHandle(pods, nodeSlice)
+	// 1. NodeUnschedulable
+	plugin, _ := nodeunschedulable.New(nil, handle)
+	nodeUnscheduleFilter := plugin.(*nodeunschedulable.NodeUnschedulable)
+	// 2. NodeAffinity
+	plugin, _ = nodeaffinity.New(nil, handle)
+	nodeAffinityFilter := plugin.(*nodeaffinity.NodeAffinity)
+	// 3. NodePorts
+	plugin, _ = nodeports.New(nil, handle)
+	nodePortFilter := plugin.(*nodeports.NodePorts)
+	// 4. TaintToleration
+	plugin, _ = tainttoleration.New(nil, handle)
+	tolerationFilter := plugin.(*tainttoleration.TaintToleration)
+	// 5. InterPodAffinity
+	plArgs := &config.InterPodAffinityArgs{}
+	plugin, _ = interpodaffinity.New(plArgs, handle)
+	podAffinityFilter := plugin.(*interpodaffinity.InterPodAffinity)
+
+	ssn.AddPredicateFn(pp.Name(), func(task *api.TaskInfo, node *api.NodeInfo) error {
+		nodeInfo, found := nodeMap[node.Name]
+		if !found {
+			return fmt.Errorf("failed to predicates, node info for %s not found", node.Name)
+		}
+
+		if node.Allocatable.MaxTaskNum <= len(nodeInfo.Pods) {
+			klog.V(4).Infof("NodePodNumber predicates Task <%s/%s> on Node <%s> failed",
+				task.Namespace, task.Name, node.Name)
+			return api.NewFitError(task, node, api.NodePodNumberExceeded)
+		}
+
+		state := k8sframework.NewCycleState()
+		// CheckNodeUnschedulable
+		status := nodeUnscheduleFilter.Filter(context.TODO(), state, task.Pod, nodeInfo)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s predicates failed %s", nodeunschedulable.Name, status.Message())
+		}
+
+		// Check NodeAffinity
+		status = nodeAffinityFilter.Filter(context.TODO(), state, task.Pod, nodeInfo)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s predicates failed %s", nodeaffinity.Name, status.Message())
+		}
+
+		// Check NodePorts
+		nodePortFilter.PreFilter(context.TODO(), state, task.Pod)
+		status = nodePortFilter.Filter(context.TODO(), state, nil, nodeInfo)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s predicates failed %s", nodeaffinity.Name, status.Message())
+		}
+
+		// PodToleratesNodeTaints: TaintToleration
+		status = tolerationFilter.Filter(context.TODO(), state, task.Pod, nodeInfo)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s predicates failed %s", tainttoleration.Name, status.Message())
+		}
+
+		// InterPodAffinity Predicate
+		status = podAffinityFilter.PreFilter(context.TODO(), state, task.Pod)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s pre-predicates failed %s", interpodaffinity.Name, status.Message())
+		}
+
+		status = podAffinityFilter.Filter(context.TODO(), state, task.Pod, nodeInfo)
+		if !status.IsSuccess() {
+			return fmt.Errorf("plugin %s predicates failed %s", interpodaffinity.Name, status.Message())
+		}
+
+		if api.GetGPUCountOfPod(task.Pod) > 0 {
+			// checkNodeGPUPredicate assume pod gpu resource on node
+			fit, err := checkNodeGPUPredicate(task, node)
+			if err != nil {
+				return err
+			}
+
+			klog.V(4).Infof("checkNodeGPUPredicate predicates Task <%s/%s> on Node <%s>: fit %v",
+				task.Namespace, task.Name, node.Name, fit)
+		}
+		return nil
+	})
+}
+
+func (pp *predicatesPlugin) OnSessionClose(ssn *framework.Session) {}
diff --git a/pkg/scheduler/plugins/kmpredicates/kmpredicates_test.go b/pkg/scheduler/plugins/kmpredicates/kmpredicates_test.go
new file mode 100644
index 0000000..aec712d
--- /dev/null
+++ b/pkg/scheduler/plugins/kmpredicates/kmpredicates_test.go
@@ -0,0 +1,196 @@
+/*
+Copyright 2019 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package kmpredicates
+
+import (
+	"fmt"
+	"testing"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/tools/record"
+
+	schedulingv1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/cache"
+	"volcano.sh/volcano/pkg/scheduler/conf"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/util"
+)
+
+func addResource(resourceList v1.ResourceList, name v1.ResourceName, need string) {
+	resourceList[name] = resource.MustParse(need)
+}
+
+func TestNode(t *testing.T) {
+	framework.RegisterPluginBuilder(PluginName, New)
+	defer framework.CleanupPluginBuilders()
+
+	GPUCount := v1.ResourceName("baidu.com/v100_32g_cgpu")
+	GPUCore := v1.ResourceName("baidu.com/v100_32g_cgpu_core")
+	GPUMemory := v1.ResourceName("baidu.com/v100_32g_cgpu_memory")
+
+	p1 := util.BuildPod("c1", "p1", "", v1.PodPending, util.BuildResourceList("40", "1Gi"), "pg1", make(map[string]string), make(map[string]string))
+	addResource(p1.Spec.Containers[0].Resources.Requests, GPUCount, "2")
+	p2 := util.BuildPod("c1", "p2", "", v1.PodPending, util.BuildResourceList("1.5", "0Gi"), "pg1", make(map[string]string), make(map[string]string))
+	p3 := util.BuildPod("c1", "p3", "", v1.PodPending, util.BuildResourceList("8", "10Gi"), "pg1", make(map[string]string), make(map[string]string))
+	addResource(p3.Spec.Containers[0].Resources.Requests, GPUCount, "2")
+	p4 := util.BuildPod("c1", "p4", "", v1.PodPending, util.BuildResourceList("3", "4Gi"), "pg1", make(map[string]string), make(map[string]string))
+	addResource(p4.Spec.Containers[0].Resources.Requests, GPUCount, "3")
+
+	n1 := util.BuildNode("n1", util.BuildResourceList("8", "4Gi"), make(map[string]string))
+	addResource(n1.Status.Allocatable, GPUCount, "8")
+	addResource(n1.Status.Allocatable, GPUCore, "800")
+	addResource(n1.Status.Allocatable, GPUMemory, "32")
+	n2 := util.BuildNode("n2", util.BuildResourceList("4", "16Gi"), make(map[string]string))
+	addResource(n2.Status.Allocatable, GPUCount, "8")
+	addResource(n2.Status.Allocatable, GPUCore, "800")
+	addResource(n2.Status.Allocatable, GPUMemory, "32")
+	n3 := util.BuildNode("n3", util.BuildResourceList("8", "4Gi"), make(map[string]string))
+	addResource(n3.Status.Allocatable, GPUCount, "8")
+	addResource(n3.Status.Allocatable, GPUCore, "800")
+	addResource(n3.Status.Allocatable, GPUMemory, "32")
+
+	pg1 := &schedulingv1.PodGroup{
+		ObjectMeta: metav1.ObjectMeta{
+			Name:      "pg1",
+			Namespace: "c1",
+		},
+		Spec: schedulingv1.PodGroupSpec{
+			Queue: "c1",
+		},
+	}
+	queue1 := &schedulingv1.Queue{
+		ObjectMeta: metav1.ObjectMeta{
+			Name: "c1",
+		},
+		Spec: schedulingv1.QueueSpec{
+			Weight: 1,
+		},
+	}
+
+	tests := []struct {
+		name      string
+		podGroups []*schedulingv1.PodGroup
+		pods      []*v1.Pod
+		nodes     []*v1.Node
+		queues    []*schedulingv1.Queue
+		arguments framework.Arguments
+		expected  map[string]map[string]error
+	}{
+		{
+			name: "single job",
+			podGroups: []*schedulingv1.PodGroup{
+				pg1,
+			},
+			queues: []*schedulingv1.Queue{
+				queue1,
+			},
+			pods: []*v1.Pod{
+				p1, p2, p3, p4,
+			},
+			nodes: []*v1.Node{
+				n1, n2, n3,
+			},
+			arguments: framework.Arguments{
+				"kmpredicate.GPUTopoEnable": "true",
+			},
+			expected: map[string]map[string]error{
+				"c1/p1": {
+					"n1": fmt.Errorf("no enough gpu resource on devices of node"),
+					"n2": nil,
+					"n3": nil,
+				},
+				"c1/p2": {
+					"n1": nil,
+					"n2": nil,
+					"n3": nil,
+				},
+				"c1/p3": {
+					"n1": nil,
+					"n2": nil,
+					"n3": nil,
+				},
+				"c1/p4": {
+					"n1": nil,
+					"n2": nil,
+					"n3": nil,
+				},
+			},
+		},
+	}
+
+	for i, test := range tests {
+		binder := &util.FakeBinder{
+			Binds:   map[string]string{},
+			Channel: make(chan string),
+		}
+		schedulerCache := &cache.SchedulerCache{
+			Nodes:         make(map[string]*api.NodeInfo),
+			Jobs:          make(map[api.JobID]*api.JobInfo),
+			Queues:        make(map[api.QueueID]*api.QueueInfo),
+			Binder:        binder,
+			StatusUpdater: &util.FakeStatusUpdater{},
+			VolumeBinder:  &util.FakeVolumeBinder{},
+
+			Recorder: record.NewFakeRecorder(100),
+		}
+		for _, node := range test.nodes {
+			schedulerCache.AddNode(node)
+		}
+		for _, pod := range test.pods {
+			schedulerCache.AddPod(pod)
+		}
+		for _, ss := range test.podGroups {
+			schedulerCache.AddPodGroupV1beta1(ss)
+		}
+		for _, q := range test.queues {
+			schedulerCache.AddQueueV1beta1(q)
+		}
+
+		trueValue := true
+		ssn := framework.OpenSession(schedulerCache, []conf.Tier{
+			{
+				Plugins: []conf.PluginOption{
+					{
+						Name:             PluginName,
+						EnabledNodeOrder: &trueValue,
+						Arguments:        test.arguments,
+					},
+				},
+			},
+		}, nil)
+		defer framework.CloseSession(ssn)
+		t.Log(ssn.Jobs)
+		t.Log(ssn.Nodes)
+
+		for _, job := range ssn.Jobs {
+			for _, task := range job.Tasks {
+				taskID := fmt.Sprintf("%s/%s", task.Namespace, task.Name)
+				for _, node := range ssn.Nodes {
+					err := ssn.PredicateFn(task, node)
+					t.Log("=============err: ", err)
+					if (err != nil && test.expected[taskID][node.Name] == nil) || (err == nil && test.expected[taskID][node.Name] != nil) {
+						t.Errorf("case%d: task %s on node %s has err %v", i, taskID, node.Name, err)
+						continue
+					}
+				}
+			}
+		}
+	}
+}
diff --git a/pkg/scheduler/plugins/proportion-pf/proportion_pf.go b/pkg/scheduler/plugins/proportion-pf/proportion_pf.go
new file mode 100644
index 0000000..2d612d8
--- /dev/null
+++ b/pkg/scheduler/plugins/proportion-pf/proportion_pf.go
@@ -0,0 +1,351 @@
+/*
+Copyright 2018 The Kubernetes Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package proportionpf
+
+import (
+	"reflect"
+
+	"k8s.io/klog"
+
+	"volcano.sh/apis/pkg/apis/scheduling"
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/scheduler/api/helpers"
+	"volcano.sh/volcano/pkg/scheduler/framework"
+	"volcano.sh/volcano/pkg/scheduler/metrics"
+	"volcano.sh/volcano/pkg/scheduler/plugins/util"
+)
+
+// PluginName indicates name of volcano scheduler plugin.
+const PluginName = "proportionpf"
+
+type proportionPlugin struct {
+	totalResource *api.Resource
+	queueOpts     map[api.QueueID]*queueAttr
+
+	// Arguments given for the plugin
+	pluginArguments framework.Arguments
+}
+
+type queueAttr struct {
+	queueID api.QueueID
+	name    string
+	weight  int32
+	share   float64
+
+	namespace         string
+	resourceQuotaName string
+
+	deserved  *api.Resource
+	allocated *api.Resource
+	request   *api.Resource
+	// inqueue represents the resource request of the inqueue job
+	inqueue    *api.Resource
+	capability *api.Resource
+}
+
+// GetJobMinResources return the min resources of podgroup.
+func GetJobMinResources(s scheduling.PodGroupSpec) *api.Resource {
+	if s.MinResources == nil {
+		return api.EmptyResource()
+	}
+
+	return api.NewResource(*s.MinResources)
+}
+
+// New return proportion action
+func New(arguments framework.Arguments) framework.Plugin {
+	return &proportionPlugin{
+		totalResource:   api.EmptyResource(),
+		queueOpts:       map[api.QueueID]*queueAttr{},
+		pluginArguments: arguments,
+	}
+}
+
+func (pp *proportionPlugin) Name() string {
+	return PluginName
+}
+
+func (pp *proportionPlugin) OnSessionOpen(ssn *framework.Session) {
+	// Prepare scheduling data for this session.
+	totalResource := api.EmptyResource()
+	for _, n := range ssn.Nodes {
+		pp.totalResource.Add(n.Idle)
+		totalResource.Add(n.Allocatable)
+	}
+	klog.V(4).Infof("The total resource is <%v>, and total idle resource is <%v>", totalResource, pp.totalResource)
+
+	// Build attributes for Queues.
+	for _, job := range ssn.Jobs {
+		klog.V(4).Infof("Considering Job <%s/%s>, task number is <%v>.", job.Namespace, job.Name, len(job.Tasks))
+		if _, found := pp.queueOpts[job.Queue]; !found {
+			queue := ssn.Queues[job.Queue]
+			attr := &queueAttr{
+				queueID:           queue.UID,
+				name:              queue.Name,
+				weight:            queue.Weight,
+
+				deserved:  api.EmptyResource(),
+				allocated: api.EmptyResource(),
+				request:   api.EmptyResource(),
+				inqueue:   api.EmptyResource(),
+			}
+			if len(queue.ElasticResourceQuota.Spec.Max) != 0 {
+				attr.capability = api.NewResource(queue.ElasticResourceQuota.Spec.Max)
+			}
+
+			pp.queueOpts[job.Queue] = attr
+			klog.V(4).Infof("Added Queue <%s> attributes.", job.Queue)
+		}
+
+		attr := pp.queueOpts[job.Queue]
+		for status, tasks := range job.TaskStatusIndex {
+			if api.AllocatedStatus(status) {
+				for _, t := range tasks {
+					attr.allocated.Add(t.Resreq)
+					attr.deserved.Add(t.Resreq)
+					attr.request.Add(t.Resreq)
+				}
+			} else if status == api.Pending {
+				for _, t := range tasks {
+					attr.request.Add(t.Resreq)
+				}
+			}
+		}
+
+		if job.PodGroup.Status.Phase == scheduling.PodGroupInqueue {
+			attr.inqueue.Add(GetJobMinResources(job.PodGroup.Spec))
+		}
+	}
+
+	// Record metrics
+	for _, attr := range pp.queueOpts {
+		metrics.UpdateQueueAllocated(attr.name, attr.allocated.MilliCPU, attr.allocated.Memory)
+		metrics.UpdateQueueRequest(attr.name, attr.request.MilliCPU, attr.request.Memory)
+		metrics.UpdateQueueWeight(attr.name, attr.weight)
+		queue := ssn.Queues[attr.queueID]
+		metrics.UpdateQueuePodGroupInqueueCount(attr.name, queue.Queue.Status.Inqueue)
+		metrics.UpdateQueuePodGroupPendingCount(attr.name, queue.Queue.Status.Pending)
+		metrics.UpdateQueuePodGroupRunningCount(attr.name, queue.Queue.Status.Running)
+		metrics.UpdateQueuePodGroupUnknownCount(attr.name, queue.Queue.Status.Unknown)
+	}
+
+	remaining := pp.totalResource.Clone()
+	klog.V(4).Infof("The total remaining resource is <%v>", remaining)
+	meet := map[api.QueueID]struct{}{}
+	for {
+		totalWeight := int32(0)
+		for _, attr := range pp.queueOpts {
+			klog.V(4).Infof("Consider queue<%v>, allocated<%v>, request<%v>", attr.queueID, attr.allocated, attr.request)
+			if _, found := meet[attr.queueID]; found {
+				continue
+			}
+			if attr.allocated.Less(attr.request) {
+				totalWeight += attr.weight
+			}
+		}
+
+		// If no queues in resourcequota, skip
+		if totalWeight == 0 {
+			klog.V(4).Infof("Skip when the total weight of queue is 0")
+			break
+		}
+
+		oldRemaining := remaining.Clone()
+		// Calculates the deserved of each Queue.
+		// increasedDeserved is the increased value for attr.deserved of processed queues
+		// decreasedDeserved is the decreased value for attr.deserved of processed queues
+		increasedDeserved := api.EmptyResource()
+		decreasedDeserved := api.EmptyResource()
+		for _, attr := range pp.queueOpts {
+			klog.V(4).Infof("Considering Queue <%s>: weight <%d>, total weight <%d>.",
+				attr.name, attr.weight, totalWeight)
+			if _, found := meet[attr.queueID]; found {
+				continue
+			}
+			if !attr.allocated.Less(attr.request) {
+				klog.V(4).Infof("Skip when there are no new jobs on the queue <%s>.", attr.name)
+				continue
+			}
+
+			oldDeserved := attr.deserved.Clone()
+			attr.deserved.Add(remaining.Clone().Multi(float64(attr.weight) / float64(totalWeight)))
+
+			klog.V(5).Infof("Queue <%s>: deserved <%+v>, request <%+v>, capability<%+v>, resource quota remaining <%+v>.",
+				attr.name, attr.deserved, attr.request, attr.capability, remaining)
+			if attr.capability != nil && !attr.deserved.LessEqualNew(attr.capability, api.Infinity) {
+				attr.deserved = helpers.Min(attr.deserved, attr.capability)
+				if attr.request.LessEqualNew(attr.deserved, api.Zero) {
+					attr.deserved = attr.request
+					meet[attr.queueID] = struct{}{}
+					klog.V(4).Infof("queue <%s> is meet cause of the capability", attr.name)
+				}
+			} else if attr.request.LessEqualNew(attr.deserved, api.Zero) {
+				attr.deserved = helpers.Min(attr.deserved, attr.request)
+				meet[attr.queueID] = struct{}{}
+				klog.V(4).Infof("queue <%s> is meet", attr.name)
+			} else {
+				attr.deserved.MinDimensionResource(attr.request)
+				klog.V(4).Infof("Format queue <%s> deserved resource to <%v>", attr.name, attr.deserved)
+			}
+			pp.updateShare(attr)
+
+			klog.V(4).Infof("The attributes of queue <%s> in proportion: deserved <%v>, allocate <%v>, request <%v>, share <%0.2f>",
+				attr.name, attr.deserved, attr.allocated, attr.request, attr.share)
+
+			increased, decreased := attr.deserved.Diff(oldDeserved)
+			increasedDeserved.Add(increased)
+			decreasedDeserved.Add(decreased)
+
+			// Record metrics
+			metrics.UpdateQueueDeserved(attr.name, attr.deserved.MilliCPU, attr.deserved.Memory)
+		}
+
+		remaining.Add(decreasedDeserved).Sub(increasedDeserved)
+		klog.V(4).Infof("Remaining resource is <%s>, oldRemaining resource is <%s>", remaining, oldRemaining)
+		if remaining.IsEmpty() || reflect.DeepEqual(remaining, oldRemaining) {
+			klog.V(4).Infof("Exiting when remaining is empty or no queue has more resource request:  <%v>", remaining)
+			break
+		}
+	}
+
+	ssn.AddQueueOrderFn(pp.Name(), func(l, r interface{}) int {
+		lv := l.(*api.QueueInfo)
+		rv := r.(*api.QueueInfo)
+
+		if pp.queueOpts[lv.UID].share == pp.queueOpts[rv.UID].share {
+			return 0
+		}
+
+		if pp.queueOpts[lv.UID].share < pp.queueOpts[rv.UID].share {
+			return -1
+		}
+
+		return 1
+	})
+
+	ssn.AddReclaimableFn(pp.Name(), func(reclaimer *api.TaskInfo, reclaimees []*api.TaskInfo) []*api.TaskInfo {
+		var victims []*api.TaskInfo
+		allocations := map[api.QueueID]*api.Resource{}
+
+		for _, reclaimee := range reclaimees {
+			job := ssn.Jobs[reclaimee.Job]
+			attr := pp.queueOpts[job.Queue]
+
+			if _, found := allocations[job.Queue]; !found {
+				allocations[job.Queue] = attr.allocated.Clone()
+			}
+			allocated := allocations[job.Queue]
+			if allocated.Less(reclaimee.Resreq) {
+				klog.V(3).Infof("Failed to allocate resource for Task <%s/%s> in Queue <%s>, not enough resource.",
+					reclaimee.Namespace, reclaimee.Name, job.Queue)
+				continue
+			}
+
+			allocated.Sub(reclaimee.Resreq)
+			if attr.deserved.LessEqualStrict(allocated) {
+				victims = append(victims, reclaimee)
+			}
+		}
+		klog.V(4).Infof("Victims from proportion plugins are %+v", victims)
+		return victims
+	})
+
+	ssn.AddOverusedFn(pp.Name(), func(obj interface{}) bool {
+		queue := obj.(*api.QueueInfo)
+		attr := pp.queueOpts[queue.UID]
+
+		overused := !attr.allocated.LessEqual(attr.deserved)
+		metrics.UpdateQueueOverused(attr.name, overused)
+		if overused {
+			klog.V(3).Infof("Queue <%v>: deserved <%v>, allocated <%v>, share <%v>",
+				queue.Name, attr.deserved, attr.allocated, attr.share)
+		}
+
+		return overused
+	})
+
+	ssn.AddJobEnqueueableFn(pp.Name(), func(obj interface{}) int {
+		job := obj.(*api.JobInfo)
+		queueID := job.Queue
+		attr := pp.queueOpts[queueID]
+		queue := ssn.Queues[queueID]
+
+		// If no capability is set, always enqueue the job.
+		if len(queue.ElasticResourceQuota.Spec.Max) == 0 {
+			klog.V(4).Infof("Capability of queue <%s> was not set, allow job <%s/%s> to Inqueue.",
+				queue.Name, job.Namespace, job.Name)
+			return util.Permit
+		}
+
+		if job.PodGroup.Spec.MinResources == nil {
+			return util.Permit
+		}
+		minReq := GetJobMinResources(job.PodGroup.Spec)
+		// The queue resource quota limit has not reached
+		inqueue := minReq.Add(attr.allocated).Add(attr.inqueue).LessEqual(api.NewResource(queue.ElasticResourceQuota.Spec.Max))
+		if inqueue {
+			attr.inqueue.Add(GetJobMinResources(job.PodGroup.Spec))
+			return util.Permit
+		}
+		return util.Reject
+	})
+
+	// Register event handlers.
+	ssn.AddEventHandler(&framework.EventHandler{
+		AllocateFunc: func(event *framework.Event) {
+			job := ssn.Jobs[event.Task.Job]
+			attr := pp.queueOpts[job.Queue]
+			attr.allocated.Add(event.Task.Resreq)
+			metrics.UpdateQueueAllocated(attr.name, attr.allocated.MilliCPU, attr.allocated.Memory)
+
+			pp.updateShare(attr)
+
+			klog.V(4).Infof("Proportion AllocateFunc: task <%v/%v>, resreq <%v>,  share <%v>",
+				event.Task.Namespace, event.Task.Name, event.Task.Resreq, attr.share)
+		},
+		DeallocateFunc: func(event *framework.Event) {
+			job := ssn.Jobs[event.Task.Job]
+			attr := pp.queueOpts[job.Queue]
+			attr.allocated.Sub(event.Task.Resreq)
+			metrics.UpdateQueueAllocated(attr.name, attr.allocated.MilliCPU, attr.allocated.Memory)
+
+			pp.updateShare(attr)
+
+			klog.V(4).Infof("Proportion EvictFunc: task <%v/%v>, resreq <%v>,  share <%v>",
+				event.Task.Namespace, event.Task.Name, event.Task.Resreq, attr.share)
+		},
+	})
+}
+
+func (pp *proportionPlugin) OnSessionClose(ssn *framework.Session) {
+	pp.totalResource = nil
+	pp.queueOpts = nil
+}
+
+func (pp *proportionPlugin) updateShare(attr *queueAttr) {
+	res := float64(0)
+
+	for _, rn := range attr.deserved.ResourceNames() {
+		share := helpers.Share(attr.allocated.Get(rn), attr.deserved.Get(rn))
+		if share > res {
+			res = share
+		}
+	}
+
+	attr.share = res
+	metrics.UpdateQueueShare(attr.name, attr.share)
+}
diff --git a/pkg/scheduler/plugins/util/nvidia/nvml.go b/pkg/scheduler/plugins/util/nvidia/nvml.go
new file mode 100644
index 0000000..47e454f
--- /dev/null
+++ b/pkg/scheduler/plugins/util/nvidia/nvml.go
@@ -0,0 +1,62 @@
+// Copyright (c) 2015-2018, NVIDIA CORPORATION. All rights reserved.
+//  github.com/NVIDIA/gpu-monitoring-tools/bindings/go/nvml/nvml.go
+
+package nvidia
+
+type P2PLinkType uint
+
+const (
+	P2PLinkUnknown P2PLinkType = iota
+	P2PLinkCrossCPU
+	P2PLinkSameCPU
+	P2PLinkHostBridge
+	P2PLinkMultiSwitch
+	P2PLinkSingleSwitch
+	P2PLinkSameBoard
+	SingleNVLINKLink
+	TwoNVLINKLinks
+	ThreeNVLINKLinks
+	FourNVLINKLinks
+	FiveNVLINKLinks
+	SixNVLINKLinks
+	SevenNVLINKLinks
+	EightNVLINKLinks
+	NineNVLINKLinks
+	TenNVLINKLinks
+	ElevenNVLINKLinks
+	TwelveNVLINKLinks
+)
+
+type P2PLink struct {
+	BusID string
+	Link  P2PLinkType
+}
+
+type ClockInfo struct {
+	Cores  *uint
+	Memory *uint
+}
+
+type PCIInfo struct {
+	BusID     string
+	BAR1      *uint64
+	Bandwidth *uint
+}
+
+type CudaComputeCapabilityInfo struct {
+	Major *int
+	Minor *int
+}
+
+type Device struct {
+	UUID                  string
+	Path                  string
+	Model                 *string
+	Power                 *uint
+	Memory                *uint64
+	CPUAffinity           *uint
+	PCI                   PCIInfo
+	Clocks                ClockInfo
+	Topology              []P2PLink
+	CudaComputeCapability CudaComputeCapabilityInfo
+}
diff --git a/pkg/webhooks/admission/elasticresourcequotas/mutate/mutate_elastic_resource_quota.go b/pkg/webhooks/admission/elasticresourcequotas/mutate/mutate_elastic_resource_quota.go
new file mode 100644
index 0000000..32bf3ee
--- /dev/null
+++ b/pkg/webhooks/admission/elasticresourcequotas/mutate/mutate_elastic_resource_quota.go
@@ -0,0 +1,135 @@
+/*
+Copyright 2018 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package mutate
+
+import (
+	"encoding/json"
+	"fmt"
+	"strings"
+
+	"k8s.io/api/admission/v1beta1"
+	whv1beta1 "k8s.io/api/admissionregistration/v1beta1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+	"volcano.sh/volcano/pkg/webhooks/router"
+	"volcano.sh/volcano/pkg/webhooks/schema"
+	"volcano.sh/volcano/pkg/webhooks/util"
+)
+
+func init() {
+	router.RegisterAdmission(service)
+}
+
+var service = &router.AdmissionService{
+	Path: "/elasticresourcequotas/mutate",
+	Func: ElasticResourceQuotas,
+
+	Config: config,
+
+	MutatingConfig: &whv1beta1.MutatingWebhookConfiguration{
+		Webhooks: []whv1beta1.MutatingWebhook{{
+			Name: "mutateelasticresourcequotas.volcano.sh",
+			Rules: []whv1beta1.RuleWithOperations{
+				{
+					Operations: []whv1beta1.OperationType{whv1beta1.Create},
+					Rule: whv1beta1.Rule{
+						APIGroups:   []string{schedulingv1beta1.SchemeGroupVersion.Group},
+						APIVersions: []string{schedulingv1beta1.SchemeGroupVersion.Version},
+						Resources:   []string{"elasticresourcequotas"},
+					},
+				},
+			},
+		}},
+	},
+}
+
+var config = &router.AdmissionServiceConfig{}
+
+type patchOperation struct {
+	Op    string      `json:"op"`
+	Path  string      `json:"path"`
+	Value interface{} `json:"value,omitempty"`
+}
+
+// ElasticResourceQuotas mutate elasticresourcequota.
+func ElasticResourceQuotas(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
+	klog.V(3).Infof("Mutating %s elasticResourceQuota[%s].", ar.Request.Operation, ar.Request.Name)
+
+	elasticResourceQuota, err := schema.DecodeElasticResourceQuota(ar.Request.Object, ar.Request.Resource)
+	if err != nil {
+		return util.ToAdmissionResponse(err)
+	}
+
+	var patchBytes []byte
+	switch ar.Request.Operation {
+	case v1beta1.Create:
+		// patch Max if not exist
+		patchBytes, err = createElasticResourceQuotaPatch(elasticResourceQuota)
+	default:
+		return util.ToAdmissionResponse(fmt.Errorf("invalid operation `%s`, "+
+			"expect operation to be `CREATE`", ar.Request.Operation))
+	}
+
+	if err != nil {
+		return &v1beta1.AdmissionResponse{
+			Allowed: false,
+			Result:  &metav1.Status{Message: err.Error()},
+		}
+	}
+
+	pt := v1beta1.PatchTypeJSONPatch
+	klog.V(4).Infof("Mutating %s elasticResourceQuota[%s] patchBytes=[%s]", ar.Request.Operation, ar.Request.Name, patchBytes)
+	return &v1beta1.AdmissionResponse{
+		Allowed:   true,
+		Patch:     patchBytes,
+		PatchType: &pt,
+	}
+}
+
+func createElasticResourceQuotaPatch(elasticResourceQuota *schedulingv1beta1.ElasticResourceQuota) ([]byte, error) {
+	var patch []patchOperation
+	// filter err or logical type
+	quotaType, exist := elasticResourceQuota.Labels[schedulingv1beta1.QuotaTypeKey]
+	if !exist {
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  fmt.Sprintf("/metadata/labels/%s", strings.ReplaceAll(schedulingv1beta1.QuotaTypeKey, "/", "~1")),
+			Value: schedulingv1beta1.QuotaTypeLogical,
+		})
+		return json.Marshal(patch)
+	} else if quotaType == schedulingv1beta1.QuotaTypeLogical {
+		return nil, nil
+	}
+
+	// get nodes resource
+	nodeSumResource, err := common.CalcManagedNodesResources(config.KubeClient, elasticResourceQuota.Name)
+	if err != nil {
+		klog.Errorf("createElasticResourceQuotaPatch CalcManagedNodesResources failed, err=[%+v]", err)
+		return nil, err
+	}
+	// patch spec.Min by nodeSumResourceList
+	nodeSumResourceList := common.NewResourceList(nodeSumResource)
+	patch = append(patch, patchOperation{
+		Op:    "add",
+		Path:  "/spec/min",
+		Value: &nodeSumResourceList,
+	})
+	return json.Marshal(patch)
+}
diff --git a/pkg/webhooks/admission/elasticresourcequotas/validate/validate_elastic_resource_quota.go b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_elastic_resource_quota.go
new file mode 100644
index 0000000..5a767d5
--- /dev/null
+++ b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_elastic_resource_quota.go
@@ -0,0 +1,215 @@
+/*
+Copyright 2018 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package validate
+
+import (
+	"context"
+	"fmt"
+	"k8s.io/api/admission/v1beta1"
+	whv1beta1 "k8s.io/api/admissionregistration/v1beta1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/webhooks/router"
+	"volcano.sh/volcano/pkg/webhooks/schema"
+	"volcano.sh/volcano/pkg/webhooks/util"
+)
+
+func init() {
+	router.RegisterAdmission(service)
+}
+
+var service = &router.AdmissionService{
+	Path: "/elasticresourcequotas/validate",
+	Func: AdmitElasticresourcequotas,
+
+	Config: config,
+
+	ValidatingConfig: &whv1beta1.ValidatingWebhookConfiguration{
+		Webhooks: []whv1beta1.ValidatingWebhook{{
+			Name: "validateelasticresourcequotas.volcano.sh",
+			Rules: []whv1beta1.RuleWithOperations{
+				{
+					Operations: []whv1beta1.OperationType{whv1beta1.Create, whv1beta1.Update, whv1beta1.Delete},
+					Rule: whv1beta1.Rule{
+						APIGroups:   []string{schedulingv1beta1.SchemeGroupVersion.Group},
+						APIVersions: []string{schedulingv1beta1.SchemeGroupVersion.Version},
+						Resources:   []string{"elasticresourcequotas"},
+					},
+				},
+			},
+		}},
+	},
+}
+
+var config = &router.AdmissionServiceConfig{}
+
+// AdmitElasticresourcequotas is to admit elasticresourcequotas and return response.
+func AdmitElasticresourcequotas(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
+	klog.V(3).Infof("Admitting %s elasticresourcequota %s.", ar.Request.Operation, ar.Request.Name)
+
+	elasticResourceQuota, err := schema.DecodeElasticResourceQuota(ar.Request.Object, ar.Request.Resource)
+	if err != nil {
+		return util.ToAdmissionResponse(err)
+	}
+
+	switch ar.Request.Operation {
+	case v1beta1.Create:
+		err = validateEQuota(elasticResourceQuota)
+	case v1beta1.Update:
+		err = validateEQuotaUpdating(elasticResourceQuota)
+	case v1beta1.Delete:
+		err = validateEQuotaDeleting(ar.Request)
+	default:
+		return util.ToAdmissionResponse(fmt.Errorf("invalid operation `%s`, "+
+			"expect operation to be `CREATE`, `UPDATE` or `DELETE`", ar.Request.Operation))
+	}
+
+	if err != nil {
+		return &v1beta1.AdmissionResponse{
+			Allowed: false,
+			Result:  &metav1.Status{Message: err.Error()},
+		}
+	}
+
+	return &v1beta1.AdmissionResponse{
+		Allowed: true,
+	}
+}
+
+// validateEQuota check spec.Min, spec.Max, spec.Namespace and meta.labels
+func validateEQuota(elasticResourceQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("validate elasticresourcequota[%s]", elasticResourceQuota.Name)
+	errs := field.ErrorList{}
+	resourcePath := field.NewPath("requestBody")
+
+	// public validation
+	if err := validateTypeOfEQuota(elasticResourceQuota, resourcePath.Child("metadata").Child("labels")); err != nil {
+		errs = append(errs, err)
+		return err
+	}
+	errs = append(errs, validateSpecNSOfEQuota(elasticResourceQuota, resourcePath.Child("spec").Child("namespace"))...)
+
+	// different validation by quotaType
+	switch elasticResourceQuota.Labels[schedulingv1beta1.QuotaTypeKey] {
+	case schedulingv1beta1.QuotaTypePhysical:
+		errs = append(errs, validateParentOfPhysicalEQuota(elasticResourceQuota, resourcePath.Child("metadata").Child("labels"))...)
+		errs = append(errs, validateResourceOfPhysicalEQuota(nil, elasticResourceQuota, resourcePath)...)
+	default:
+		errs = append(errs, validateParentOfLogicalEQuota(elasticResourceQuota, resourcePath.Child("metadata").Child("labels"))...)
+		errs = append(errs, validateResourceOfLogicalEQuota(nil, elasticResourceQuota, resourcePath)...)
+	}
+
+	if len(errs) > 0 {
+		return errs.ToAggregate()
+	}
+
+	return nil
+}
+
+// validateEQuotaUpdating check spec.Min, spec.Max, spec.Namespace and meta.labels if changed
+func validateEQuotaUpdating(eQuota *schedulingv1beta1.ElasticResourceQuota) error {
+	klog.V(4).Infof("validate updating elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+	resourcePath := field.NewPath("requestBody")
+
+	oldEQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), eQuota.Name, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("validateEQuotaUpdating occurred error, get elasticresourcequota failed, err=[%+v]", err)
+		errs = append(errs, convertToFieldError(err, resourcePath.Child("metadata").Child("name"), eQuota.Name))
+		return errs.ToAggregate()
+	}
+
+	// validate types of EQuota
+	if err := validateTypeOfEQuota(eQuota, resourcePath.Child("metadata").Child("labels")); err != nil {
+		errs = append(errs, err)
+		return errs.ToAggregate()
+	}
+	// validate namespace
+	if eQuota.Spec.Namespace != oldEQuota.Spec.Namespace {
+		errs = append(errs, validateSpecNSOfEQuota(eQuota, resourcePath.Child("spec").Child("namespace"))...)
+	}
+
+	oldParentKey := oldEQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	newParentKey := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	isParentUpdate := oldParentKey != newParentKey
+
+	switch eQuota.Labels[schedulingv1beta1.QuotaTypeKey] {
+	case schedulingv1beta1.QuotaTypePhysical:
+		if isParentUpdate {
+			errs = append(errs, validateParentOfPhysicalEQuota(eQuota, resourcePath.Child("metadata").Child("labels"))...)
+		}
+		errs = append(errs, validateResourceOfPhysicalEQuota(oldEQuota, eQuota, resourcePath)...)
+	default:
+		if isParentUpdate {
+			errs = append(errs, validateParentOfLogicalEQuota(eQuota, resourcePath.Child("metadata").Child("labels"))...)
+		}
+		errs = append(errs, validateResourceOfLogicalEQuota(oldEQuota, eQuota, resourcePath)...)
+	}
+
+	if len(errs) > 0 {
+		klog.Errorf("validateEQuota errs=[%+v]", errs)
+		return errs.ToAggregate()
+	}
+
+	return nil
+}
+
+// validateEQuotaDeleting deny delete eQuota when bound queue or spec.Isleaf = false
+func validateEQuotaDeleting(req *v1beta1.AdmissionRequest) error {
+	eQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), req.Name, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("validateEQuotaDeleting occurred error, get elasticresourcequota failed, err=[%+v]", err)
+		return err
+	}
+	if !eQuota.Status.IsLeaf {
+		return fmt.Errorf("cannot delete non-leaf elastic resource quota")
+	}
+	// get queue, if exist then deny
+	if len(eQuota.Status.QueueName) == 0 {
+		return nil
+	}
+	if queue, err := config.VolcanoClient.SchedulingV1beta1().Queues().Get(context.TODO(),
+		eQuota.Status.QueueName, metav1.GetOptions{}); err != nil {
+		klog.Errorf("validateEQuotaDeleting occurred error, get queue failed, err=[%+v]", err)
+		return err
+	} else {
+		return fmt.Errorf("cannot delete elastic resource quota[%s] when bound queue[%s]", req.Name, queue.Name)
+	}
+}
+
+// validateSpecNSOfEQuota validate spec.namespace of elasticresourcequota
+func validateSpecNSOfEQuota(eQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	ns := eQuota.Spec.Namespace
+	if _, err := config.KubeClient.CoreV1().Namespaces().Get(context.TODO(), ns, metav1.GetOptions{}); err != nil {
+		klog.Errorf("validateSpecNSOfEQuota occurred error, get namespace failed, err=[%+v]", err)
+		return append(errs, field.Invalid(fldPath, ns, err.Error()))
+	}
+	return errs
+}
+
+// validateTypeOfEQuota label[QuotaTypeKey] is required
+func validateTypeOfEQuota(eQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) *field.Error {
+	quotaType := eQuota.Labels[schedulingv1beta1.QuotaTypeKey]
+	if quotaType != schedulingv1beta1.QuotaTypeLogical && quotaType != schedulingv1beta1.QuotaTypePhysical {
+		return field.Invalid(fldPath, eQuota.Name, fmt.Sprintf("label[%s] is required", schedulingv1beta1.QuotaTypeKey))
+	}
+	return nil
+}
diff --git a/pkg/webhooks/admission/elasticresourcequotas/validate/validate_logical_type.go b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_logical_type.go
new file mode 100644
index 0000000..56bf2c8
--- /dev/null
+++ b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_logical_type.go
@@ -0,0 +1,99 @@
+package validate
+
+import (
+	"context"
+	"fmt"
+
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+)
+
+// validateParentOfLogicalEQuota validate parent of logical eQuota
+func validateParentOfLogicalEQuota(eQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate parent of logical elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+	parentQuotaName, exist := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	if exist {
+		if _, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentQuotaName, metav1.GetOptions{}); err != nil {
+			klog.Errorf("validateParentOfEQuota occurred error, get elasticresourcequota failed, err=[%+v]", err)
+			return append(errs, convertToFieldError(err, fldPath, parentQuotaName))
+		}
+		return errs
+	}
+
+	labelSelector := metav1.LabelSelector{}
+	labelSelector.MatchExpressions = append(labelSelector.MatchExpressions, metav1.LabelSelectorRequirement{
+		Key:      schedulingv1beta1.ElasticQuotaParentKey,
+		Operator: metav1.LabelSelectorOpDoesNotExist,
+	})
+	eQuotas, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().List(context.TODO(),
+		metav1.ListOptions{LabelSelector: metav1.FormatLabelSelector(&labelSelector)})
+	if err != nil {
+		klog.Errorf("validateParentOfEQuota occurred error, get elasticresourcequota failed, err=[%+v]", err)
+		return append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+	}
+
+	if len(eQuotas.Items) == 0 || len(eQuotas.Items) == 1 && eQuotas.Items[0].Name == eQuota.Name {
+		klog.V(3).Infof("add first eQuota or update root eQuota, return permit")
+		return errs
+	}
+
+	return append(errs, field.Invalid(fldPath, eQuota.Name,
+		fmt.Sprintf("[%s] is not root elasticeresourcequota, must specify its parent in Labels", eQuota.Name)))
+}
+
+// validateResourceOfLogicalEQuota validate resource for logical type elasticresourcequota
+func validateResourceOfLogicalEQuota(oldEQuota, eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate resource for logical type elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	// 1. validate between min with max
+	errs = append(errs, validateBetweenMinMax(eQuota, rootFldPath)...)
+
+	// 2. validate min or max
+	_, exist := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	if !exist {
+		return append(errs, validateRootResourceOfLogical(eQuota, rootFldPath)...)
+	} else {
+		return append(errs, validateNonRootResource(oldEQuota, eQuota, rootFldPath)...)
+	}
+
+	return errs
+}
+
+// validateRootResourceOfLogical validate resource of root logical eQuota by compare node with spec.Min
+// spec.Min must less equal than sum resource of node
+func validateRootResourceOfLogical(eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate root elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	fldPath := rootFldPath.Child("spec").Child("min")
+	minResource := common.NewResource(eQuota.Spec.Min)
+
+	// get list of public-type nodes
+	nodeList, err := common.ListPublicNodes(config.KubeClient)
+	if err != nil && errors.IsNotFound(err) {
+		klog.Errorf("failed to list publicNodes, err=[%v]", err)
+		errs = append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+		return errs
+	}
+	// calculate the sum of nodes
+	nodeSumResource := common.EmptyResource()
+	for _, n := range nodeList {
+		nodeSumResource.Add(common.NewResource(n.Status.Allocatable))
+	}
+	// validate nodeResource and elasticresourcequota
+	if !minResource.LessEqual(nodeSumResource) {
+		klog.V(4).Infof("minResource[%s] is more than nodeSumResource[%s] elasticresourcequota[%s]",
+			minResource.String(), nodeSumResource.String(), eQuota.Name)
+		errs = append(errs, field.Invalid(fldPath, minResource.String(), fmt.Sprintf(
+			"spec.min must less equal than the sum resources of nodes[%s]", nodeSumResource.String())))
+		return errs
+	}
+	return errs
+}
diff --git a/pkg/webhooks/admission/elasticresourcequotas/validate/validate_physical_type.go b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_physical_type.go
new file mode 100644
index 0000000..82bf10a
--- /dev/null
+++ b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_physical_type.go
@@ -0,0 +1,71 @@
+package validate
+
+import (
+	"context"
+	"fmt"
+
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+)
+
+// validateParentOfPhysicalEQuota validate parent of physical eQuota
+func validateParentOfPhysicalEQuota(eQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate parent of logical elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+	parentQuotaName, exist := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	if exist {
+		if _, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentQuotaName, metav1.GetOptions{}); err != nil {
+			klog.Errorf("validateParentOfEQuota occurred error, get elasticresourcequota failed, err=[%+v]", err)
+			return append(errs, convertToFieldError(err, fldPath, parentQuotaName))
+		}
+	}
+	return errs
+}
+
+// validateResourceOfPhysicalEQuota validate resource for physical type elasticresourcequota
+func validateResourceOfPhysicalEQuota(oldEQuota, eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate resource for physical type elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	// 1. validate between min with max
+	errs = append(errs, validateBetweenMinMax(eQuota, rootFldPath)...)
+
+	// 2. validate min or max
+	_, exist := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	if !exist {
+		return append(errs, validateRootResourceOfPhysical(eQuota, rootFldPath)...)
+	} else {
+		return append(errs, validateNonRootResource(oldEQuota, eQuota, rootFldPath)...)
+	}
+
+	return errs
+}
+
+// validateRootResourceOfPhysical validate resource of root physical eQuota by compare node with spec.Max
+func validateRootResourceOfPhysical(eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate root elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	fldPath := rootFldPath.Child("spec").Child("max")
+	maxResource := common.NewResource(eQuota.Spec.Max)
+	// get sum resource of node
+	managedNodeResource, err := common.CalcManagedNodesResources(config.KubeClient, eQuota.Name)
+	if err != nil {
+		klog.Errorf("validateRootResourceOfPhysical failed, CalcManagedNodesResources occur a err[%+v]", err)
+		errs = append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+		return errs
+	}
+	// validate resource between managedNodes and max
+	if !maxResource.LessEqual(managedNodeResource) {
+		klog.V(4).Infof("maxResource[%s] is more than managedNodeResource[%s] elasticresourcequota[%s]",
+			maxResource.String(), managedNodeResource.String(), eQuota.Name)
+		errs = append(errs, field.Invalid(fldPath, maxResource.String(), fmt.Sprintf(
+			"spec.max must less than the sum resources of nodes[%s]", managedNodeResource.String())))
+		return errs
+	}
+	return errs
+}
diff --git a/pkg/webhooks/admission/elasticresourcequotas/validate/validate_utils.go b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_utils.go
new file mode 100644
index 0000000..ca39568
--- /dev/null
+++ b/pkg/webhooks/admission/elasticresourcequotas/validate/validate_utils.go
@@ -0,0 +1,169 @@
+package validate
+
+import (
+	"context"
+	"fmt"
+	"reflect"
+
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+)
+
+func validateBetweenMinMax(eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validateBetweenMinMax in elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	if eQuota.Spec.Min == nil {
+		errs = append(errs, field.Required(rootFldPath.Child("spec").Child("min"), fmt.Sprint("spec.Min must be filled")))
+		return errs
+	}
+	if eQuota.Spec.Max == nil {
+		errs = append(errs, field.Required(rootFldPath.Child("spec").Child("max"), fmt.Sprint("spec.Max must be filled")))
+		return errs
+	}
+
+	minResource := common.NewResource(eQuota.Spec.Min)
+	maxResource := common.NewResource(eQuota.Spec.Max)
+
+	// validate min<max
+	if !minResource.LessEqual(maxResource) {
+		klog.V(4).Infof("minResource is more than maxResource in elasticresourcequota[%s]", eQuota.Name)
+		return append(errs, field.Invalid(rootFldPath, minResource.String(), fmt.Sprint("spec.Min must be less than spec.Max")))
+	}
+	return errs
+}
+
+func validateNonRootResource(oldEQuota, eQuota *schedulingv1beta1.ElasticResourceQuota, rootFldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validate non-root elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	parentName := eQuota.Labels[schedulingv1beta1.ElasticQuotaParentKey]
+	maxFieldPath := rootFldPath.Child("spec").Child("max")
+	minFieldPath := rootFldPath.Child("spec").Child("min")
+	maxResource := common.NewResource(eQuota.Spec.Max)
+	minResource := common.NewResource(eQuota.Spec.Min)
+	isCreating := oldEQuota == nil
+	isMinUpdate := oldEQuota != nil && !reflect.DeepEqual(oldEQuota.Spec.Min, eQuota.Spec.Min)
+	isMaxUpdate := oldEQuota != nil && !reflect.DeepEqual(oldEQuota.Spec.Max, eQuota.Spec.Max)
+
+	// validate min or max with parent
+	parentEQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), parentName, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("validateResourceOfEQuota occurred error, get elasticresourcequota failed, err=[%+v]", err)
+		errs = append(errs, convertToFieldError(err, rootFldPath.Child("metadata").Child("labels"), parentName))
+		return errs
+	}
+	// 2.1 validate min. Min Resource is about to current/brothers/parent eQuota
+	if isCreating || isMinUpdate {
+		// min Resource is bigger than before, it's necessary to check parent minResource of eQuota
+		errs = append(errs, validateSpecMinOfEQuota(minResource, eQuota, parentEQuota, minFieldPath)...)
+	}
+	if isMinUpdate {
+		oldMinResource := common.NewResource(oldEQuota.Spec.Min)
+		if !oldMinResource.LessEqual(minResource) {
+			klog.V(4).Infof("spec.Min has been updated, oldMinResource=[%s], minResource=[%s]", oldMinResource.String(), minResource.String())
+			// min Resource is smaller than before, it's necessary to check children minResource of eQuota
+			return append(errs, validateSpecMinOfChildren(minResource, eQuota, minFieldPath)...)
+		}
+	}
+	// 2.2 validate max. Max Resource should be smaller than parentEQuota's
+	if isCreating || isMaxUpdate {
+		klog.V(6).Infof("validate max. Max Resource should be smaller than parentEQuota's")
+		return append(errs, validateSpecMaxOfEQuota(maxResource, parentEQuota.Spec.Max, maxFieldPath)...)
+	}
+	return errs
+}
+
+// validateSpecMinOfChildren validate the sum of minResource in eQuota's children
+func validateSpecMinOfChildren(minResource *common.Resource, eQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validateSpecMinOfChildren[%s] ", eQuota)
+	errs := field.ErrorList{}
+	// leaf eQuota has not children, passed
+	if eQuota.Status.IsLeaf {
+		return errs
+	}
+	// validate children
+	children, err := common.GetEQuotaChildren(config.VolcanoClient, eQuota.Name)
+	if err != nil {
+		klog.Errorf("validateSpecMinOfChildren occurred error, GetEQuotaChildren failed, err=[%+v]", err)
+		return append(errs, field.Invalid(fldPath, minResource.String(), err.Error()))
+	}
+	childrenMinResource := common.EmptyResource()
+	for _, child := range children {
+		childrenMinResource.Add(common.NewResource(child.Spec.Min))
+	}
+	// validate children
+	klog.V(3).Infof("compare sumResource[%s] of children with minResource[%s] of elasticresourcequota[%s]",
+		childrenMinResource.String(), minResource.String(), eQuota.Name)
+	if !childrenMinResource.LessEqualStrict(minResource) {
+		return append(errs, field.Invalid(fldPath, minResource.String(), "spec.Min of children elasticresourcequota"+
+			" required more than current, please reduce spec.min of children at first"))
+	}
+	return errs
+}
+
+// validateSpecMinOfEQuota is sub function of validateEQuota
+func validateSpecMinOfEQuota(minResource *common.Resource, eQuota, parentEQuota *schedulingv1beta1.ElasticResourceQuota, fldPath *field.Path) field.ErrorList {
+	klog.V(4).Infof("validateSpecMinOfEQuota elasticresourcequota[%s]", eQuota.Name)
+	errs := field.ErrorList{}
+
+	if minResource.IsLessThanMinimum() {
+		return append(errs, field.Invalid(fldPath, minResource.String(), fmt.Sprint("any resource must be positive")))
+	}
+	var brotherMinResource = common.EmptyResource()
+	if children, err := common.GetEQuotaChildren(config.VolcanoClient, parentEQuota.Name); err != nil {
+		klog.Errorf("GetEQuotaChildren occurred error, get elasticresourcequotas failed, err=[%+v]", err)
+		errs = append(errs, field.Invalid(fldPath, minResource.String(), err.Error()))
+		return errs
+	} else {
+		for _, child := range children {
+			if child.Name == eQuota.Name {
+				// snapshot value of eQuota is outdated
+				continue
+			}
+			brotherMinResource.Add(common.NewResource(child.Spec.Min))
+		}
+	}
+	// add newest value of current eQuota
+	brotherMinResource.Add(minResource)
+	// compare resources between parent and brother
+	parentResource := common.NewResource(parentEQuota.Spec.Min)
+	klog.V(3).Infof("parentResource[%s] versus brothers[%s]", parentResource.String(), brotherMinResource.String())
+	if !brotherMinResource.LessEqual(parentResource) {
+		return append(errs, field.Invalid(fldPath, minResource.String(), fmt.Sprintf(
+			"spec.Min exceeds available resource of parent elasticresourcequota[%s]", parentEQuota.Name)))
+	}
+	return errs
+}
+
+// validateSpecMaxOfEQuota is sub function of validateEQuota
+func validateSpecMaxOfEQuota(maxResource *common.Resource, parentMax v1.ResourceList, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	klog.V(4).Infof("validate spec.max of elasticresourcequota")
+	// max can not be zero
+	if maxResource.IsLessThanMinimum() {
+		return append(errs, field.Invalid(fldPath, maxResource.String(), fmt.Sprint("any resource must be positive")))
+	}
+	parentMaxResource := common.NewResource(parentMax)
+	// current max should be less than parent's max
+	if !maxResource.LessEqual(parentMaxResource) {
+		errs = append(errs, field.Invalid(fldPath, maxResource.String(), fmt.Sprint("spec.Max must be less than parent's")))
+		return errs
+	}
+	return errs
+}
+
+// convertToFieldError convert error of getting elasticresourcequota to field error
+func convertToFieldError(err error, fldPath *field.Path, eQuotaName string) *field.Error {
+	if errors.IsNotFound(err) {
+		return field.Invalid(fldPath, eQuotaName, err.Error())
+	} else {
+		return field.InternalError(fldPath, err)
+	}
+}
diff --git a/pkg/webhooks/admission/nodes/validate/validate_node.go b/pkg/webhooks/admission/nodes/validate/validate_node.go
new file mode 100644
index 0000000..b433e10
--- /dev/null
+++ b/pkg/webhooks/admission/nodes/validate/validate_node.go
@@ -0,0 +1,326 @@
+package validate
+
+import (
+	"context"
+	"fmt"
+
+	"k8s.io/api/admission/v1beta1"
+	whv1beta1 "k8s.io/api/admissionregistration/v1beta1"
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+	"volcano.sh/volcano/pkg/webhooks/router"
+	"volcano.sh/volcano/pkg/webhooks/schema"
+	"volcano.sh/volcano/pkg/webhooks/util"
+)
+
+var (
+	RemoveNodeFromRPErrorInfo = `node[%s] is managed by resource quotas, please remove node from resource quotas first.
+ Please set node with label "`+ schedulingv1beta1.QuotaLabelKey + `=`+ schedulingv1beta1.NodeScaleDown + `" . `
+)
+
+func init() {
+	router.RegisterAdmission(service)
+}
+
+var service = &router.AdmissionService{
+	Path: "/nodes/validate",
+	Func: AdmitNodes,
+
+	Config: config,
+
+	ValidatingConfig: &whv1beta1.ValidatingWebhookConfiguration{
+		Webhooks: []whv1beta1.ValidatingWebhook{{
+			Name: "validatenode.volcano.sh",
+			Rules: []whv1beta1.RuleWithOperations{
+				{
+					Operations: []whv1beta1.OperationType{whv1beta1.Update, whv1beta1.Delete},
+					Rule: whv1beta1.Rule{
+						APIGroups:   []string{""},
+						APIVersions: []string{"v1"},
+						Resources:   []string{"nodes"},
+					},
+				},
+			},
+		}},
+	},
+}
+
+var config = &router.AdmissionServiceConfig{}
+
+// AdmitNodes is to admit resourcequotas and return response.
+func AdmitNodes(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
+	klog.V(3).Infof("Admitting %s node %s.", ar.Request.Operation, ar.Request.Name)
+
+	node, err := schema.DecodeNode(ar.Request.Object, ar.Request.Resource)
+	if err != nil {
+		return util.ToAdmissionResponse(err)
+	}
+
+	switch ar.Request.Operation {
+	case v1beta1.Update:
+		err = validateNodeUpdating(node)
+	case v1beta1.Delete:
+		err = validateNodeDeleting(ar.Request.Name)
+	default:
+		return util.ToAdmissionResponse(fmt.Errorf("invalid operation `%s`, "+
+			"expect operation to be `UPDATE` or `DELETE`", ar.Request.Operation))
+	}
+
+	if err != nil {
+		return &v1beta1.AdmissionResponse{
+			Allowed: false,
+			Result:  &metav1.Status{Message: err.Error()},
+		}
+	}
+
+	return &v1beta1.AdmissionResponse{
+		Allowed: true,
+	}
+}
+
+func validateNodeUpdating(node *v1.Node) error {
+	errs := field.ErrorList{}
+	resourcePath := field.NewPath("requestBody")
+
+	// validating node label updating
+	oldNode, err := config.KubeClient.CoreV1().Nodes().Get(context.TODO(), node.Name, metav1.GetOptions{})
+	if err != nil {
+		return err
+	}
+
+	errs = append(errs, validateLabelOfNode(node, oldNode, resourcePath.Child("metadata").Child("labels"))...)
+
+	if len(errs) > 0 {
+		return errs.ToAggregate()
+	}
+
+	return nil
+}
+
+// validateLabelOfNode validate the update action of node's label
+func validateLabelOfNode(newNode, oldNode *v1.Node, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	if newNode == nil || oldNode == nil {
+		return errs
+	}
+	newNodeLabels := newNode.Labels
+	oldNodeLabels := oldNode.Labels
+	if newNodeLabels == nil {
+		newNodeLabels = map[string]string{}
+	}
+	if oldNodeLabels == nil {
+		oldNodeLabels = map[string]string{}
+	}
+
+	_, oldRPNMLabelExist := oldNodeLabels[schedulingv1beta1.QuotaManagedNodeKey]
+
+	oldQuotaKey, _ := oldNodeLabels[schedulingv1beta1.QuotaLabelKey]
+	oldNS, oldQuotaName, err := common.SplitMetaNamespaceKey(oldQuotaKey)
+	if err != nil {
+		errs = append(errs, field.Invalid(fldPath, oldNodeLabels, fmt.Sprintf("%s", err)))
+		return errs
+	}
+
+	_, newRPNMLabelExist := newNodeLabels[schedulingv1beta1.QuotaManagedNodeKey]
+	newQuotaKey, isLabelExist := newNodeLabels[schedulingv1beta1.QuotaLabelKey]
+	if isLabelExist && len(newQuotaKey) == 0 {
+		errs = append(errs, field.Invalid(fldPath, oldNodeLabels, fmt.Sprintf("node[%s] label[%s] format error, length cannot be zero",
+			newNode.Name, schedulingv1beta1.QuotaLabelKey)))
+		return errs
+	}
+	newNS, newRPLabel, err := common.SplitMetaNamespaceKey(newQuotaKey)
+	if err != nil {
+		errs = append(errs, field.Invalid(fldPath, newNodeLabels, fmt.Sprintf("%s", err)))
+		return errs
+	}
+
+	klog.V(4).Infof("node old rp label: [%s/%s], new rp label: [%s/%s]", oldNS, oldQuotaName, newNS, newRPLabel)
+
+	if schedulingv1beta1.GlobalNodeManageEnabled {
+		// Check whether resourcequota label is changed or not
+		if err := validateQuotasLabel(newNode, oldNS, oldQuotaName, newNS, newRPLabel); err != nil {
+			errs = append(errs, field.Invalid(fldPath, newNodeLabels, fmt.Sprintf("%s", err)))
+		}
+		return errs
+	}
+    // cluster is not manage all node, label not exist means not managed by volcano
+	if oldRPNMLabelExist && newRPNMLabelExist {
+		// Node is managed by ResourceQuota
+		// Check whether resourcequota label is changed or not
+		if err := validateQuotasLabel(newNode, oldNS, oldQuotaName, newNS, newRPLabel); err != nil {
+			errs = append(errs, field.Invalid(fldPath, newNodeLabels,
+				fmt.Sprintf("%s", err)))
+		}
+	} else if oldRPNMLabelExist && !newRPNMLabelExist {
+		// Node will not be managed by a resourcequota
+		// Check remove QuotaManagedNodeKey
+		if oldQuotaName != schedulingv1beta1.NodeScaleDown {
+			errs = append(errs, field.Invalid(fldPath, newNodeLabels, fmt.Sprintf(RemoveNodeFromRPErrorInfo, newNode.Name)))
+		}
+	} else if !oldRPNMLabelExist && newRPNMLabelExist {
+		// Node will be managed by quotas
+		// Check whether resource quotas is enough or not
+		if err := validateQuotasWhenScaleUp(newNode, newNS, newRPLabel); err != nil {
+			errs = append(errs, field.Invalid(fldPath, newNodeLabels,
+				fmt.Sprintf("%s", err)))
+		}
+	}
+	return errs
+}
+
+// validateQuotasLabel validate the changes of binding elasticresourcequota by changing label
+// elasticresourcequota label changed between {"scale-down", namespace x, namespace y}
+func validateQuotasLabel(node *v1.Node, oldNamespace, oldQuotaLabel, newNamespace, newQuotaLabel string) error {
+	// Label is not changed
+	if oldQuotaLabel == newQuotaLabel && oldNamespace == newNamespace {
+		return nil
+	}
+
+	// node changes between physical and scale-down or
+	// node from scale-down to namespace[newQuotaLabel]
+	if oldQuotaLabel == schedulingv1beta1.NodeScaleDown && newQuotaLabel != schedulingv1beta1.NodeScaleDown {
+		// node from scale-down to namespace[oldQuotaLabel]
+		// Check whether resource quotas is enough or not
+		if err := validateQuotasWhenScaleUp(node, newNamespace, newQuotaLabel); err != nil {
+			return err
+		}
+	} else if oldQuotaLabel != schedulingv1beta1.NodeScaleDown && newQuotaLabel == schedulingv1beta1.NodeScaleDown {
+		// node from namespace[oldQuotaLabel] to scale-down
+		// Check scale down resource quotas
+		if err := validateQuotasWhenScaleDown(node, oldNamespace, oldQuotaLabel); err != nil {
+			return err
+		}
+	} else if oldQuotaLabel != schedulingv1beta1.NodeScaleDown && newQuotaLabel != schedulingv1beta1.NodeScaleDown {
+		// node from namespace[oldQuotaLabel] to namespace[newQuotaLabel]
+		// Check resourcequota label updating
+		return fmt.Errorf("node belongs to resourcequota [%s], must be scale down first, then can change label", oldQuotaLabel)
+	}
+	return nil
+}
+
+// validateQuotasWhenScaleUp validate the quotas' resource when node will be scheduled
+func validateQuotasWhenScaleUp(node *v1.Node, namespace string, quotaLabel string) error {
+	if len(quotaLabel) ==0 {
+		// scale-down node changes to shared type for logical resource quotas
+		klog.V(4).Infof("node[%s] will be scheduled", node.Name)
+		return nil
+	}
+	// validate quotaLabel
+	eQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), quotaLabel, metav1.GetOptions{})
+	if err != nil {
+		klog.V(4).Infof("ElasticResourceQuotas[%s] not found, err=[%v]", quotaLabel, err)
+		return err
+	}
+	// validate quotaType
+	_, exist := eQuota.Labels[schedulingv1beta1.QuotaTypeKey]
+	if !exist {
+		return fmt.Errorf("quota[%s] labels[%s] doesn't exist, please patch it first", namespace, schedulingv1beta1.QuotaTypeKey)
+	}
+	return nil
+}
+
+// validateQuotasWhenScaleDown validate resource when node's label from namespace/null to scale-down.
+func validateQuotasWhenScaleDown(node *v1.Node, namespace string, quotaLabel string) error {
+	if len(quotaLabel) == 0 {
+		return validatePublicTypeNode(node)
+	} else {
+		return validatePrivateTypeNode(node, namespace, quotaLabel)
+	}
+}
+
+// validatePublicTypeNode check resources when node changes from public mode to scale-down
+func validatePublicTypeNode(node *v1.Node) error {
+	// get total used-resource of logic quotas
+	eQuotaList, err := common.ListLogicLeafQuotas(config.VolcanoClient)
+	if err != nil {
+		klog.Errorf("failed to list logic resourcequotas, err=[%v]", err)
+		return err
+	}
+	usedTotalResource := common.EmptyResource()
+	for _, quota := range eQuotaList {
+		usedTotalResource.Add(common.NewResource(quota.Status.Used))
+	}
+
+	// get list of public-type nodes
+	nodeList, err := common.ListPublicNodes(config.KubeClient)
+	if err != nil {
+		klog.Errorf("failed to list publicNodes, err=[%v]", err)
+		return err
+	}
+	// calculate if it is enough when node be free
+	nodeSumResource := common.EmptyResource()
+	for _, n := range nodeList {
+		if n.Name != node.Name {
+			nodeSumResource.Add(common.NewResource(n.Status.Allocatable))
+		}
+	}
+
+	klog.V(4).Infof("sum of nodes is [%s] versus quotas' is [%s]", nodeSumResource.String(), usedTotalResource.String())
+	if !usedTotalResource.LessEqualStrict(nodeSumResource) {
+		return fmt.Errorf("the capability of resource is not enough when node[%s] scale down," +
+			"sum of nodes is [%s] versus sum of quotas is [%s]", node.Name, nodeSumResource.String(), usedTotalResource.String())
+	}
+	// check all leaf logical equota.spec.min <= nodeSumResource
+	for _, eQuota := range eQuotaList {
+		minResource := common.NewResource(eQuota.Spec.Min)
+		if !minResource.LessEqual(nodeSumResource) {
+			return fmt.Errorf("spec.Min[%s] must less than nodes' avaliable resource[%s], please reduce the spec.Min in " +
+				"elasticresourcequota[%s] before node[%s] scale down", minResource.String(), nodeSumResource.String(), eQuota.Name, node.Name)
+		}
+	}
+	return nil
+}
+
+// validatePrivateTypeNode check resources when node changes from private mode to scale-down
+func validatePrivateTypeNode(node *v1.Node, namespace string, quotaLabel string) error {
+	eQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), quotaLabel, metav1.GetOptions{})
+	if err != nil {
+		if errors.IsNotFound(err) {
+			return nil
+		}
+		klog.Errorf("validatePrivateNodeInEQuota get elasticresourcequota failed, err=[%+v]", err)
+		return err
+	}
+	// validate: maxResource - nodeResource >= usedResource
+	usedResource := common.NewResource(eQuota.Status.Used)
+	maxResource := common.NewResource(eQuota.Spec.Max)
+	nodeResource := common.NewResource(node.Status.Allocatable)
+	usedResource = usedResource.Add(nodeResource)
+	if !usedResource.LessEqualStrict(maxResource) {
+		return fmt.Errorf("the capability of resource is not enough when node[%s] scale down, "+
+			"please wait the reduction of using resource in quotas[%s]", node.Name, quotaLabel)
+	}
+	// validate: maxResource - nodeResource >= minResource
+	minResource := common.NewResource(eQuota.Spec.Min)
+	minResource = minResource.Add(nodeResource)
+	if !minResource.LessEqual(maxResource) {
+		return fmt.Errorf("spec.Min must less than spec.Max, please reduce the spec.Min in " +
+			"elasticresourcequota[%s] before node[%s] scale down", quotaLabel, node.Name)
+	}
+	return nil
+}
+
+func validateNodeDeleting(nodeName string) error {
+	oldNode, err := config.KubeClient.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{})
+	if err != nil {
+		return err
+	}
+	nodeLabels := oldNode.Labels
+	if nodeLabels == nil {
+		return nil
+	}
+	_, RPNMLabelExist := nodeLabels[schedulingv1beta1.QuotaManagedNodeKey]
+	RPLabel, _ := nodeLabels[schedulingv1beta1.QuotaLabelKey]
+
+	if (schedulingv1beta1.GlobalNodeManageEnabled || (!schedulingv1beta1.GlobalNodeManageEnabled && RPNMLabelExist)) &&
+		RPLabel != schedulingv1beta1.NodeScaleDown {
+		return fmt.Errorf(RemoveNodeFromRPErrorInfo, nodeName)
+	}
+	return nil
+}
\ No newline at end of file
diff --git a/pkg/webhooks/admission/pods/admit_pod.go b/pkg/webhooks/admission/pods/admit_pod.go
index cea53f5..adddf80 100644
--- a/pkg/webhooks/admission/pods/admit_pod.go
+++ b/pkg/webhooks/admission/pods/admit_pod.go
@@ -76,6 +76,10 @@ func AdmitPods(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
 	if err != nil {
 		return util.ToAdmissionResponse(err)
 	}
+	if len(pod.Namespace) == 0 {
+		klog.V(3).Infof("repair pod[%v] namespace with AdmissionReview[%v]", pod.Name, ar.Request.Namespace)
+		pod.Namespace = ar.Request.Namespace
+	}
 
 	var msg string
 	reviewResponse := v1beta1.AdmissionResponse{}
diff --git a/pkg/webhooks/admission/pods/mutate/mutate_pod.go b/pkg/webhooks/admission/pods/mutate/mutate_pod.go
new file mode 100644
index 0000000..db9975d
--- /dev/null
+++ b/pkg/webhooks/admission/pods/mutate/mutate_pod.go
@@ -0,0 +1,650 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package mutate
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"strconv"
+	"strings"
+
+	"k8s.io/api/admission/v1beta1"
+	whv1beta1 "k8s.io/api/admissionregistration/v1beta1"
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/klog"
+
+	batch "volcano.sh/apis/pkg/apis/batch/v1alpha1"
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+	"volcano.sh/volcano/pkg/scheduler/api"
+	"volcano.sh/volcano/pkg/webhooks/router"
+	"volcano.sh/volcano/pkg/webhooks/schema"
+	"volcano.sh/volcano/pkg/webhooks/util"
+)
+
+func init() {
+	router.RegisterAdmission(service)
+}
+
+var service = &router.AdmissionService{
+	Path:   "/pods/mutate",
+	Func:   Pods,
+	Config: config,
+
+	MutatingConfig: &whv1beta1.MutatingWebhookConfiguration{
+		Webhooks: []whv1beta1.MutatingWebhook{{
+			Name: "mutatepod.volcano.sh",
+			SideEffects: func() *whv1beta1.SideEffectClass {
+				value := whv1beta1.SideEffectClassNone
+				return &value
+			}(),
+			Rules: []whv1beta1.RuleWithOperations{
+				{
+					Operations: []whv1beta1.OperationType{whv1beta1.Create},
+					Rule: whv1beta1.Rule{
+						APIGroups:   []string{""},
+						APIVersions: []string{"v1"},
+						Resources:   []string{"pods"},
+					},
+				},
+			},
+		}},
+	},
+}
+
+var config = &router.AdmissionServiceConfig{}
+
+type patchOperation struct {
+	Op    string      `json:"op"`
+	Path  string      `json:"path"`
+	Value interface{} `json:"value,omitempty"`
+}
+
+// Pods mutate pods.
+func Pods(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
+	klog.V(3).Infof("Mutating %s pod %s.", ar.Request.Operation, ar.Request.Name)
+
+	pod, err := schema.DecodePod(ar.Request.Object, ar.Request.Resource)
+	if err != nil {
+		return util.ToAdmissionResponse(err)
+	}
+	if len(pod.Namespace) == 0 {
+		klog.V(3).Infof("repair pod namespace with AdmissionReview[%v]", ar.Request.Namespace)
+		pod.Namespace = ar.Request.Namespace
+	}
+
+	var patchBytes []byte
+	switch ar.Request.Operation {
+	case v1beta1.Create:
+		patchBytes, err = createPodPatch(pod)
+	default:
+		return util.ToAdmissionResponse(fmt.Errorf("invalid operation `%s`, "+
+			"expect operation to be `CREATE`", ar.Request.Operation))
+	}
+
+	if err != nil {
+		return &v1beta1.AdmissionResponse{
+			Allowed: false,
+			Result:  &metav1.Status{Message: err.Error()},
+		}
+	}
+
+	pt := v1beta1.PatchTypeJSONPatch
+	return &v1beta1.AdmissionResponse{
+		Allowed:   true,
+		Patch:     patchBytes,
+		PatchType: &pt,
+	}
+}
+
+func createPodPatch(pod *v1.Pod) ([]byte, error) {
+	var patch []patchOperation
+	// create affinity patch
+	affinityPatch, err := createAffinityPatch(pod)
+	if err != nil {
+		return nil, err
+	}
+	if (affinityPatch != patchOperation{}) {
+		patch = append(patch, affinityPatch)
+	} else {
+		klog.V(3).Infof("pod[%s] affinity patch is null", pod.Name)
+	}
+	// create CGPU patch
+	if config.CgpuOptions.GPUCGroupEnable {
+		cgpuPatch, err := createCGPUPatch(pod)
+		if err != nil {
+			return nil, err
+		}
+		if len(cgpuPatch) != 0 {
+			patch = append(patch, cgpuPatch...)
+		} else {
+			klog.V(3).Infof("pod[%s] CGPU patch is null", pod.Name)
+		}
+	}
+
+	return json.Marshal(patch)
+}
+
+func createAffinityPatch(pod *v1.Pod) (patchOperation, error) {
+	klog.V(3).Infof("create affinity patch for pod %s.", pod.Name)
+	// get queue name from pod labels
+	queueName, exist := pod.Annotations[batch.QueueNameKey]
+	if !exist {
+		// get queue name from podGroup spec.queue
+		pgName, ok := pod.Annotations[schedulingv1beta1.KubeGroupNameAnnotationKey]
+		if !ok {
+			klog.V(3).Infof("failed to get podGroup name from annotation, skip affinity patch")
+			return patchOperation{}, nil
+		}
+		klog.V(3).Infof("podGroup name[%s]", pgName)
+		pg, err := config.VolcanoClient.SchedulingV1beta1().PodGroups(pod.Namespace).Get(context.TODO(), pgName, metav1.GetOptions{})
+		if err != nil {
+			klog.Warningf("failed to get podGroup[%s] from api server, err[%+v], skip affinity patch", pgName, err)
+			return patchOperation{}, nil
+		}
+		queueName = pg.Spec.Queue
+	}
+
+	klog.V(3).Infof("queue name[%s]", queueName)
+	queue, err := config.VolcanoClient.SchedulingV1beta1().Queues().Get(context.TODO(), queueName, metav1.GetOptions{})
+	if err != nil {
+		return patchOperation{}, err
+	}
+	equotaName, exist := queue.Labels[schedulingv1beta1.QueueBindingElasticQuotaKey]
+	if !exist || len(equotaName) == 0 {
+		return patchOperation{}, nil
+	}
+	rootEQuota, ok , err := common.GetEQuotaRoot(config.VolcanoClient, equotaName)
+	if err != nil || !ok {
+		return patchOperation{}, err
+	}
+
+	affinity := GetAffinityPolicy(queue, rootEQuota, pod.Spec.Affinity)
+	return patchOperation{
+		Op:    "add",
+		Path:  "/spec/affinity",
+		Value: &affinity,
+	}, nil
+}
+
+func GetAffinityPolicy(queue *schedulingv1beta1.Queue, rootEQuota *schedulingv1beta1.ElasticResourceQuota, affinity *v1.Affinity) *v1.Affinity {
+	if queue == nil || rootEQuota == nil {
+		return affinity
+	}
+	queueAffinity := getQueueAffinityPolicy(queue, rootEQuota)
+	if affinity == nil {
+		return queueAffinity
+	}
+	// merge node affinity from user and and queue
+	newAffinity := &v1.Affinity{
+		NodeAffinity:    affinity.NodeAffinity,
+		PodAffinity:     affinity.PodAffinity,
+		PodAntiAffinity: affinity.PodAntiAffinity,
+	}
+	if newAffinity.NodeAffinity == nil {
+		newAffinity.NodeAffinity = queueAffinity.NodeAffinity
+	} else {
+		klog.V(3).Infof("merge node affinity from user: [%+v]", affinity.NodeAffinity)
+		newAffinity.NodeAffinity = mergeNodeAffinity(affinity.NodeAffinity, queueAffinity.NodeAffinity)
+	}
+	return newAffinity
+}
+
+// mergeNodeAffinity get the intersection of node affinity
+func mergeNodeAffinity(pod, queue *v1.NodeAffinity) *v1.NodeAffinity {
+	if pod.RequiredDuringSchedulingIgnoredDuringExecution == nil {
+		pod.RequiredDuringSchedulingIgnoredDuringExecution = queue.RequiredDuringSchedulingIgnoredDuringExecution
+		return pod
+	}
+	// merge node selector terms
+	if queue.RequiredDuringSchedulingIgnoredDuringExecution != nil {
+		var nodeSelectorTerms []v1.NodeSelectorTerm
+		for _, term1 := range queue.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms {
+			for _, term2 := range pod.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms {
+				nodeSelectorTerm := v1.NodeSelectorTerm{
+					MatchExpressions: append(term1.MatchExpressions, term2.MatchExpressions...),
+					MatchFields:      append(term1.MatchFields, term2.MatchFields...),
+				}
+				nodeSelectorTerms = append(nodeSelectorTerms, nodeSelectorTerm)
+			}
+		}
+		klog.V(3).Infof("merge node selector terms of pod and queue, [%+v]", nodeSelectorTerms)
+		pod.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms = nodeSelectorTerms
+	}
+	return pod
+}
+
+// getQueueAffinityPolicy generate affinity policy from queue
+func getQueueAffinityPolicy(queue *schedulingv1beta1.Queue, rootEQuota *schedulingv1beta1.ElasticResourceQuota) *v1.Affinity {
+	// Add resourcequota node manager label selector
+	var nodeSelectorRequirement []v1.NodeSelectorRequirement
+	if !schedulingv1beta1.GlobalNodeManageEnabled {
+		nodeSelectorRequirement = append(nodeSelectorRequirement, v1.NodeSelectorRequirement{
+			Key:      schedulingv1beta1.QuotaManagedNodeKey,
+			Operator: v1.NodeSelectorOpExists,
+		})
+	}
+	// Add queue hardware type for affinity, such as v100, t4
+	hardwareTypesStr := queue.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	hardwareTypes := common.StringToSlice(hardwareTypesStr)
+	if len(hardwareTypes) > 0 {
+		nodeSelectorRequirement = append(nodeSelectorRequirement, v1.NodeSelectorRequirement{
+			Key:      schedulingv1beta1.QuotaHardwareTypeLabelKey,
+			Operator: v1.NodeSelectorOpIn,
+			Values:   hardwareTypes,
+		})
+	} else if hardwareTypeStr, ok := rootEQuota.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]; ok {
+		hardwareTypeList := common.StringToSlice(hardwareTypeStr)
+		nodeSelectorRequirement = append(nodeSelectorRequirement, v1.NodeSelectorRequirement{
+			Key:      schedulingv1beta1.QuotaHardwareTypeLabelKey,
+			Operator: v1.NodeSelectorOpNotIn,
+			Values:   hardwareTypeList,
+		})
+	}
+
+	// Add node label selector
+	var nodeSelectorTerms []v1.NodeSelectorTerm
+	rpLabelValue := []string{rootEQuota.Name}
+	switch rootEQuota.Labels[schedulingv1beta1.QuotaTypeKey] {
+	case schedulingv1beta1.QuotaTypeLogical:
+		// select node has no elastic resource quota label
+		nodeSelectorTerms = append(nodeSelectorTerms, v1.NodeSelectorTerm{
+			MatchExpressions: append(nodeSelectorRequirement, v1.NodeSelectorRequirement{
+				Key:      schedulingv1beta1.QuotaLabelKey,
+				Operator: v1.NodeSelectorOpDoesNotExist,
+			}),
+		})
+	case schedulingv1beta1.QuotaTypePhysical:
+		nodeSelectorTerms = append(nodeSelectorTerms, v1.NodeSelectorTerm{
+			MatchExpressions: append(nodeSelectorRequirement, v1.NodeSelectorRequirement{
+				Key:      schedulingv1beta1.QuotaLabelKey,
+				Operator: v1.NodeSelectorOpIn,
+				Values:   rpLabelValue,
+			}),
+		})
+	}
+
+	affinity := &v1.Affinity{
+		NodeAffinity: &v1.NodeAffinity{
+			RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
+				NodeSelectorTerms: nodeSelectorTerms,
+			},
+		}}
+	return affinity
+}
+
+func createCGPUPatch(pod *v1.Pod) ([]patchOperation, error) {
+	klog.V(3).Infof("create CGPU patch for pod %s.", pod.Name)
+	var patch []patchOperation
+	var patchCGPUMemVolumes, patchCGPUCoreVolumes bool
+	customMountPath := getMountLibPath(pod)
+	for id, c := range pod.Spec.Containers {
+		klog.V(3).Infof("patch into container[%s] in pod[%s]", c.Name, pod.Name)
+		// check GPUmem isolation
+		if !isGPUMemoryIsolation(c.Resources.Requests) {
+			continue
+		}
+
+		patchCGPUMemVolumes = true
+		// check GPUcore isolation
+		cgpuCoreIsolation := isGPUCoreIsolation(c.Resources.Requests)
+		if cgpuCoreIsolation {
+			patchCGPUCoreVolumes = true
+		}
+		// patch volumeMounts
+		patchPath := fmt.Sprintf("/spec/containers/%s/volumeMounts", strconv.Itoa(id))
+		if patchRes := patchVolumeMount(&c, patchPath, customMountPath, cgpuCoreIsolation); len(patchRes) != 0 {
+			patch = append(patch, patchRes...)
+		}
+		// patch env
+		patchPath = fmt.Sprintf("/spec/containers/%s/env", strconv.Itoa(id))
+		if patchRes := patchEnv(&c, patchPath, customMountPath, cgpuCoreIsolation); len(patchRes) != 0 {
+			patch = append(patch, patchRes...)
+		}
+		//patch c.Lifecycle.PostStart.Exec.Command
+		patchPath = fmt.Sprintf("/spec/containers/%s/lifecycle", strconv.Itoa(id))
+		if patchRes := patchLifecycle(&c, patchPath, customMountPath); len(patchRes) != 0 {
+			patch = append(patch, patchRes...)
+		}
+	}
+	if patchCGPUMemVolumes {
+		if patchRes := patchVolumes(pod, patchCGPUCoreVolumes); len(patchRes) != 0 {
+			patch = append(patch, patchRes...)
+		}
+	}
+	if patchCGPUCoreVolumes && !pod.Spec.HostPID {
+		klog.V(3).Infof("patch pod.Spec.HostPID=true for pod %s.", pod.Name)
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  "/spec/hostPID",
+			Value: true,
+		})
+	}
+	return patch, nil
+}
+
+func isGPUMemoryIsolation(resourceList v1.ResourceList) bool {
+	cgpuMemExist := false
+	for resType := range resourceList {
+		if strings.HasSuffix(resType.String(), api.CGPUMemory) {
+			cgpuMemExist = true
+		}
+	}
+	return cgpuMemExist
+}
+
+func isGPUCoreIsolation(resourceList v1.ResourceList) bool {
+	cgpuCoreExist := false
+	for resType := range resourceList {
+		if strings.HasSuffix(resType.String(), api.CGPUCore) {
+			cgpuCoreExist = true
+		}
+	}
+	return cgpuCoreExist
+}
+
+func patchVolumeMount(c *v1.Container, patchPath string, libMountPath string, cgpuCoreIsolation bool) []patchOperation {
+	klog.V(3).Infof("create volumeMount for container %s.", c.Name)
+	var patch []patchOperation
+	// construct volumeMounts
+	patchVolumeMounts := map[string]v1.VolumeMount{
+		libMountPath: v1.VolumeMount{
+			Name:      "nvidia-lib-cuda",
+			MountPath: libMountPath,
+		},
+	}
+
+	if len(c.VolumeMounts) == 0 {
+		volumeMounts := make([]v1.VolumeMount, 0)
+		for _, volume := range patchVolumeMounts {
+			volumeMounts = append(volumeMounts, volume)
+		}
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  patchPath,
+			Value: volumeMounts,
+		})
+	} else {
+		existedVolume := make(map[string]bool)
+		for id, volume := range c.VolumeMounts {
+			if patchVolume, exist := patchVolumeMounts[volume.MountPath]; exist {
+				existedVolume[volume.MountPath] = true
+				patch = append(patch, patchOperation{
+					Op:    "replace",
+					Path:  fmt.Sprintf("%s/%s", patchPath, strconv.Itoa(id)),
+					Value: patchVolume,
+				})
+			}
+		}
+		for mountPath, volume := range patchVolumeMounts {
+			if _, found := existedVolume[mountPath]; !found {
+				patch = append(patch, patchOperation{
+					Op:    "add",
+					Path:  fmt.Sprintf("%s/-", patchPath),
+					Value: volume,
+				})
+			}
+		}
+	}
+	return patch
+}
+
+func patchEnv(c *v1.Container, patchPath string, mountPath string, cgpuCoreIsolation bool) []patchOperation {
+	klog.V(3).Infof("patch env in container[%s]", c.Name)
+	var patch []patchOperation
+
+	// caculate gpu memory
+	cGPUMemory, cGPUMemoryUnit, err := getCGPUMemory(c.Resources.Requests)
+	if err != nil {
+		klog.Errorf("failed to get cGPUMemory, err:[%+v]", err)
+	}
+
+	// construct env
+	patchEnvs := map[string]v1.EnvVar{
+		"GPU_CGROUP_ENABLE": v1.EnvVar{
+			Name:  "GPU_CGROUP_ENABLE",
+			Value: "1",
+		},
+		"NVIDIA_VISIBLE_DEVICES": v1.EnvVar{
+			Name:  "NVIDIA_VISIBLE_DEVICES",
+			Value: "all",
+		},
+		// docker pattern:0, km pattern:1
+		"CGROUP_MAP_MODE": v1.EnvVar{
+			Name:  "CGROUP_MAP_MODE",
+			Value: "0",
+		},
+		"LIMIT": v1.EnvVar{
+			Name:  "LIMIT",
+			Value: "1",
+		},
+		"LD_LIBRARY_PATH": v1.EnvVar{
+			Name:  "LD_LIBRARY_PATH",
+			Value: mountPath,
+		},
+		"GPU_MEMORY": v1.EnvVar{
+			Name:  "GPU_MEMORY",
+			Value: fmt.Sprintf("%s%s", strconv.FormatInt(cGPUMemory, 10), cGPUMemoryUnit),
+		},
+	}
+
+	if cgpuCoreIsolation {
+		// caculate gpu memory
+		cGPUCore, err := getCGPUCore(c.Resources.Requests)
+		if err != nil {
+			klog.Errorf("failed to get cGPUMemory, err:[%+v]", err)
+		}
+		patchEnvs["GPU_COMPUTATION"] = v1.EnvVar{
+			Name:  "GPU_COMPUTATION",
+			Value: strconv.FormatInt(cGPUCore, 10),
+		}
+	}
+	if len(c.Env) == 0 {
+		envs := make([]v1.EnvVar, 0)
+		for _, env := range patchEnvs {
+			envs = append(envs, env)
+		}
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  patchPath,
+			Value: envs,
+		})
+	} else {
+		existedEnv := make(map[string]bool)
+		for id, env := range c.Env {
+			if patchEnv, exist := patchEnvs[env.Name]; exist {
+				existedEnv[env.Name] = true
+				patch = append(patch, patchOperation{
+					Op:    "replace",
+					Path:  fmt.Sprintf("%s/%s", patchPath, strconv.Itoa(id)),
+					Value: patchEnv,
+				})
+			}
+		}
+		for name, env := range patchEnvs {
+			if _, found := existedEnv[name]; !found {
+				patch = append(patch, patchOperation{
+					Op:    "add",
+					Path:  fmt.Sprintf("%s/-", patchPath),
+					Value: env,
+				})
+			}
+		}
+	}
+	return patch
+}
+
+func patchLifecycle(c *v1.Container, patchPath, mountLibPath string) []patchOperation {
+	klog.V(3).Infof("patchLifecycle in container[%s] exec on path[%s]", c.Name, mountLibPath)
+	var patch []patchOperation
+
+	// construct command
+	manageResetCmd := fmt.Sprintf("%s/manage --reset", mountLibPath)
+	postStartCommand := []string{"/bin/sh", "-c", manageResetCmd}
+	// init lifeCycle
+	lifeCycle := &v1.Lifecycle{}
+	// record len of c.Lifecycle.PostStart.Exec.Command
+	commandLen := 0
+	if c.Lifecycle != nil && c.Lifecycle.PostStart != nil && c.Lifecycle.PostStart.Exec != nil && c.Lifecycle.PostStart.Exec.
+		Command != nil {
+		commandLen = len(c.Lifecycle.PostStart.Exec.Command)
+		lifeCycle = c.Lifecycle
+	}
+	// patch Lifecycle.PostStart.Exec.Command
+	switch commandLen {
+	case 0:
+		lifeCycle.PostStart = &v1.Handler{Exec: &v1.ExecAction{Command: postStartCommand}}
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  patchPath,
+			Value: lifeCycle,
+		})
+	case 1:
+		manageResetCmd = fmt.Sprintf("%s && %s", manageResetCmd, c.Lifecycle.PostStart.Exec.Command[0])
+		postStartCommand[2] = manageResetCmd
+		lifeCycle.PostStart = &v1.Handler{Exec: &v1.ExecAction{Command: postStartCommand}}
+		patch = append(patch, patchOperation{
+			Op:    "replace",
+			Path:  patchPath,
+			Value: lifeCycle,
+		})
+	default:
+		command := c.Lifecycle.PostStart.Exec.Command
+		if len(command) == 3 && command[1] == "-c" {
+			manageResetCmd = fmt.Sprintf("%s && %s", manageResetCmd, c.Lifecycle.PostStart.Exec.Command[2])
+			postStartCommand = c.Lifecycle.PostStart.Exec.Command
+			postStartCommand[2] = manageResetCmd
+			lifeCycle.PostStart = &v1.Handler{Exec: &v1.ExecAction{Command: postStartCommand}}
+			patch = append(patch, patchOperation{
+				Op:    "replace",
+				Path:  patchPath,
+				Value: lifeCycle,
+			})
+		} else {
+			klog.Errorf("failed to patch Lifecycle.PostStart.Exec.Command, unsupport command")
+		}
+	}
+	klog.V(3).Infof("patchLifecycle content=[%+v]", patch)
+	return patch
+}
+
+func patchVolumes(pod *v1.Pod, patchCGPUCoreVolumes bool) []patchOperation {
+	klog.V(3).Infof("create volumeMount for pod %s.", pod.Name)
+	var patch []patchOperation
+	patchPath := "/spec/volumes"
+
+	patchVolumes := map[string]v1.Volume{
+		"nvidia-lib-cuda": v1.Volume{
+			Name: "nvidia-lib-cuda",
+			VolumeSource: v1.VolumeSource{
+				HostPath: &v1.HostPathVolumeSource{
+					Path: config.CgpuOptions.NvidiaLibCudaVolumeHostPath,
+				},
+			},
+		},
+	}
+	if len(pod.Spec.Volumes) == 0 {
+		volumes := make([]v1.Volume, 0)
+		for _, volume := range patchVolumes {
+			volumes = append(volumes, volume)
+		}
+		patch = append(patch, patchOperation{
+			Op:    "add",
+			Path:  patchPath,
+			Value: volumes,
+		})
+	} else {
+		existedVolume := make(map[string]bool)
+		for id, volume := range pod.Spec.Volumes {
+			if patchVolume, exist := patchVolumes[volume.Name]; exist {
+				existedVolume[volume.Name] = true
+				patch = append(patch, patchOperation{
+					Op:    "replace",
+					Path:  fmt.Sprintf("%s/%s", patchPath, strconv.Itoa(id)),
+					Value: patchVolume,
+				})
+			}
+		}
+		for name, volume := range patchVolumes {
+			if _, found := existedVolume[name]; !found {
+				patch = append(patch, patchOperation{
+					Op:    "add",
+					Path:  fmt.Sprintf("%s/-", patchPath),
+					Value: volume,
+				})
+			}
+		}
+	}
+	return patch
+}
+
+func getMountLibPath(pod *v1.Pod) string {
+	// libPath in pod settings, not required
+	if customMountPath, found := pod.Annotations[api.NvidiaDriverLib]; found && len(customMountPath) != 0 {
+		return customMountPath
+	} else {
+		// mountPath
+		mountPath := config.CgpuOptions.NvidiaLibCudaVolumeContainerPath
+		return mountPath
+	}
+}
+
+func getCGPUMemory(resourceList v1.ResourceList) (int64, string, error) {
+	cgpuMemValue := int64(0)
+	var cgpuMemUnit string
+	switch config.CgpuOptions.SharedGPUMemoryUnit {
+	case "GiB":
+		cgpuMemUnit = "G"
+	case "MiB":
+		cgpuMemUnit = "M"
+	case "KiB":
+		cgpuMemUnit = "K"
+	default:
+		return cgpuMemValue, cgpuMemUnit, fmt.Errorf("invalid cgpu memory unit[%s]", config.CgpuOptions.SharedGPUMemoryUnit)
+	}
+
+	for resType, resValue := range resourceList {
+		resourceName := resType.String()
+		if strings.HasSuffix(resourceName, api.CGPUMemory) {
+			mem, ok := resValue.AsInt64()
+			if !ok {
+				return cgpuMemValue, cgpuMemUnit, fmt.Errorf("invalid cgpu memory resource, type: %v, value: %v", resType, resValue)
+			}
+			cgpuMemValue = mem
+			break
+		}
+	}
+	return cgpuMemValue, cgpuMemUnit, nil
+}
+
+func getCGPUCore(resourceList v1.ResourceList) (int64, error) {
+	cgpuCoreValue := int64(0)
+	for resType, resValue := range resourceList {
+		resourceName := resType.String()
+		if strings.HasSuffix(resourceName, api.CGPUCore) {
+			core, ok := resValue.AsInt64()
+			if !ok {
+				return cgpuCoreValue, fmt.Errorf("invalid cgpu core resource, type: %v, value: %v", resType, resValue)
+			}
+			cgpuCoreValue = core
+			break
+		}
+	}
+	return cgpuCoreValue, nil
+}
diff --git a/pkg/webhooks/admission/queues/mutate/mutate_queue.go b/pkg/webhooks/admission/queues/mutate/mutate_queue.go
index 56b46de..2e401e3 100644
--- a/pkg/webhooks/admission/queues/mutate/mutate_queue.go
+++ b/pkg/webhooks/admission/queues/mutate/mutate_queue.go
@@ -40,6 +40,8 @@ var service = &router.AdmissionService{
 	Path: "/queues/mutate",
 	Func: Queues,
 
+	Config: config,
+
 	MutatingConfig: &whv1beta1.MutatingWebhookConfiguration{
 		Webhooks: []whv1beta1.MutatingWebhook{{
 			Name: "mutatequeue.volcano.sh",
@@ -57,6 +59,8 @@ var service = &router.AdmissionService{
 	},
 }
 
+var config = &router.AdmissionServiceConfig{}
+
 type patchOperation struct {
 	Op    string      `json:"op"`
 	Path  string      `json:"path"`
diff --git a/pkg/webhooks/admission/queues/validate/validate_queue.go b/pkg/webhooks/admission/queues/validate/validate_queue.go
index 41b5ac2..8737c5e 100644
--- a/pkg/webhooks/admission/queues/validate/validate_queue.go
+++ b/pkg/webhooks/admission/queues/validate/validate_queue.go
@@ -73,8 +73,10 @@ func AdmitQueues(ar v1beta1.AdmissionReview) *v1beta1.AdmissionResponse {
 	}
 
 	switch ar.Request.Operation {
-	case v1beta1.Create, v1beta1.Update:
+	case v1beta1.Create:
 		err = validateQueue(queue)
+	case v1beta1.Update:
+		err = validateQueueUpdating(queue)
 	case v1beta1.Delete:
 		err = validateQueueDeleting(ar.Request.Name)
 	default:
@@ -100,7 +102,9 @@ func validateQueue(queue *schedulingv1beta1.Queue) error {
 
 	errs = append(errs, validateStateOfQueue(queue.Status.State, resourcePath.Child("spec").Child("state"))...)
 	errs = append(errs, validateWeightOfQueue(queue.Spec.Weight, resourcePath.Child("spec").Child("weight"))...)
+	errs = append(errs, validateHardwareTypesOfQueue(queue, resourcePath.Child("metadata").Child("annotations"))...)
 	errs = append(errs, validateHierarchicalAttributes(queue, resourcePath.Child("metadata").Child("annotations"))...)
+	errs = append(errs, validateBindEQuota(queue, resourcePath.Child("metadata").Child("labels"))...)
 
 	if len(errs) > 0 {
 		return errs.ToAggregate()
@@ -108,6 +112,23 @@ func validateQueue(queue *schedulingv1beta1.Queue) error {
 
 	return nil
 }
+
+func validateQueueUpdating(queue *schedulingv1beta1.Queue) error {
+	errs := field.ErrorList{}
+	resourcePath := field.NewPath("requestBody")
+
+	errs = append(errs, validateStateOfQueue(queue.Status.State, resourcePath.Child("spec").Child("state"))...)
+	errs = append(errs, validateWeightOfQueue(queue.Spec.Weight, resourcePath.Child("spec").Child("weight"))...)
+	errs = append(errs, validateHierarchicalAttributes(queue, resourcePath.Child("metadata").Child("annotations"))...)
+	errs = append(errs, validateHardwareTypesOfQueueUpdating(queue, resourcePath.Child("metadata").Child("annotations"))...)
+	errs = append(errs, validateBindEQuotaUpdating(queue, resourcePath.Child("metadata").Child("labels"))...)
+
+	if len(errs) > 0 {
+		return errs.ToAggregate()
+	}
+	return nil
+}
+
 func validateHierarchicalAttributes(queue *schedulingv1beta1.Queue, fldPath *field.Path) field.ErrorList {
 	errs := field.ErrorList{}
 	hierarchy := queue.Annotations[schedulingv1beta1.KubeHierarchyAnnotationKey]
@@ -210,6 +231,9 @@ func validateQueueDeleting(queue string) error {
 		return fmt.Errorf("only queue with state `%s` can be deleted, queue `%s` state is `%s`",
 			schedulingv1beta1.QueueStateClosed, q.Name, q.Status.State)
 	}
+	if err := validateUnBindEQuota(q); err != nil {
+		return err
+	}
 
 	return nil
 }
diff --git a/pkg/webhooks/admission/queues/validate/validate_utils.go b/pkg/webhooks/admission/queues/validate/validate_utils.go
new file mode 100644
index 0000000..e1e9512
--- /dev/null
+++ b/pkg/webhooks/admission/queues/validate/validate_utils.go
@@ -0,0 +1,200 @@
+/*
+Copyright 2018 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package validate
+
+import (
+	"context"
+	"fmt"
+	"reflect"
+
+	"k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/klog"
+
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	"volcano.sh/volcano/pkg/common"
+)
+
+// validateHardwareTypesOfQueue validate queue when patch new hardwareTypes
+func validateHardwareTypesOfQueue(queue *schedulingv1beta1.Queue, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	hardwareTypesStr := queue.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	queueHardwareTypes := common.StringToSlice(hardwareTypesStr)
+	if len(queueHardwareTypes) == 0 {
+		return errs
+	}
+	if len(queueHardwareTypes) > 1 {
+		return append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("only one hardware type is allowed for queue")))
+	}
+	eQuota, err := getBoundEQuota(queue)
+	if err != nil {
+		errMsg := fmt.Sprintf("validateHardwareTypesOfQueue occur a error when getBoundEQuota by queue[%s], err=[%+v]", queue.Name, err)
+		klog.Errorf(errMsg)
+		return append(errs, field.Invalid(fldPath, queue.Name, errMsg))
+	}
+	if eQuota == nil {
+		return append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("cannot set hardware-type when havn't bound elasticresourcequota")))
+	}
+	found := false
+	hardwareTypesStr = eQuota.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	QuotaHardwareTypes := common.StringToSlice(hardwareTypesStr)
+	for _, hardwareType := range QuotaHardwareTypes {
+		if hardwareType == queueHardwareTypes[0] {
+			found = true
+			break
+		}
+	}
+	if !found {
+		errs = append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("queue[%s] hardware type not found", queue.Name)))
+	}
+	return errs
+}
+
+// validateHardwareTypesOfQueueUpdating validate queue when updating hardwareTypes
+func validateHardwareTypesOfQueueUpdating(queue *schedulingv1beta1.Queue, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	oldQueue, err := config.VolcanoClient.SchedulingV1beta1().Queues().Get(context.TODO(), queue.Name, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("validateHardwareTypesOfQueueUpdating failed, err=[%+v]", err)
+		errs = append(errs, field.Invalid(fldPath, queue.Name, err.Error()))
+	}
+	newHardwareTypes := queue.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	oldHardwareTypes := oldQueue.Annotations[schedulingv1beta1.QuotaHardwareTypeLabelKey]
+	if !reflect.DeepEqual(newHardwareTypes, oldHardwareTypes) {
+		errs = append(errs, validateHardwareTypesOfQueue(queue, fldPath)...)
+	}
+	return errs
+}
+
+// validateBindEQuota validate elasticresourcequota, ensure eQuota exist and not to repeat bind
+func validateBindEQuota(queue *schedulingv1beta1.Queue, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	eQuota, err := getBoundEQuota(queue)
+	if err != nil {
+		klog.Errorf("validateBindEQuota occurred error, get bound elasticresourcequota for queue[%s] failed, err=[%+v]", queue.Name, err)
+		return append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+	} else if eQuota == nil {
+		return errs
+	} else if !eQuota.Status.IsLeaf {
+		// double check for eQuota, due to eQuota.IsLeaf hasn't updated by controller-manager while queue bind to it
+		eQuotaChildren, err := common.GetEQuotaChildren(config.VolcanoClient, eQuota.Name)
+		if err != nil {
+			klog.Errorf("validateBindEQuota occurred error, get elasticresourcequota[%s]'s children for queue[%s] failed, err=[%+v]",
+				eQuota.Name, queue.Name, err)
+			return append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+		}
+		if len(eQuotaChildren) != 0 {
+			return append(errs, field.Invalid(fldPath, eQuota.Name, fmt.Sprint("cannot bind non-leaf elastic resource quota")))
+		}
+	}
+	klog.V(4).Infof("validate bound elasticresourcequota[%s] for queue[%s]", eQuota.Name, queue.Name)
+	bindingQueueName := eQuota.Status.QueueName
+	if len(bindingQueueName) > 0 {
+		if bindingQueueName != queue.Name {
+			errs = append(errs, field.Invalid(fldPath, eQuota.Name, fmt.Sprint("elastic resource quota has bound another queue ")))
+		}
+		return errs
+	}
+
+	eQuota.Status.QueueName = queue.Name
+	klog.V(4).Infof("update .Status.QueueName, elasticresourcequota[%+v]", eQuota)
+	newEQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().UpdateStatus(context.TODO(), eQuota, metav1.UpdateOptions{})
+	klog.V(4).Infof("newEQuota [%+v]", newEQuota)
+	if err != nil {
+		klog.Errorf("validateBindEQuota occurred error, update status of elasticresourcequota[%s] failed, err=[%+v]", newEQuota.Name, err)
+		return append(errs, field.Invalid(fldPath, eQuota.Name, err.Error()))
+	}
+	return errs
+}
+
+// validateBindEQuotaUpdating validate and execute the update action of binding elasticresourcequota
+func validateBindEQuotaUpdating(queue *schedulingv1beta1.Queue, fldPath *field.Path) field.ErrorList {
+	errs := field.ErrorList{}
+	oldQueue, err := config.VolcanoClient.SchedulingV1beta1().Queues().Get(context.TODO(), queue.Name, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("validateBindEQuotaUpdating occurred error, update queue failed, err=[%+v]", err)
+		return append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("%s", err)))
+	}
+	oldEQuotaKey, oldExist := oldQueue.Labels[schedulingv1beta1.QueueBindingElasticQuotaKey]
+	eQuotaKey, exist := queue.Labels[schedulingv1beta1.QueueBindingElasticQuotaKey]
+	// nothing changed then return
+	if oldEQuotaKey == eQuotaKey {
+		return errs
+	}
+	if oldExist && !exist {
+		klog.V(4).Infof("queue[%s] unbind with old elasticresourcequota", queue.Name)
+		if err := validateUnBindEQuota(oldQueue); err != nil {
+			errs = append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("%s", err)))
+			return errs
+		}
+	} else if !oldExist && exist {
+		klog.V(4).Infof("queue[%s] binding new elasticresourcequota", queue.Name)
+		if queue.Status.State != schedulingv1beta1.QueueStateClosed {
+			return append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprintf("please makes queue in state=[%s] first",
+				schedulingv1beta1.QueueStateClosed)))
+		}
+		errs = append(errs, validateBindEQuota(queue, fldPath)...)
+	} else if oldExist && exist {
+		return append(errs, field.Invalid(fldPath, queue.Name, fmt.Sprint("cannot update bind elasticresourcequota")))
+	}
+
+	return errs
+}
+
+// validateUnBindEQuota validate and execute the update action of binding elasticresourcequota when deleting queue or updating queue
+func validateUnBindEQuota(queue *schedulingv1beta1.Queue) error {
+	if queue.Status.State != schedulingv1beta1.QueueStateClosed {
+		return fmt.Errorf("unbind elastic resource quota only if queue state=[%s]", schedulingv1beta1.QueueStateClosed)
+	}
+
+	eQuota, err := getBoundEQuota(queue)
+	if err != nil {
+		klog.Errorf("validateUnBindEQuota occurred error, get bound elasticresourcequota failed, err=[%+v]", err)
+		// already been deleted
+		if errors.IsNotFound(err) {
+			return nil
+		}
+		return err
+	}
+	// eQuota has deleted the key or queue has no label of eQuotaKey
+	if eQuota == nil || len(eQuota.Status.QueueName) == 0 {
+		klog.V(4).Infof("elasticresourcequota is not exist or has already unbind queue[%s]", queue.Name)
+		return nil
+	}
+	klog.V(4).Infof("Update status of elasticresourcequota[%s] for unbinding with queue[%s]", eQuota.Name, queue.Name)
+	eQuota.Status.QueueName = ""
+	if _, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().UpdateStatus(context.TODO(), eQuota, metav1.UpdateOptions{}); err != nil {
+		klog.Errorf("validateUnBindEQuota occurred error, update elasticresourcequota failed, err=[%+v]", err)
+		return err
+	}
+	return nil
+}
+
+// getBoundEQuota getting elasticresourcequota based on queue labels
+func getBoundEQuota(queue *schedulingv1beta1.Queue) (*schedulingv1beta1.ElasticResourceQuota, error) {
+	eQuotaName, exist := queue.Labels[schedulingv1beta1.QueueBindingElasticQuotaKey]
+	if !exist {
+		return nil, nil
+	}
+	eQuota, err := config.VolcanoClient.SchedulingV1beta1().ElasticResourceQuotas().Get(context.TODO(), eQuotaName, metav1.GetOptions{})
+	if err != nil {
+		klog.Errorf("getBoundEQuota occurred error, get elasticresourcequota[%s] failed, err=[%+v]", eQuotaName, err)
+		return nil, err
+	}
+	return eQuota, nil
+}
diff --git a/pkg/webhooks/router/interface.go b/pkg/webhooks/router/interface.go
index 503567a..48ae2bd 100644
--- a/pkg/webhooks/router/interface.go
+++ b/pkg/webhooks/router/interface.go
@@ -28,12 +28,23 @@ import (
 type AdmitFunc func(v1beta1.AdmissionReview) *v1beta1.AdmissionResponse
 
 type AdmissionServiceConfig struct {
+	CgpuOptions   CGPUOptions
 	SchedulerName string
 	KubeClient    kubernetes.Interface
 	VolcanoClient versioned.Interface
 	Recorder      record.EventRecorder
 }
 
+type CGPUOptions struct {
+	NvidiaLibCudaVolumeHostPath      string // libcuda.so hostpath
+	NvidiaLibCudaVolumeContainerPath string // libcuda.so containerPath
+	GPUCGroupEnable                  bool   // Global switch to limit vGPU resource
+	CGroupMapMode                    string // type of mapping progress in host, 0:docker 1:kongming
+	SharedGPUMemoryUnit              string // The vgpu resources unit reported by device-plugin
+	Limit                            string // LIMIT 0:soft limit & 1:strict limit, which is different type of resource limitation
+	NvidiaVisibleDevices             string // the visible devices ids of nvidia
+}
+
 type AdmissionService struct {
 	Path    string
 	Func    AdmitFunc
diff --git a/pkg/webhooks/schema/schema.go b/pkg/webhooks/schema/schema.go
index e4492fe..1c24e4a 100644
--- a/pkg/webhooks/schema/schema.go
+++ b/pkg/webhooks/schema/schema.go
@@ -85,6 +85,25 @@ func DecodePod(object runtime.RawExtension, resource metav1.GroupVersionResource
 	return &pod, nil
 }
 
+func DecodeNode(object runtime.RawExtension, resource metav1.GroupVersionResource) (*v1.Node, error) {
+	nodeResource := metav1.GroupVersionResource{Group: "", Version: "v1", Resource: "nodes"}
+	raw := object.Raw
+	node := v1.Node{}
+
+	if resource != nodeResource {
+		err := fmt.Errorf("expect resource to be %s", nodeResource)
+		return &node, err
+	}
+
+	deserializer := Codecs.UniversalDeserializer()
+	if _, _, err := deserializer.Decode(raw, nil, &node); err != nil {
+		return &node, err
+	}
+	klog.V(3).Infof("the node struct is %+v", node)
+
+	return &node, nil
+}
+
 // DecodeQueue decodes the queue using deserializer from the raw object.
 func DecodeQueue(object runtime.RawExtension, resource metav1.GroupVersionResource) (*schedulingv1beta1.Queue, error) {
 	queueResource := metav1.GroupVersionResource{
@@ -124,3 +143,23 @@ func DecodePodGroup(object runtime.RawExtension, resource metav1.GroupVersionRes
 
 	return &podgroup, nil
 }
+
+// DecodeElasticResourceQuota decodes the elasticResourceQuota using deserializer from the raw object.
+func DecodeElasticResourceQuota(object runtime.RawExtension, resource metav1.GroupVersionResource) (*schedulingv1beta1.ElasticResourceQuota, error) {
+	eQuotaResource := metav1.GroupVersionResource{
+		Group:    schedulingv1beta1.SchemeGroupVersion.Group,
+		Version:  schedulingv1beta1.SchemeGroupVersion.Version,
+		Resource: "elasticresourcequotas",
+	}
+
+	if resource != eQuotaResource {
+		return nil, fmt.Errorf("expect resource to be %s", eQuotaResource)
+	}
+
+	elasticresourcequotas := schedulingv1beta1.ElasticResourceQuota{}
+	if _, _, err := Codecs.UniversalDeserializer().Decode(object.Raw, nil, &elasticresourcequotas); err != nil {
+		return nil, err
+	}
+
+	return &elasticresourcequotas, nil
+}
diff --git a/vendor/volcano.sh/apis/pkg/apis/helpers/helpers.go b/vendor/volcano.sh/apis/pkg/apis/helpers/helpers.go
index 32e37c5..3044b9a 100644
--- a/vendor/volcano.sh/apis/pkg/apis/helpers/helpers.go
+++ b/vendor/volcano.sh/apis/pkg/apis/helpers/helpers.go
@@ -178,15 +178,6 @@ func DeleteSecret(job *vcbatch.Job, kubeClients kubernetes.Interface, secretName
 func GeneratePodgroupName(pod *v1.Pod) string {
 	pgName := vcbatch.PodgroupNamePrefix
 
-	if len(pod.OwnerReferences) != 0 {
-		for _, ownerReference := range pod.OwnerReferences {
-			if ownerReference.Controller != nil && *ownerReference.Controller {
-				pgName += string(ownerReference.UID)
-				return pgName
-			}
-		}
-	}
-
 	pgName += string(pod.UID)
 
 	return pgName
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/types_equota.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/types_equota.go
new file mode 100644
index 0000000..0c3099c
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/types_equota.go
@@ -0,0 +1,78 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package scheduling
+
+import (
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+)
+
+// +genclient
+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
+
+// Elastic Resource Quota
+type ElasticResourceQuota struct {
+	metav1.TypeMeta `json:",inline"`
+	// +optional
+	metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"`
+	// Specification of the desired behavior of the ElasticResourceQuota.
+	// More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status
+	// +optional
+	Spec ElasticResourceQuotaSpec `json:"spec,omitempty" protobuf:"bytes,2,opt,name=spec"`
+	// The status of ElasticResourceQuota.
+	// +optional
+	Status ElasticResourceQuotaStatus `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"`
+}
+
+// ElasticResourceQuotaSpec represents the template of Elastic Resource Quota.
+type ElasticResourceQuotaSpec struct {
+	// Max is the upper bound of elastic resource quota
+	Max v1.ResourceList `json:"max,omitempty" protobuf:"bytes,1,opt,name=max"`
+	// Min is the lower bound of elastic resource quota
+	Min v1.ResourceList `json:"min,omitempty" protobuf:"bytes,2,opt,name=min"`
+	// Reclaimable indicate whether the elastic quota can be reclaimed by other elastic resource quota
+	Reclaimable bool `json:"reclaimable,omitempty" protobuf:"bytes,3,opt,name=reclaimable"`
+	// HardwareTypes defines hardware types of elastic resource quota
+	HardwareTypes []string `json:"hardwareTypes,omitempty" protobuf:"bytes,4,opt,name=hardwareTypes"`
+	// namespace defines elastic resource quota belongs to one namespace
+	Namespace string `json:"namespace,omitempty" protobuf:"bytes,5,opt,name=namespace"`
+}
+
+// ElasticResourceQuotaStatus represents the status of Elastic Resource Quota.
+type ElasticResourceQuotaStatus struct {
+	// IsLeaf defines whether elastic resource quota is leaf or not
+	IsLeaf bool `json:"isLeaf,omitempty" protobuf:"bytes,1,opt,name=isLeaf"`
+	// Used resource of elastic resource quota
+	Used v1.ResourceList `json:"used,omitempty" protobuf:"bytes,2,opt,name=used"`
+	// QueueName indicate bound queue
+	QueueName string `json:"queueName,omitempty" protobuf:"bytes,3,opt,name=queueName"`
+}
+
+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
+// +kubebuilder:object:root=true
+
+// ElasticResourceQuotaList is a collection of ElasticResourceQuota.
+type ElasticResourceQuotaList struct {
+	metav1.TypeMeta `json:",inline"`
+	// Standard list metadata
+	// More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
+	// +optional
+	metav1.ListMeta `json:"metadata,omitempty"`
+
+	// items is the list of PodGroup
+	Items []ElasticResourceQuota `json:"items"`
+}
\ No newline at end of file
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/labels_elastic_quota.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/labels_elastic_quota.go
new file mode 100644
index 0000000..d15dce3
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/labels_elastic_quota.go
@@ -0,0 +1,100 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package v1beta1
+
+import (
+	"os"
+	"strings"
+
+	"k8s.io/klog"
+)
+
+// QuotaTypeKey identify the type of quota in labels
+// which can be 'logical' or 'physical'.
+// soft means share the same nodes set with other quotas
+// hard specified a node set
+const QuotaTypeKey = "paddleflow.baidu.com/resource-isolation-type"
+const QuotaTypePhysical = "physical"
+const QuotaTypeLogical = "logical"
+
+// QueueBindingElasticQuotaKey is a label key, which identify the name of elasticResourceQuota which binding current queue
+const QueueBindingElasticQuotaKey = "paddleflow.baidu.com/binding-elastic-resource-quota-name"
+
+// ElasticQuotaParentKey is a label key, which indicate the parent Name for current elasticResourceQuota
+const ElasticQuotaParentKey = "paddleflow.baidu.com/elastic-resource-quota-parent"
+
+var (
+	// GlobalNodeManageEnabled
+	GlobalNodeManageEnabled = true
+	// QuotaManagedNodeKey indicate label of node, which is managed by quota
+	QuotaManagedNodeKey string = "paddleflow.baidu.com/cluster-role"
+	// QuotaLabelKey indicate node label key of quota
+	QuotaLabelKey string = "paddleflow.baidu.com/quota"
+	// QuotaHardwareTypeLabelKey label key of hardware type
+	QuotaHardwareTypeLabelKey string = "paddleflow.baidu.com/hardware-type"
+	// NodeScaleDown idle node state, which is one of type in labels[ResourceQuotaLabel]
+	NodeScaleDown = "scale-down"
+	// DefaultResourceQuota the name of default resourcequota
+	DefaultResourceQuota = "default"
+	// DefaultNamespace the key of default resourcequota in default namespace
+	DefaultNamespace = "default"
+	// resourceFieldFilter
+	ResourceFieldFilter = map[string]bool{"hugepages-1Gi": true, "hugepages-2Mi": true, "ephemeral-storage": true, "pods": true}
+)
+
+const (
+	GlobalNodeManageEnabledEnv           = "GLOBAL_NODE_MANAGE_ENABLE"
+	ResourceQuotaNodeManageLabelKeyEnv   = "RESOURCE_QUOTA_NODE_MANAGE_LABEL_KEY"
+	ResourceQuotaLabelKeyEnv             = "RESOURCE_QUOTA_LABEL_KEY"
+	ResourceQuotaHardwareTypeLabelKeyEnv = "RESOURCE_QUOTA_HARDWARE_TYPE_LABEL_KEY"
+	ResourceFieldFilterEnv               = "RESOURCE_FIELD_FILTER"
+)
+
+func ResourceQuotaConfigInit() {
+	value := os.Getenv(GlobalNodeManageEnabledEnv)
+	if value == "false" {
+		GlobalNodeManageEnabled = false
+	}
+	rpnm := os.Getenv(ResourceQuotaNodeManageLabelKeyEnv)
+	if len(rpnm) != 0 {
+		QuotaManagedNodeKey = rpnm
+	}
+	rpLabelKey := os.Getenv(ResourceQuotaLabelKeyEnv)
+	if len(rpLabelKey) != 0 {
+		QuotaLabelKey = rpLabelKey
+	}
+	hardwareTypeLabelKey := os.Getenv(ResourceQuotaHardwareTypeLabelKeyEnv)
+	if len(hardwareTypeLabelKey) != 0 {
+		QuotaHardwareTypeLabelKey = hardwareTypeLabelKey
+	}
+	resourceFieldFilter := os.Getenv(ResourceFieldFilterEnv)
+	if len(resourceFieldFilter) != 0 {
+		fieldFilterList := strings.Split(resourceFieldFilter, ",")
+		ResourceFieldFilter = map[string]bool{}
+		for _, fieldFilter := range fieldFilterList {
+			ResourceFieldFilter[fieldFilter] = true
+		}
+	}
+
+	// It is recommended to code according to the line-format when printing new variables
+	klog.Infof("resourcequota config: GlobalNodeManageEnabled[%v], QuotaManagedNodeKey[%v], "+
+		"QuotaLabelKey[%v], QuotaHardwareTypeLabelKey[%v], NodeScaleDown[%v], DefaultResourceQuota[%v], "+
+		"ResourceFieldFilter[%v]",
+		GlobalNodeManageEnabled, QuotaManagedNodeKey,
+		QuotaLabelKey, QuotaHardwareTypeLabelKey, NodeScaleDown, DefaultResourceQuota,
+		ResourceFieldFilter)
+}
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/register.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/register.go
index 43b6a43..eccd7df 100644
--- a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/register.go
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/register.go
@@ -51,6 +51,8 @@ func addKnownTypes(scheme *runtime.Scheme) error {
 		&PodGroupList{},
 		&Queue{},
 		&QueueList{},
+		&ElasticResourceQuota{},
+		&ElasticResourceQuotaList{},
 	)
 
 	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/types_equota.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/types_equota.go
new file mode 100644
index 0000000..47e08e5
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/types_equota.go
@@ -0,0 +1,82 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package v1beta1
+
+import (
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+)
+
+// +genclient
+// +genclient:nonNamespaced
+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
+// +kubebuilder:object:root=true
+// +kubebuilder:resource:path=elasticresourcequotas,scope=Cluster,shortName=equota;equota-v1beta1
+// +kubebuilder:subresource:status
+
+// Elastic Resource Quota
+type ElasticResourceQuota struct {
+	metav1.TypeMeta `json:",inline"`
+	// +optional
+	metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"`
+	// Specification of the desired behavior of the ElasticResourceQuota.
+	// More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status
+	// +optional
+	Spec ElasticResourceQuotaSpec `json:"spec,omitempty" protobuf:"bytes,2,opt,name=spec"`
+	// The status of ElasticResourceQuota.
+	// +optional
+	Status ElasticResourceQuotaStatus `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"`
+}
+
+// ElasticResourceQuotaSpec represents the template of Elastic Resource Quota.
+type ElasticResourceQuotaSpec struct {
+	// Max is the upper bound of elastic resource quota
+	Max v1.ResourceList `json:"max,omitempty" protobuf:"bytes,1,opt,name=max"`
+	// Min is the lower bound of elastic resource quota
+	Min v1.ResourceList `json:"min,omitempty" protobuf:"bytes,2,opt,name=min"`
+	// Reclaimable indicate whether the elastic quota can be reclaimed by other elastic resource quota
+	Reclaimable bool `json:"reclaimable,omitempty" protobuf:"bytes,3,opt,name=reclaimable"`
+	// HardwareTypes defines hardware types of elastic resource quota
+	HardwareTypes []string `json:"hardwareTypes,omitempty" protobuf:"bytes,4,opt,name=hardwareTypes"`
+	// namespace defines elastic resource quota belongs to one namespace
+	Namespace string `json:"namespace,omitempty" protobuf:"bytes,5,opt,name=namespace"`
+}
+
+// ElasticResourceQuotaStatus represents the status of Elastic Resource Quota.
+type ElasticResourceQuotaStatus struct {
+	// IsLeaf defines whether elastic resource quota is leaf or not
+	IsLeaf bool `json:"isLeaf,omitempty" protobuf:"bytes,1,opt,name=isLeaf"`
+	// Used resource of elastic resource quota
+	Used v1.ResourceList `json:"used,omitempty" protobuf:"bytes,2,opt,name=used"`
+	// QueueName indicate bound queue
+	QueueName string `json:"queueName,omitempty" protobuf:"bytes,3,opt,name=queueName"`
+}
+
+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
+// +kubebuilder:object:root=true
+
+// ElasticResourceQuotaList is a collection of ElasticResourceQuota.
+type ElasticResourceQuotaList struct {
+	metav1.TypeMeta `json:",inline"`
+	// Standard list metadata
+	// More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
+	// +optional
+	metav1.ListMeta `json:"metadata,omitempty"`
+
+	// items is the list of PodGroup
+	Items []ElasticResourceQuota `json:"items"`
+}
\ No newline at end of file
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.conversion.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.conversion.go
index 6541c0d..4c7a35b 100644
--- a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.conversion.go
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.conversion.go
@@ -35,6 +35,46 @@ func init() {
 // RegisterConversions adds conversion functions to the given scheme.
 // Public to allow building arbitrary schemes.
 func RegisterConversions(s *runtime.Scheme) error {
+	if err := s.AddGeneratedConversionFunc((*ElasticResourceQuota)(nil), (*scheduling.ElasticResourceQuota)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_v1beta1_ElasticResourceQuota_To_scheduling_ElasticResourceQuota(a.(*ElasticResourceQuota), b.(*scheduling.ElasticResourceQuota), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*scheduling.ElasticResourceQuota)(nil), (*ElasticResourceQuota)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_scheduling_ElasticResourceQuota_To_v1beta1_ElasticResourceQuota(a.(*scheduling.ElasticResourceQuota), b.(*ElasticResourceQuota), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*ElasticResourceQuotaList)(nil), (*scheduling.ElasticResourceQuotaList)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_v1beta1_ElasticResourceQuotaList_To_scheduling_ElasticResourceQuotaList(a.(*ElasticResourceQuotaList), b.(*scheduling.ElasticResourceQuotaList), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*scheduling.ElasticResourceQuotaList)(nil), (*ElasticResourceQuotaList)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_scheduling_ElasticResourceQuotaList_To_v1beta1_ElasticResourceQuotaList(a.(*scheduling.ElasticResourceQuotaList), b.(*ElasticResourceQuotaList), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*ElasticResourceQuotaSpec)(nil), (*scheduling.ElasticResourceQuotaSpec)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec(a.(*ElasticResourceQuotaSpec), b.(*scheduling.ElasticResourceQuotaSpec), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*scheduling.ElasticResourceQuotaSpec)(nil), (*ElasticResourceQuotaSpec)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec(a.(*scheduling.ElasticResourceQuotaSpec), b.(*ElasticResourceQuotaSpec), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*ElasticResourceQuotaStatus)(nil), (*scheduling.ElasticResourceQuotaStatus)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus(a.(*ElasticResourceQuotaStatus), b.(*scheduling.ElasticResourceQuotaStatus), scope)
+	}); err != nil {
+		return err
+	}
+	if err := s.AddGeneratedConversionFunc((*scheduling.ElasticResourceQuotaStatus)(nil), (*ElasticResourceQuotaStatus)(nil), func(a, b interface{}, scope conversion.Scope) error {
+		return Convert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus(a.(*scheduling.ElasticResourceQuotaStatus), b.(*ElasticResourceQuotaStatus), scope)
+	}); err != nil {
+		return err
+	}
 	if err := s.AddGeneratedConversionFunc((*PodGroup)(nil), (*scheduling.PodGroup)(nil), func(a, b interface{}, scope conversion.Scope) error {
 		return Convert_v1beta1_PodGroup_To_scheduling_PodGroup(a.(*PodGroup), b.(*scheduling.PodGroup), scope)
 	}); err != nil {
@@ -128,6 +168,112 @@ func RegisterConversions(s *runtime.Scheme) error {
 	return nil
 }
 
+func autoConvert_v1beta1_ElasticResourceQuota_To_scheduling_ElasticResourceQuota(in *ElasticResourceQuota, out *scheduling.ElasticResourceQuota, s conversion.Scope) error {
+	out.ObjectMeta = in.ObjectMeta
+	if err := Convert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec(&in.Spec, &out.Spec, s); err != nil {
+		return err
+	}
+	if err := Convert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus(&in.Status, &out.Status, s); err != nil {
+		return err
+	}
+	return nil
+}
+
+// Convert_v1beta1_ElasticResourceQuota_To_scheduling_ElasticResourceQuota is an autogenerated conversion function.
+func Convert_v1beta1_ElasticResourceQuota_To_scheduling_ElasticResourceQuota(in *ElasticResourceQuota, out *scheduling.ElasticResourceQuota, s conversion.Scope) error {
+	return autoConvert_v1beta1_ElasticResourceQuota_To_scheduling_ElasticResourceQuota(in, out, s)
+}
+
+func autoConvert_scheduling_ElasticResourceQuota_To_v1beta1_ElasticResourceQuota(in *scheduling.ElasticResourceQuota, out *ElasticResourceQuota, s conversion.Scope) error {
+	out.ObjectMeta = in.ObjectMeta
+	if err := Convert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec(&in.Spec, &out.Spec, s); err != nil {
+		return err
+	}
+	if err := Convert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus(&in.Status, &out.Status, s); err != nil {
+		return err
+	}
+	return nil
+}
+
+// Convert_scheduling_ElasticResourceQuota_To_v1beta1_ElasticResourceQuota is an autogenerated conversion function.
+func Convert_scheduling_ElasticResourceQuota_To_v1beta1_ElasticResourceQuota(in *scheduling.ElasticResourceQuota, out *ElasticResourceQuota, s conversion.Scope) error {
+	return autoConvert_scheduling_ElasticResourceQuota_To_v1beta1_ElasticResourceQuota(in, out, s)
+}
+
+func autoConvert_v1beta1_ElasticResourceQuotaList_To_scheduling_ElasticResourceQuotaList(in *ElasticResourceQuotaList, out *scheduling.ElasticResourceQuotaList, s conversion.Scope) error {
+	out.ListMeta = in.ListMeta
+	out.Items = *(*[]scheduling.ElasticResourceQuota)(unsafe.Pointer(&in.Items))
+	return nil
+}
+
+// Convert_v1beta1_ElasticResourceQuotaList_To_scheduling_ElasticResourceQuotaList is an autogenerated conversion function.
+func Convert_v1beta1_ElasticResourceQuotaList_To_scheduling_ElasticResourceQuotaList(in *ElasticResourceQuotaList, out *scheduling.ElasticResourceQuotaList, s conversion.Scope) error {
+	return autoConvert_v1beta1_ElasticResourceQuotaList_To_scheduling_ElasticResourceQuotaList(in, out, s)
+}
+
+func autoConvert_scheduling_ElasticResourceQuotaList_To_v1beta1_ElasticResourceQuotaList(in *scheduling.ElasticResourceQuotaList, out *ElasticResourceQuotaList, s conversion.Scope) error {
+	out.ListMeta = in.ListMeta
+	out.Items = *(*[]ElasticResourceQuota)(unsafe.Pointer(&in.Items))
+	return nil
+}
+
+// Convert_scheduling_ElasticResourceQuotaList_To_v1beta1_ElasticResourceQuotaList is an autogenerated conversion function.
+func Convert_scheduling_ElasticResourceQuotaList_To_v1beta1_ElasticResourceQuotaList(in *scheduling.ElasticResourceQuotaList, out *ElasticResourceQuotaList, s conversion.Scope) error {
+	return autoConvert_scheduling_ElasticResourceQuotaList_To_v1beta1_ElasticResourceQuotaList(in, out, s)
+}
+
+func autoConvert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec(in *ElasticResourceQuotaSpec, out *scheduling.ElasticResourceQuotaSpec, s conversion.Scope) error {
+	out.Max = *(*v1.ResourceList)(unsafe.Pointer(&in.Max))
+	out.Min = *(*v1.ResourceList)(unsafe.Pointer(&in.Min))
+	out.Reclaimable = in.Reclaimable
+	out.HardwareTypes = *(*[]string)(unsafe.Pointer(&in.HardwareTypes))
+	out.Namespace = in.Namespace
+	return nil
+}
+
+// Convert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec is an autogenerated conversion function.
+func Convert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec(in *ElasticResourceQuotaSpec, out *scheduling.ElasticResourceQuotaSpec, s conversion.Scope) error {
+	return autoConvert_v1beta1_ElasticResourceQuotaSpec_To_scheduling_ElasticResourceQuotaSpec(in, out, s)
+}
+
+func autoConvert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec(in *scheduling.ElasticResourceQuotaSpec, out *ElasticResourceQuotaSpec, s conversion.Scope) error {
+	out.Max = *(*v1.ResourceList)(unsafe.Pointer(&in.Max))
+	out.Min = *(*v1.ResourceList)(unsafe.Pointer(&in.Min))
+	out.Reclaimable = in.Reclaimable
+	out.HardwareTypes = *(*[]string)(unsafe.Pointer(&in.HardwareTypes))
+	out.Namespace = in.Namespace
+	return nil
+}
+
+// Convert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec is an autogenerated conversion function.
+func Convert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec(in *scheduling.ElasticResourceQuotaSpec, out *ElasticResourceQuotaSpec, s conversion.Scope) error {
+	return autoConvert_scheduling_ElasticResourceQuotaSpec_To_v1beta1_ElasticResourceQuotaSpec(in, out, s)
+}
+
+func autoConvert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus(in *ElasticResourceQuotaStatus, out *scheduling.ElasticResourceQuotaStatus, s conversion.Scope) error {
+	out.IsLeaf = in.IsLeaf
+	out.Used = *(*v1.ResourceList)(unsafe.Pointer(&in.Used))
+	out.QueueName = in.QueueName
+	return nil
+}
+
+// Convert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus is an autogenerated conversion function.
+func Convert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus(in *ElasticResourceQuotaStatus, out *scheduling.ElasticResourceQuotaStatus, s conversion.Scope) error {
+	return autoConvert_v1beta1_ElasticResourceQuotaStatus_To_scheduling_ElasticResourceQuotaStatus(in, out, s)
+}
+
+func autoConvert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus(in *scheduling.ElasticResourceQuotaStatus, out *ElasticResourceQuotaStatus, s conversion.Scope) error {
+	out.IsLeaf = in.IsLeaf
+	out.Used = *(*v1.ResourceList)(unsafe.Pointer(&in.Used))
+	out.QueueName = in.QueueName
+	return nil
+}
+
+// Convert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus is an autogenerated conversion function.
+func Convert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus(in *scheduling.ElasticResourceQuotaStatus, out *ElasticResourceQuotaStatus, s conversion.Scope) error {
+	return autoConvert_scheduling_ElasticResourceQuotaStatus_To_v1beta1_ElasticResourceQuotaStatus(in, out, s)
+}
+
 func autoConvert_v1beta1_PodGroup_To_scheduling_PodGroup(in *PodGroup, out *scheduling.PodGroup, s conversion.Scope) error {
 	out.ObjectMeta = in.ObjectMeta
 	if err := Convert_v1beta1_PodGroupSpec_To_scheduling_PodGroupSpec(&in.Spec, &out.Spec, s); err != nil {
@@ -214,6 +360,7 @@ func Convert_scheduling_PodGroupList_To_v1beta1_PodGroupList(in *scheduling.PodG
 
 func autoConvert_v1beta1_PodGroupSpec_To_scheduling_PodGroupSpec(in *PodGroupSpec, out *scheduling.PodGroupSpec, s conversion.Scope) error {
 	out.MinMember = in.MinMember
+	out.MinTaskMember = *(*map[string]int32)(unsafe.Pointer(&in.MinTaskMember))
 	out.Queue = in.Queue
 	out.PriorityClassName = in.PriorityClassName
 	out.MinResources = (*v1.ResourceList)(unsafe.Pointer(in.MinResources))
@@ -227,6 +374,7 @@ func Convert_v1beta1_PodGroupSpec_To_scheduling_PodGroupSpec(in *PodGroupSpec, o
 
 func autoConvert_scheduling_PodGroupSpec_To_v1beta1_PodGroupSpec(in *scheduling.PodGroupSpec, out *PodGroupSpec, s conversion.Scope) error {
 	out.MinMember = in.MinMember
+	out.MinTaskMember = *(*map[string]int32)(unsafe.Pointer(&in.MinTaskMember))
 	out.Queue = in.Queue
 	out.PriorityClassName = in.PriorityClassName
 	out.MinResources = (*v1.ResourceList)(unsafe.Pointer(in.MinResources))
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.deepcopy.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.deepcopy.go
index eeb8fa6..83d37a5 100644
--- a/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.deepcopy.go
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/v1beta1/zz_generated.deepcopy.go
@@ -25,6 +25,125 @@ import (
 	runtime "k8s.io/apimachinery/pkg/runtime"
 )
 
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuota) DeepCopyInto(out *ElasticResourceQuota) {
+	*out = *in
+	out.TypeMeta = in.TypeMeta
+	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
+	in.Spec.DeepCopyInto(&out.Spec)
+	in.Status.DeepCopyInto(&out.Status)
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuota.
+func (in *ElasticResourceQuota) DeepCopy() *ElasticResourceQuota {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuota)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
+func (in *ElasticResourceQuota) DeepCopyObject() runtime.Object {
+	if c := in.DeepCopy(); c != nil {
+		return c
+	}
+	return nil
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaList) DeepCopyInto(out *ElasticResourceQuotaList) {
+	*out = *in
+	out.TypeMeta = in.TypeMeta
+	in.ListMeta.DeepCopyInto(&out.ListMeta)
+	if in.Items != nil {
+		in, out := &in.Items, &out.Items
+		*out = make([]ElasticResourceQuota, len(*in))
+		for i := range *in {
+			(*in)[i].DeepCopyInto(&(*out)[i])
+		}
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaList.
+func (in *ElasticResourceQuotaList) DeepCopy() *ElasticResourceQuotaList {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaList)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
+func (in *ElasticResourceQuotaList) DeepCopyObject() runtime.Object {
+	if c := in.DeepCopy(); c != nil {
+		return c
+	}
+	return nil
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaSpec) DeepCopyInto(out *ElasticResourceQuotaSpec) {
+	*out = *in
+	if in.Max != nil {
+		in, out := &in.Max, &out.Max
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	if in.Min != nil {
+		in, out := &in.Min, &out.Min
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	if in.HardwareTypes != nil {
+		in, out := &in.HardwareTypes, &out.HardwareTypes
+		*out = make([]string, len(*in))
+		copy(*out, *in)
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaSpec.
+func (in *ElasticResourceQuotaSpec) DeepCopy() *ElasticResourceQuotaSpec {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaSpec)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaStatus) DeepCopyInto(out *ElasticResourceQuotaStatus) {
+	*out = *in
+	if in.Used != nil {
+		in, out := &in.Used, &out.Used
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaStatus.
+func (in *ElasticResourceQuotaStatus) DeepCopy() *ElasticResourceQuotaStatus {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaStatus)
+	in.DeepCopyInto(out)
+	return out
+}
+
 // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
 func (in *PodGroup) DeepCopyInto(out *PodGroup) {
 	*out = *in
diff --git a/vendor/volcano.sh/apis/pkg/apis/scheduling/zz_generated.deepcopy.go b/vendor/volcano.sh/apis/pkg/apis/scheduling/zz_generated.deepcopy.go
index 62e3a68..a568e8c 100644
--- a/vendor/volcano.sh/apis/pkg/apis/scheduling/zz_generated.deepcopy.go
+++ b/vendor/volcano.sh/apis/pkg/apis/scheduling/zz_generated.deepcopy.go
@@ -25,6 +25,125 @@ import (
 	runtime "k8s.io/apimachinery/pkg/runtime"
 )
 
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuota) DeepCopyInto(out *ElasticResourceQuota) {
+	*out = *in
+	out.TypeMeta = in.TypeMeta
+	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
+	in.Spec.DeepCopyInto(&out.Spec)
+	in.Status.DeepCopyInto(&out.Status)
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuota.
+func (in *ElasticResourceQuota) DeepCopy() *ElasticResourceQuota {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuota)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
+func (in *ElasticResourceQuota) DeepCopyObject() runtime.Object {
+	if c := in.DeepCopy(); c != nil {
+		return c
+	}
+	return nil
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaList) DeepCopyInto(out *ElasticResourceQuotaList) {
+	*out = *in
+	out.TypeMeta = in.TypeMeta
+	in.ListMeta.DeepCopyInto(&out.ListMeta)
+	if in.Items != nil {
+		in, out := &in.Items, &out.Items
+		*out = make([]ElasticResourceQuota, len(*in))
+		for i := range *in {
+			(*in)[i].DeepCopyInto(&(*out)[i])
+		}
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaList.
+func (in *ElasticResourceQuotaList) DeepCopy() *ElasticResourceQuotaList {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaList)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
+func (in *ElasticResourceQuotaList) DeepCopyObject() runtime.Object {
+	if c := in.DeepCopy(); c != nil {
+		return c
+	}
+	return nil
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaSpec) DeepCopyInto(out *ElasticResourceQuotaSpec) {
+	*out = *in
+	if in.Max != nil {
+		in, out := &in.Max, &out.Max
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	if in.Min != nil {
+		in, out := &in.Min, &out.Min
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	if in.HardwareTypes != nil {
+		in, out := &in.HardwareTypes, &out.HardwareTypes
+		*out = make([]string, len(*in))
+		copy(*out, *in)
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaSpec.
+func (in *ElasticResourceQuotaSpec) DeepCopy() *ElasticResourceQuotaSpec {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaSpec)
+	in.DeepCopyInto(out)
+	return out
+}
+
+// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
+func (in *ElasticResourceQuotaStatus) DeepCopyInto(out *ElasticResourceQuotaStatus) {
+	*out = *in
+	if in.Used != nil {
+		in, out := &in.Used, &out.Used
+		*out = make(v1.ResourceList, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val.DeepCopy()
+		}
+	}
+	return
+}
+
+// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ElasticResourceQuotaStatus.
+func (in *ElasticResourceQuotaStatus) DeepCopy() *ElasticResourceQuotaStatus {
+	if in == nil {
+		return nil
+	}
+	out := new(ElasticResourceQuotaStatus)
+	in.DeepCopyInto(out)
+	return out
+}
+
 // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
 func (in *PodGroup) DeepCopyInto(out *PodGroup) {
 	*out = *in
@@ -106,6 +225,13 @@ func (in *PodGroupList) DeepCopyObject() runtime.Object {
 // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
 func (in *PodGroupSpec) DeepCopyInto(out *PodGroupSpec) {
 	*out = *in
+	if in.MinTaskMember != nil {
+		in, out := &in.MinTaskMember, &out.MinTaskMember
+		*out = make(map[string]int32, len(*in))
+		for key, val := range *in {
+			(*out)[key] = val
+		}
+	}
 	if in.MinResources != nil {
 		in, out := &in.MinResources, &out.MinResources
 		*out = new(v1.ResourceList)
diff --git a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/elasticresourcequota.go b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/elasticresourcequota.go
new file mode 100644
index 0000000..c711a10
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/elasticresourcequota.go
@@ -0,0 +1,183 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+// Code generated by client-gen. DO NOT EDIT.
+
+package v1beta1
+
+import (
+	"context"
+	"time"
+
+	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	types "k8s.io/apimachinery/pkg/types"
+	watch "k8s.io/apimachinery/pkg/watch"
+	rest "k8s.io/client-go/rest"
+	v1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	scheme "volcano.sh/apis/pkg/client/clientset/versioned/scheme"
+)
+
+// ElasticResourceQuotasGetter has a method to return a ElasticResourceQuotaInterface.
+// A group's client should implement this interface.
+type ElasticResourceQuotasGetter interface {
+	ElasticResourceQuotas() ElasticResourceQuotaInterface
+}
+
+// ElasticResourceQuotaInterface has methods to work with ElasticResourceQuota resources.
+type ElasticResourceQuotaInterface interface {
+	Create(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.CreateOptions) (*v1beta1.ElasticResourceQuota, error)
+	Update(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (*v1beta1.ElasticResourceQuota, error)
+	UpdateStatus(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (*v1beta1.ElasticResourceQuota, error)
+	Delete(ctx context.Context, name string, opts v1.DeleteOptions) error
+	DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error
+	Get(ctx context.Context, name string, opts v1.GetOptions) (*v1beta1.ElasticResourceQuota, error)
+	List(ctx context.Context, opts v1.ListOptions) (*v1beta1.ElasticResourceQuotaList, error)
+	Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error)
+	Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *v1beta1.ElasticResourceQuota, err error)
+	ElasticResourceQuotaExpansion
+}
+
+// elasticResourceQuotas implements ElasticResourceQuotaInterface
+type elasticResourceQuotas struct {
+	client rest.Interface
+}
+
+// newElasticResourceQuotas returns a ElasticResourceQuotas
+func newElasticResourceQuotas(c *SchedulingV1beta1Client) *elasticResourceQuotas {
+	return &elasticResourceQuotas{
+		client: c.RESTClient(),
+	}
+}
+
+// Get takes name of the elasticResourceQuota, and returns the corresponding elasticResourceQuota object, and an error if there is any.
+func (c *elasticResourceQuotas) Get(ctx context.Context, name string, options v1.GetOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	result = &v1beta1.ElasticResourceQuota{}
+	err = c.client.Get().
+		Resource("elasticresourcequotas").
+		Name(name).
+		VersionedParams(&options, scheme.ParameterCodec).
+		Do(ctx).
+		Into(result)
+	return
+}
+
+// List takes label and field selectors, and returns the list of ElasticResourceQuotas that match those selectors.
+func (c *elasticResourceQuotas) List(ctx context.Context, opts v1.ListOptions) (result *v1beta1.ElasticResourceQuotaList, err error) {
+	var timeout time.Duration
+	if opts.TimeoutSeconds != nil {
+		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
+	}
+	result = &v1beta1.ElasticResourceQuotaList{}
+	err = c.client.Get().
+		Resource("elasticresourcequotas").
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Timeout(timeout).
+		Do(ctx).
+		Into(result)
+	return
+}
+
+// Watch returns a watch.Interface that watches the requested elasticResourceQuotas.
+func (c *elasticResourceQuotas) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
+	var timeout time.Duration
+	if opts.TimeoutSeconds != nil {
+		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
+	}
+	opts.Watch = true
+	return c.client.Get().
+		Resource("elasticresourcequotas").
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Timeout(timeout).
+		Watch(ctx)
+}
+
+// Create takes the representation of a elasticResourceQuota and creates it.  Returns the server's representation of the elasticResourceQuota, and an error, if there is any.
+func (c *elasticResourceQuotas) Create(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.CreateOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	result = &v1beta1.ElasticResourceQuota{}
+	err = c.client.Post().
+		Resource("elasticresourcequotas").
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Body(elasticResourceQuota).
+		Do(ctx).
+		Into(result)
+	return
+}
+
+// Update takes the representation of a elasticResourceQuota and updates it. Returns the server's representation of the elasticResourceQuota, and an error, if there is any.
+func (c *elasticResourceQuotas) Update(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	result = &v1beta1.ElasticResourceQuota{}
+	err = c.client.Put().
+		Resource("elasticresourcequotas").
+		Name(elasticResourceQuota.Name).
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Body(elasticResourceQuota).
+		Do(ctx).
+		Into(result)
+	return
+}
+
+// UpdateStatus was generated because the type contains a Status member.
+// Add a +genclient:noStatus comment above the type to avoid generating UpdateStatus().
+func (c *elasticResourceQuotas) UpdateStatus(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	result = &v1beta1.ElasticResourceQuota{}
+	err = c.client.Put().
+		Resource("elasticresourcequotas").
+		Name(elasticResourceQuota.Name).
+		SubResource("status").
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Body(elasticResourceQuota).
+		Do(ctx).
+		Into(result)
+	return
+}
+
+// Delete takes name of the elasticResourceQuota and deletes it. Returns an error if one occurs.
+func (c *elasticResourceQuotas) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
+	return c.client.Delete().
+		Resource("elasticresourcequotas").
+		Name(name).
+		Body(&opts).
+		Do(ctx).
+		Error()
+}
+
+// DeleteCollection deletes a collection of objects.
+func (c *elasticResourceQuotas) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
+	var timeout time.Duration
+	if listOpts.TimeoutSeconds != nil {
+		timeout = time.Duration(*listOpts.TimeoutSeconds) * time.Second
+	}
+	return c.client.Delete().
+		Resource("elasticresourcequotas").
+		VersionedParams(&listOpts, scheme.ParameterCodec).
+		Timeout(timeout).
+		Body(&opts).
+		Do(ctx).
+		Error()
+}
+
+// Patch applies the patch and returns the patched elasticResourceQuota.
+func (c *elasticResourceQuotas) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *v1beta1.ElasticResourceQuota, err error) {
+	result = &v1beta1.ElasticResourceQuota{}
+	err = c.client.Patch(pt).
+		Resource("elasticresourcequotas").
+		Name(name).
+		SubResource(subresources...).
+		VersionedParams(&opts, scheme.ParameterCodec).
+		Body(data).
+		Do(ctx).
+		Into(result)
+	return
+}
diff --git a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_elasticresourcequota.go b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_elasticresourcequota.go
new file mode 100644
index 0000000..0227ca9
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_elasticresourcequota.go
@@ -0,0 +1,132 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+// Code generated by client-gen. DO NOT EDIT.
+
+package fake
+
+import (
+	"context"
+
+	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	labels "k8s.io/apimachinery/pkg/labels"
+	schema "k8s.io/apimachinery/pkg/runtime/schema"
+	types "k8s.io/apimachinery/pkg/types"
+	watch "k8s.io/apimachinery/pkg/watch"
+	testing "k8s.io/client-go/testing"
+	v1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+)
+
+// FakeElasticResourceQuotas implements ElasticResourceQuotaInterface
+type FakeElasticResourceQuotas struct {
+	Fake *FakeSchedulingV1beta1
+}
+
+var elasticresourcequotasResource = schema.GroupVersionResource{Group: "scheduling.volcano.sh", Version: "v1beta1", Resource: "elasticresourcequotas"}
+
+var elasticresourcequotasKind = schema.GroupVersionKind{Group: "scheduling.volcano.sh", Version: "v1beta1", Kind: "ElasticResourceQuota"}
+
+// Get takes name of the elasticResourceQuota, and returns the corresponding elasticResourceQuota object, and an error if there is any.
+func (c *FakeElasticResourceQuotas) Get(ctx context.Context, name string, options v1.GetOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootGetAction(elasticresourcequotasResource, name), &v1beta1.ElasticResourceQuota{})
+	if obj == nil {
+		return nil, err
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), err
+}
+
+// List takes label and field selectors, and returns the list of ElasticResourceQuotas that match those selectors.
+func (c *FakeElasticResourceQuotas) List(ctx context.Context, opts v1.ListOptions) (result *v1beta1.ElasticResourceQuotaList, err error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootListAction(elasticresourcequotasResource, elasticresourcequotasKind, opts), &v1beta1.ElasticResourceQuotaList{})
+	if obj == nil {
+		return nil, err
+	}
+
+	label, _, _ := testing.ExtractFromListOptions(opts)
+	if label == nil {
+		label = labels.Everything()
+	}
+	list := &v1beta1.ElasticResourceQuotaList{ListMeta: obj.(*v1beta1.ElasticResourceQuotaList).ListMeta}
+	for _, item := range obj.(*v1beta1.ElasticResourceQuotaList).Items {
+		if label.Matches(labels.Set(item.Labels)) {
+			list.Items = append(list.Items, item)
+		}
+	}
+	return list, err
+}
+
+// Watch returns a watch.Interface that watches the requested elasticResourceQuotas.
+func (c *FakeElasticResourceQuotas) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
+	return c.Fake.
+		InvokesWatch(testing.NewRootWatchAction(elasticresourcequotasResource, opts))
+}
+
+// Create takes the representation of a elasticResourceQuota and creates it.  Returns the server's representation of the elasticResourceQuota, and an error, if there is any.
+func (c *FakeElasticResourceQuotas) Create(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.CreateOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootCreateAction(elasticresourcequotasResource, elasticResourceQuota), &v1beta1.ElasticResourceQuota{})
+	if obj == nil {
+		return nil, err
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), err
+}
+
+// Update takes the representation of a elasticResourceQuota and updates it. Returns the server's representation of the elasticResourceQuota, and an error, if there is any.
+func (c *FakeElasticResourceQuotas) Update(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (result *v1beta1.ElasticResourceQuota, err error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootUpdateAction(elasticresourcequotasResource, elasticResourceQuota), &v1beta1.ElasticResourceQuota{})
+	if obj == nil {
+		return nil, err
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), err
+}
+
+// UpdateStatus was generated because the type contains a Status member.
+// Add a +genclient:noStatus comment above the type to avoid generating UpdateStatus().
+func (c *FakeElasticResourceQuotas) UpdateStatus(ctx context.Context, elasticResourceQuota *v1beta1.ElasticResourceQuota, opts v1.UpdateOptions) (*v1beta1.ElasticResourceQuota, error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootUpdateSubresourceAction(elasticresourcequotasResource, "status", elasticResourceQuota), &v1beta1.ElasticResourceQuota{})
+	if obj == nil {
+		return nil, err
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), err
+}
+
+// Delete takes name of the elasticResourceQuota and deletes it. Returns an error if one occurs.
+func (c *FakeElasticResourceQuotas) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
+	_, err := c.Fake.
+		Invokes(testing.NewRootDeleteAction(elasticresourcequotasResource, name), &v1beta1.ElasticResourceQuota{})
+	return err
+}
+
+// DeleteCollection deletes a collection of objects.
+func (c *FakeElasticResourceQuotas) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
+	action := testing.NewRootDeleteCollectionAction(elasticresourcequotasResource, listOpts)
+
+	_, err := c.Fake.Invokes(action, &v1beta1.ElasticResourceQuotaList{})
+	return err
+}
+
+// Patch applies the patch and returns the patched elasticResourceQuota.
+func (c *FakeElasticResourceQuotas) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *v1beta1.ElasticResourceQuota, err error) {
+	obj, err := c.Fake.
+		Invokes(testing.NewRootPatchSubresourceAction(elasticresourcequotasResource, name, pt, data, subresources...), &v1beta1.ElasticResourceQuota{})
+	if obj == nil {
+		return nil, err
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), err
+}
diff --git a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_scheduling_client.go b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_scheduling_client.go
index b653aa8..6fbbf6d 100644
--- a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_scheduling_client.go
+++ b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/fake/fake_scheduling_client.go
@@ -27,6 +27,10 @@ type FakeSchedulingV1beta1 struct {
 	*testing.Fake
 }
 
+func (c *FakeSchedulingV1beta1) ElasticResourceQuotas() v1beta1.ElasticResourceQuotaInterface {
+	return &FakeElasticResourceQuotas{c}
+}
+
 func (c *FakeSchedulingV1beta1) PodGroups(namespace string) v1beta1.PodGroupInterface {
 	return &FakePodGroups{c, namespace}
 }
diff --git a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/generated_expansion.go b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/generated_expansion.go
index b5e7e74..7be1721 100644
--- a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/generated_expansion.go
+++ b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/generated_expansion.go
@@ -17,6 +17,8 @@ limitations under the License.
 
 package v1beta1
 
+type ElasticResourceQuotaExpansion interface{}
+
 type PodGroupExpansion interface{}
 
 type QueueExpansion interface{}
diff --git a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/scheduling_client.go b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/scheduling_client.go
index 67a271a..8a32b09 100644
--- a/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/scheduling_client.go
+++ b/vendor/volcano.sh/apis/pkg/client/clientset/versioned/typed/scheduling/v1beta1/scheduling_client.go
@@ -25,6 +25,7 @@ import (
 
 type SchedulingV1beta1Interface interface {
 	RESTClient() rest.Interface
+	ElasticResourceQuotasGetter
 	PodGroupsGetter
 	QueuesGetter
 }
@@ -34,6 +35,10 @@ type SchedulingV1beta1Client struct {
 	restClient rest.Interface
 }
 
+func (c *SchedulingV1beta1Client) ElasticResourceQuotas() ElasticResourceQuotaInterface {
+	return newElasticResourceQuotas(c)
+}
+
 func (c *SchedulingV1beta1Client) PodGroups(namespace string) PodGroupInterface {
 	return newPodGroups(c, namespace)
 }
diff --git a/vendor/volcano.sh/apis/pkg/client/informers/externalversions/generic.go b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/generic.go
index 2b06549..0ee3eb5 100644
--- a/vendor/volcano.sh/apis/pkg/client/informers/externalversions/generic.go
+++ b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/generic.go
@@ -67,6 +67,8 @@ func (f *sharedInformerFactory) ForResource(resource schema.GroupVersionResource
 		return &genericInformer{resource: resource.GroupResource(), informer: f.Nodeinfo().V1alpha1().Numatopologies().Informer()}, nil
 
 		// Group=scheduling.volcano.sh, Version=v1beta1
+	case v1beta1.SchemeGroupVersion.WithResource("elasticresourcequotas"):
+		return &genericInformer{resource: resource.GroupResource(), informer: f.Scheduling().V1beta1().ElasticResourceQuotas().Informer()}, nil
 	case v1beta1.SchemeGroupVersion.WithResource("podgroups"):
 		return &genericInformer{resource: resource.GroupResource(), informer: f.Scheduling().V1beta1().PodGroups().Informer()}, nil
 	case v1beta1.SchemeGroupVersion.WithResource("queues"):
diff --git a/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/elasticresourcequota.go b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/elasticresourcequota.go
new file mode 100644
index 0000000..9eb8003
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/elasticresourcequota.go
@@ -0,0 +1,88 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+// Code generated by informer-gen. DO NOT EDIT.
+
+package v1beta1
+
+import (
+	"context"
+	time "time"
+
+	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	runtime "k8s.io/apimachinery/pkg/runtime"
+	watch "k8s.io/apimachinery/pkg/watch"
+	cache "k8s.io/client-go/tools/cache"
+	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+	versioned "volcano.sh/apis/pkg/client/clientset/versioned"
+	internalinterfaces "volcano.sh/apis/pkg/client/informers/externalversions/internalinterfaces"
+	v1beta1 "volcano.sh/apis/pkg/client/listers/scheduling/v1beta1"
+)
+
+// ElasticResourceQuotaInformer provides access to a shared informer and lister for
+// ElasticResourceQuotas.
+type ElasticResourceQuotaInformer interface {
+	Informer() cache.SharedIndexInformer
+	Lister() v1beta1.ElasticResourceQuotaLister
+}
+
+type elasticResourceQuotaInformer struct {
+	factory          internalinterfaces.SharedInformerFactory
+	tweakListOptions internalinterfaces.TweakListOptionsFunc
+}
+
+// NewElasticResourceQuotaInformer constructs a new informer for ElasticResourceQuota type.
+// Always prefer using an informer factory to get a shared informer instead of getting an independent
+// one. This reduces memory footprint and number of connections to the server.
+func NewElasticResourceQuotaInformer(client versioned.Interface, resyncPeriod time.Duration, indexers cache.Indexers) cache.SharedIndexInformer {
+	return NewFilteredElasticResourceQuotaInformer(client, resyncPeriod, indexers, nil)
+}
+
+// NewFilteredElasticResourceQuotaInformer constructs a new informer for ElasticResourceQuota type.
+// Always prefer using an informer factory to get a shared informer instead of getting an independent
+// one. This reduces memory footprint and number of connections to the server.
+func NewFilteredElasticResourceQuotaInformer(client versioned.Interface, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
+	return cache.NewSharedIndexInformer(
+		&cache.ListWatch{
+			ListFunc: func(options v1.ListOptions) (runtime.Object, error) {
+				if tweakListOptions != nil {
+					tweakListOptions(&options)
+				}
+				return client.SchedulingV1beta1().ElasticResourceQuotas().List(context.TODO(), options)
+			},
+			WatchFunc: func(options v1.ListOptions) (watch.Interface, error) {
+				if tweakListOptions != nil {
+					tweakListOptions(&options)
+				}
+				return client.SchedulingV1beta1().ElasticResourceQuotas().Watch(context.TODO(), options)
+			},
+		},
+		&schedulingv1beta1.ElasticResourceQuota{},
+		resyncPeriod,
+		indexers,
+	)
+}
+
+func (f *elasticResourceQuotaInformer) defaultInformer(client versioned.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
+	return NewFilteredElasticResourceQuotaInformer(client, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
+}
+
+func (f *elasticResourceQuotaInformer) Informer() cache.SharedIndexInformer {
+	return f.factory.InformerFor(&schedulingv1beta1.ElasticResourceQuota{}, f.defaultInformer)
+}
+
+func (f *elasticResourceQuotaInformer) Lister() v1beta1.ElasticResourceQuotaLister {
+	return v1beta1.NewElasticResourceQuotaLister(f.Informer().GetIndexer())
+}
diff --git a/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/interface.go b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/interface.go
index b42dea4..0327bb9 100644
--- a/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/interface.go
+++ b/vendor/volcano.sh/apis/pkg/client/informers/externalversions/scheduling/v1beta1/interface.go
@@ -23,6 +23,8 @@ import (
 
 // Interface provides access to all the informers in this group version.
 type Interface interface {
+	// ElasticResourceQuotas returns a ElasticResourceQuotaInformer.
+	ElasticResourceQuotas() ElasticResourceQuotaInformer
 	// PodGroups returns a PodGroupInformer.
 	PodGroups() PodGroupInformer
 	// Queues returns a QueueInformer.
@@ -40,6 +42,11 @@ func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakList
 	return &version{factory: f, namespace: namespace, tweakListOptions: tweakListOptions}
 }
 
+// ElasticResourceQuotas returns a ElasticResourceQuotaInformer.
+func (v *version) ElasticResourceQuotas() ElasticResourceQuotaInformer {
+	return &elasticResourceQuotaInformer{factory: v.factory, tweakListOptions: v.tweakListOptions}
+}
+
 // PodGroups returns a PodGroupInformer.
 func (v *version) PodGroups() PodGroupInformer {
 	return &podGroupInformer{factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions}
diff --git a/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/elasticresourcequota.go b/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/elasticresourcequota.go
new file mode 100644
index 0000000..a34e35a
--- /dev/null
+++ b/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/elasticresourcequota.go
@@ -0,0 +1,67 @@
+/*
+Copyright 2021 The Volcano Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+// Code generated by lister-gen. DO NOT EDIT.
+
+package v1beta1
+
+import (
+	"k8s.io/apimachinery/pkg/api/errors"
+	"k8s.io/apimachinery/pkg/labels"
+	"k8s.io/client-go/tools/cache"
+	v1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
+)
+
+// ElasticResourceQuotaLister helps list ElasticResourceQuotas.
+// All objects returned here must be treated as read-only.
+type ElasticResourceQuotaLister interface {
+	// List lists all ElasticResourceQuotas in the indexer.
+	// Objects returned here must be treated as read-only.
+	List(selector labels.Selector) (ret []*v1beta1.ElasticResourceQuota, err error)
+	// Get retrieves the ElasticResourceQuota from the index for a given name.
+	// Objects returned here must be treated as read-only.
+	Get(name string) (*v1beta1.ElasticResourceQuota, error)
+	ElasticResourceQuotaListerExpansion
+}
+
+// elasticResourceQuotaLister implements the ElasticResourceQuotaLister interface.
+type elasticResourceQuotaLister struct {
+	indexer cache.Indexer
+}
+
+// NewElasticResourceQuotaLister returns a new ElasticResourceQuotaLister.
+func NewElasticResourceQuotaLister(indexer cache.Indexer) ElasticResourceQuotaLister {
+	return &elasticResourceQuotaLister{indexer: indexer}
+}
+
+// List lists all ElasticResourceQuotas in the indexer.
+func (s *elasticResourceQuotaLister) List(selector labels.Selector) (ret []*v1beta1.ElasticResourceQuota, err error) {
+	err = cache.ListAll(s.indexer, selector, func(m interface{}) {
+		ret = append(ret, m.(*v1beta1.ElasticResourceQuota))
+	})
+	return ret, err
+}
+
+// Get retrieves the ElasticResourceQuota from the index for a given name.
+func (s *elasticResourceQuotaLister) Get(name string) (*v1beta1.ElasticResourceQuota, error) {
+	obj, exists, err := s.indexer.GetByKey(name)
+	if err != nil {
+		return nil, err
+	}
+	if !exists {
+		return nil, errors.NewNotFound(v1beta1.Resource("elasticresourcequota"), name)
+	}
+	return obj.(*v1beta1.ElasticResourceQuota), nil
+}
diff --git a/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/expansion_generated.go b/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/expansion_generated.go
index 9894d48..71609be 100644
--- a/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/expansion_generated.go
+++ b/vendor/volcano.sh/apis/pkg/client/listers/scheduling/v1beta1/expansion_generated.go
@@ -17,6 +17,10 @@ limitations under the License.
 
 package v1beta1
 
+// ElasticResourceQuotaListerExpansion allows custom methods to be added to
+// ElasticResourceQuotaLister.
+type ElasticResourceQuotaListerExpansion interface{}
+
 // PodGroupListerExpansion allows custom methods to be added to
 // PodGroupLister.
 type PodGroupListerExpansion interface{}
